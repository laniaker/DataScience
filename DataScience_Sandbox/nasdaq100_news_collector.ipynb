{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starte Crawling. Letzter Ticker war: Keiner\n",
      "-> Versuch 1/3: Suche News für ADBE...\n",
      "!!! Temporärer Serverfehler (500). Warte 10s und versuche erneut.\n",
      "-> Versuch 2/3: Suche News für ADBE...\n",
      "   -> Gefunden: 4 Artikel.\n",
      "-> Versuch 1/3: Suche News für AMD...\n",
      "   -> Gefunden: 99 Artikel.\n",
      "-> Versuch 1/3: Suche News für ABNB...\n",
      "   -> Gefunden: 96 Artikel.\n",
      "-> Versuch 1/3: Suche News für GOOGL...\n",
      "!!! Temporärer Serverfehler (500). Warte 10s und versuche erneut.\n",
      "-> Versuch 2/3: Suche News für GOOGL...\n",
      "!!! Temporärer Serverfehler (500). Warte 10s und versuche erneut.\n",
      "-> Versuch 3/3: Suche News für GOOGL...\n",
      "   -> Gefunden: 56 Artikel.\n",
      "-> Versuch 1/3: Suche News für GOOG...\n",
      "   -> Gefunden: 31 Artikel.\n",
      "-> Versuch 1/3: Suche News für AMZN...\n",
      "   -> Gefunden: 99 Artikel.\n",
      "-> Versuch 1/3: Suche News für AEP...\n",
      "   -> Gefunden: 5 Artikel.\n",
      "-> Versuch 1/3: Suche News für AMGN...\n",
      "   -> Gefunden: 46 Artikel.\n",
      "-> Versuch 1/3: Suche News für ADI...\n",
      "   -> Gefunden: 50 Artikel.\n",
      "-> Versuch 1/3: Suche News für AAPL...\n",
      "!!! Temporärer Serverfehler (500). Warte 10s und versuche erneut.\n",
      "-> Versuch 2/3: Suche News für AAPL...\n",
      "!!! Temporärer Serverfehler (500). Warte 10s und versuche erneut.\n",
      "-> Versuch 3/3: Suche News für AAPL...\n",
      "   -> Gefunden: 97 Artikel.\n",
      "-> Versuch 1/3: Suche News für AMAT...\n",
      "!!! Temporärer Serverfehler (500). Warte 10s und versuche erneut.\n",
      "-> Versuch 2/3: Suche News für AMAT...\n",
      "   -> Gefunden: 97 Artikel.\n",
      "-> Versuch 1/3: Suche News für APP...\n",
      "   -> Gefunden: 98 Artikel.\n",
      "-> Versuch 1/3: Suche News für ARM...\n",
      "   -> Gefunden: 99 Artikel.\n",
      "-> Versuch 1/3: Suche News für ASML...\n",
      "   -> Gefunden: 23 Artikel.\n",
      "-> Versuch 1/3: Suche News für AZN...\n",
      "   -> Gefunden: 51 Artikel.\n",
      "-> Versuch 1/3: Suche News für TEAM...\n",
      "   -> Gefunden: 99 Artikel.\n",
      "-> Versuch 1/3: Suche News für ADSK...\n",
      "   -> Gefunden: 20 Artikel.\n",
      "-> Versuch 1/3: Suche News für ADP...\n",
      "   -> Gefunden: 99 Artikel.\n",
      "-> Versuch 1/3: Suche News für AXON...\n",
      "   -> Gefunden: 29 Artikel.\n",
      "-> Versuch 1/3: Suche News für BKR...\n",
      "   -> Gefunden: 43 Artikel.\n",
      "-> Versuch 1/3: Suche News für BIIB...\n",
      "   -> Gefunden: 15 Artikel.\n",
      "-> Versuch 1/3: Suche News für BKNG...\n",
      "!!! Temporärer Serverfehler (500). Warte 10s und versuche erneut.\n",
      "-> Versuch 2/3: Suche News für BKNG...\n",
      "   -> Gefunden: 11 Artikel.\n",
      "-> Versuch 1/3: Suche News für AVGO...\n",
      "   -> Gefunden: 97 Artikel.\n",
      "-> Versuch 1/3: Suche News für CDNS...\n",
      "   -> Gefunden: 9 Artikel.\n",
      "-> Versuch 1/3: Suche News für CDW...\n",
      "!!! Temporärer Serverfehler (500). Warte 10s und versuche erneut.\n",
      "-> Versuch 2/3: Suche News für CDW...\n",
      "   -> Gefunden: 2 Artikel.\n",
      "-> Versuch 1/3: Suche News für CHTR...\n",
      "   -> Gefunden: 22 Artikel.\n",
      "-> Versuch 1/3: Suche News für CTAS...\n",
      "!!! Temporärer Serverfehler (500). Warte 10s und versuche erneut.\n",
      "-> Versuch 2/3: Suche News für CTAS...\n",
      "   -> Gefunden: 24 Artikel.\n",
      "-> Versuch 1/3: Suche News für CSCO...\n",
      "   -> Gefunden: 99 Artikel.\n",
      "-> Versuch 1/3: Suche News für CCEP...\n",
      "   -> Gefunden: 1 Artikel.\n",
      "-> Versuch 1/3: Suche News für CTSH...\n",
      "!!! Temporärer Serverfehler (500). Warte 10s und versuche erneut.\n",
      "-> Versuch 2/3: Suche News für CTSH...\n",
      "   -> Gefunden: 55 Artikel.\n",
      "-> Versuch 1/3: Suche News für CMCSA...\n",
      "!!! Temporärer Serverfehler (500). Warte 10s und versuche erneut.\n",
      "-> Versuch 2/3: Suche News für CMCSA...\n",
      "   -> Gefunden: 96 Artikel.\n",
      "-> Versuch 1/3: Suche News für CEG...\n",
      "   -> Gefunden: 17 Artikel.\n",
      "-> Versuch 1/3: Suche News für CPRT...\n",
      "   -> Gefunden: 2 Artikel.\n",
      "-> Versuch 1/3: Suche News für CSGP...\n",
      "   -> Gefunden: 6 Artikel.\n",
      "-> Versuch 1/3: Suche News für COST...\n",
      "   -> Gefunden: 95 Artikel.\n",
      "-> Versuch 1/3: Suche News für CRWD...\n",
      "   -> Gefunden: 35 Artikel.\n",
      "-> Versuch 1/3: Suche News für CSX...\n",
      "!!! Temporärer Serverfehler (500). Warte 10s und versuche erneut.\n",
      "-> Versuch 2/3: Suche News für CSX...\n",
      "!!! Temporärer Serverfehler (500). Warte 10s und versuche erneut.\n",
      "-> Versuch 3/3: Suche News für CSX...\n",
      "   -> Gefunden: 5 Artikel.\n",
      "-> Versuch 1/3: Suche News für DDOG...\n",
      "   -> Gefunden: 24 Artikel.\n",
      "-> Versuch 1/3: Suche News für DXCM...\n",
      "   -> Gefunden: 27 Artikel.\n",
      "-> Versuch 1/3: Suche News für FANG...\n",
      "!!! Temporärer Serverfehler (500). Warte 10s und versuche erneut.\n",
      "-> Versuch 2/3: Suche News für FANG...\n",
      "   -> Gefunden: 93 Artikel.\n",
      "-> Versuch 1/3: Suche News für DASH...\n",
      "   -> Gefunden: 99 Artikel.\n",
      "-> Versuch 1/3: Suche News für EA...\n",
      "   -> Gefunden: 96 Artikel.\n",
      "-> Versuch 1/3: Suche News für EXC...\n",
      "   -> Gefunden: 10 Artikel.\n",
      "-> Versuch 1/3: Suche News für FAST...\n",
      "   -> Gefunden: 97 Artikel.\n",
      "-> Versuch 1/3: Suche News für FTNT...\n",
      "   -> Gefunden: 45 Artikel.\n",
      "-> Versuch 1/3: Suche News für GEHC...\n",
      "   -> Gefunden: 27 Artikel.\n",
      "-> Versuch 1/3: Suche News für GILD...\n",
      "   -> Gefunden: 18 Artikel.\n",
      "-> Versuch 1/3: Suche News für GFS...\n",
      "   -> Gefunden: 41 Artikel.\n",
      "-> Versuch 1/3: Suche News für HON...\n",
      "   -> Gefunden: 90 Artikel.\n",
      "-> Versuch 1/3: Suche News für IDXX...\n",
      "   -> Gefunden: 3 Artikel.\n",
      "-> Versuch 1/3: Suche News für INTC...\n",
      "   -> Gefunden: 97 Artikel.\n",
      "-> Versuch 1/3: Suche News für INTU...\n",
      "   -> Gefunden: 96 Artikel.\n",
      "-> Versuch 1/3: Suche News für ISRG...\n",
      "   -> Gefunden: 12 Artikel.\n",
      "-> Versuch 1/3: Suche News für KDP...\n",
      "!!! Temporärer Serverfehler (500). Warte 10s und versuche erneut.\n",
      "-> Versuch 2/3: Suche News für KDP...\n",
      "   -> Gefunden: 15 Artikel.\n",
      "-> Versuch 1/3: Suche News für KLAC...\n",
      "   -> Gefunden: 8 Artikel.\n",
      "-> Versuch 1/3: Suche News für KHC...\n",
      "!!! Temporärer Serverfehler (500). Warte 10s und versuche erneut.\n",
      "-> Versuch 2/3: Suche News für KHC...\n",
      "   -> Gefunden: 16 Artikel.\n",
      "-> Versuch 1/3: Suche News für LRCX...\n",
      "   -> Gefunden: 25 Artikel.\n",
      "-> Versuch 1/3: Suche News für LIN...\n",
      "   -> Gefunden: 95 Artikel.\n",
      "-> Versuch 1/3: Suche News für LULU...\n",
      "   -> Gefunden: 90 Artikel.\n",
      "-> Versuch 1/3: Suche News für MAR...\n",
      "!!! Temporärer Serverfehler (500). Warte 10s und versuche erneut.\n",
      "-> Versuch 2/3: Suche News für MAR...\n",
      "   -> Gefunden: 92 Artikel.\n",
      "-> Versuch 1/3: Suche News für MRVL...\n",
      "   -> Gefunden: 15 Artikel.\n",
      "-> Versuch 1/3: Suche News für MELI...\n",
      "   -> Gefunden: 6 Artikel.\n",
      "-> Versuch 1/3: Suche News für META...\n",
      "   -> Gefunden: 100 Artikel.\n",
      "-> Versuch 1/3: Suche News für MCHP...\n",
      "   -> Gefunden: 9 Artikel.\n",
      "-> Versuch 1/3: Suche News für MU...\n",
      "   -> Gefunden: 100 Artikel.\n",
      "-> Versuch 1/3: Suche News für MSFT...\n",
      "   -> Gefunden: 98 Artikel.\n",
      "-> Versuch 1/3: Suche News für MSTR...\n",
      "   -> Gefunden: 93 Artikel.\n",
      "-> Versuch 1/3: Suche News für MDLZ...\n",
      "   -> Gefunden: 12 Artikel.\n",
      "-> Versuch 1/3: Suche News für MNST...\n",
      "   -> Gefunden: 2 Artikel.\n",
      "-> Versuch 1/3: Suche News für NFLX...\n",
      "!!! Temporärer Serverfehler (500). Warte 10s und versuche erneut.\n",
      "-> Versuch 2/3: Suche News für NFLX...\n",
      "   -> Gefunden: 30 Artikel.\n",
      "-> Versuch 1/3: Suche News für NVDA...\n",
      "   -> Gefunden: 100 Artikel.\n",
      "-> Versuch 1/3: Suche News für NXPI...\n",
      "   -> Gefunden: 6 Artikel.\n",
      "-> Versuch 1/3: Suche News für ORLY...\n",
      "!!! Temporärer Serverfehler (500). Warte 10s und versuche erneut.\n",
      "-> Versuch 2/3: Suche News für ORLY...\n",
      "   -> Gefunden: 2 Artikel.\n",
      "-> Versuch 1/3: Suche News für ODFL...\n",
      "!!! Temporärer Serverfehler (500). Warte 10s und versuche erneut.\n",
      "-> Versuch 2/3: Suche News für ODFL...\n",
      "   -> Gefunden: 4 Artikel.\n",
      "-> Versuch 1/3: Suche News für ON...\n",
      "   -> Gefunden: 95 Artikel.\n",
      "-> Versuch 1/3: Suche News für PCAR...\n",
      "   -> Gefunden: 2 Artikel.\n",
      "-> Versuch 1/3: Suche News für PLTR...\n",
      "   -> Gefunden: 95 Artikel.\n",
      "-> Versuch 1/3: Suche News für PANW...\n",
      "   -> Gefunden: 39 Artikel.\n",
      "-> Versuch 1/3: Suche News für PAYX...\n",
      "   -> Gefunden: 3 Artikel.\n",
      "-> Versuch 1/3: Suche News für PYPL...\n",
      "   -> Gefunden: 100 Artikel.\n",
      "-> Versuch 1/3: Suche News für PDD...\n",
      "   -> Gefunden: 19 Artikel.\n",
      "-> Versuch 1/3: Suche News für PEP...\n",
      "   -> Gefunden: 99 Artikel.\n",
      "-> Versuch 1/3: Suche News für QCOM...\n",
      "   -> Gefunden: 99 Artikel.\n",
      "-> Versuch 1/3: Suche News für REGN...\n",
      "   -> Gefunden: 7 Artikel.\n",
      "-> Versuch 1/3: Suche News für ROP...\n",
      "   -> Gefunden: 7 Artikel.\n",
      "-> Versuch 1/3: Suche News für ROST...\n",
      "   -> Gefunden: 4 Artikel.\n",
      "-> Versuch 1/3: Suche News für SHOP...\n",
      "   -> Gefunden: 95 Artikel.\n",
      "-> Versuch 1/3: Suche News für SOLS...\n",
      "   -> Gefunden: 18 Artikel.\n",
      "-> Versuch 1/3: Suche News für SBUX...\n",
      "   -> Gefunden: 99 Artikel.\n",
      "-> Versuch 1/3: Suche News für SNPS...\n",
      "   -> Gefunden: 68 Artikel.\n",
      "-> Versuch 1/3: Suche News für TMUS...\n",
      "   -> Gefunden: 20 Artikel.\n",
      "-> Versuch 1/3: Suche News für TTWO...\n",
      "   -> Gefunden: 16 Artikel.\n",
      "-> Versuch 1/3: Suche News für TSLA...\n",
      "   -> Gefunden: 100 Artikel.\n",
      "-> Versuch 1/3: Suche News für TXN...\n",
      "   -> Gefunden: 23 Artikel.\n",
      "-> Versuch 1/3: Suche News für TRI...\n",
      "   -> Gefunden: 99 Artikel.\n",
      "-> Versuch 1/3: Suche News für TTD...\n",
      "   -> Gefunden: 6 Artikel.\n",
      "-> Versuch 1/3: Suche News für VRSK...\n",
      "   -> Gefunden: 10 Artikel.\n",
      "-> Versuch 1/3: Suche News für VRTX...\n",
      "   -> Gefunden: 5 Artikel.\n",
      "-> Versuch 1/3: Suche News für WBD...\n",
      "   -> Gefunden: 99 Artikel.\n",
      "-> Versuch 1/3: Suche News für WDAY...\n",
      "   -> Gefunden: 1 Artikel.\n",
      "-> Versuch 1/3: Suche News für XEL...\n",
      "!!! LIMIT ERREICHT (429 Too Many Requests). Skript stoppt und speichert Fortschritt.\n",
      "Crawling beendet wegen API-Limit.\n",
      "\n",
      "--- ZUSAMMENFASSUNG ---\n",
      "Gesamtanzahl Artikel (nach Deduplizierung): 4850\n",
      "Anzahl entfernter Duplikate: 3\n",
      "Daten erfolgreich gespeichert in: gesammelte_nasdaq_news.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# --- KONFIGURATION ---\n",
    "NEWS_API_KEY = \"41f7159b90304d589a6156bf6f726ab6\" \n",
    "CSV_DATEIPFAD = \"WikiNasdaq_100_constituents.csv\" \n",
    "OUTPUT_CSV = \"gesammelte_nasdaq_news.csv\" \n",
    "STATUS_FILE = \"status.txt\" # Speichert den Ticker des zuletzt verarbeiteten Unternehmens\n",
    "\n",
    "MAX_RETRIES = 3     # Maximale Versuche pro API-Anruf\n",
    "WAIT_TIME_SECONDS = 2 # Wartezeit zwischen erfolgreichen Abfragen (API-Schonung)\n",
    "RETRY_WAIT_SECONDS = 10 # Längere Wartezeit bei Serverfehlern\n",
    "\n",
    "# --- HILFSFUNKTIONEN ---\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"Lädt den Ticker des zuletzt erfolgreich verarbeiteten Unternehmens.\"\"\"\n",
    "    if os.path.exists(STATUS_FILE):\n",
    "        with open(STATUS_FILE, 'r') as f:\n",
    "            return f.read().strip()\n",
    "    return None\n",
    "\n",
    "def save_progress(ticker):\n",
    "    \"\"\"Speichert den Ticker des zuletzt erfolgreich verarbeiteten Unternehmens.\"\"\"\n",
    "    with open(STATUS_FILE, 'w') as f:\n",
    "        f.write(ticker)\n",
    "\n",
    "# --- KERNFUNKTION ---\n",
    "\n",
    "def fetch_news_for_company(company_name, ticker, max_retries):\n",
    "    \"\"\"Ruft NewsAPI-Headlines für ein Unternehmen ab mit Retry-Logik.\"\"\"\n",
    "    \n",
    "    query = f'\"{company_name}\" OR \"{ticker}\"'\n",
    "    url = \"https://newsapi.org/v2/everything\"\n",
    "    \n",
    "    # Abruf der letzten 7 Tage\n",
    "    seven_days_ago = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    params = {\n",
    "        'q': query,\n",
    "        'language': 'en',\n",
    "        'sortBy': 'publishedAt',\n",
    "        'apiKey': NEWS_API_KEY,\n",
    "        'from': seven_days_ago,\n",
    "        'pageSize': 100 \n",
    "    }\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"-> Versuch {attempt + 1}/{max_retries}: Suche News für {ticker}...\")\n",
    "            response = requests.get(url, params=params)\n",
    "            response.raise_for_status() # Löst Fehler für 4xx/5xx Statuscodes aus\n",
    "            data = response.json()\n",
    "            \n",
    "            articles = data.get('articles', [])\n",
    "            print(f\"   -> Gefunden: {len(articles)} Artikel.\")\n",
    "            \n",
    "            company_news = []\n",
    "            for article in articles:\n",
    "                company_news.append({\n",
    "                    'ticker': ticker,\n",
    "                    'company_name': company_name,\n",
    "                    'title': article.get('title'),\n",
    "                    'description': article.get('description'),\n",
    "                    'published_at': article.get('publishedAt'),\n",
    "                    'source_name': article.get('source', {}).get('name'),\n",
    "                    'url': article.get('url')\n",
    "                })\n",
    "                \n",
    "            return company_news\n",
    "            \n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            status_code = e.response.status_code\n",
    "            if status_code == 429:\n",
    "                print(\"!!! LIMIT ERREICHT (429 Too Many Requests). Skript stoppt und speichert Fortschritt.\")\n",
    "                raise # Stoppt die Hauptschleife\n",
    "            elif status_code in [500, 502, 503, 504]:\n",
    "                # Temporäre Serverfehler\n",
    "                print(f\"!!! Temporärer Serverfehler ({status_code}). Warte {RETRY_WAIT_SECONDS}s und versuche erneut.\")\n",
    "                time.sleep(RETRY_WAIT_SECONDS)\n",
    "                continue\n",
    "            else:\n",
    "                # Andere Fehler (z.B. 401, 404)\n",
    "                print(f\"!!! Dauerhafter HTTP Fehler ({status_code}) bei {ticker}. Wird übersprungen.\")\n",
    "                return []\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # Fehler wie ConnectionError (z.B. Ausfall)\n",
    "            print(f\"!!! Verbindungsfehler bei {ticker}: {e}. Warte {RETRY_WAIT_SECONDS}s und versuche erneut.\")\n",
    "            time.sleep(RETRY_WAIT_SECONDS)\n",
    "            continue\n",
    "            \n",
    "    print(f\"!!! ALLE VERSUCHE FÜR {ticker} SIND GESCHEITERT. Wird übersprungen.\")\n",
    "    return []\n",
    "\n",
    "# --- HAUPTPROGRAMM ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Hauptfunktion zur Koordination des Crawlings.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        df_nasdaq = pd.read_csv(CSV_DATEIPFAD)\n",
    "        # KORREKTUR: Prüfung auf 'Company' und 'Ticker'\n",
    "        if 'Ticker' not in df_nasdaq.columns or 'Company' not in df_nasdaq.columns:\n",
    "            raise ValueError(f\"CSV muss die Spalten 'Ticker' und 'Company' enthalten. Gefundene Spalten: {df_nasdaq.columns.tolist()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Laden der CSV-Datei: {e}\")\n",
    "        return\n",
    "\n",
    "    # 1. Fortschritt laden\n",
    "    last_processed_ticker = load_progress()\n",
    "    start_crawling = True if last_processed_ticker is None else False\n",
    "    \n",
    "    # Laden alter Daten, falls vorhanden\n",
    "    if os.path.exists(OUTPUT_CSV):\n",
    "        df_existing = pd.read_csv(OUTPUT_CSV)\n",
    "        all_articles = df_existing.to_dict('records')\n",
    "        print(f\"Lade {len(all_articles)} existierende Artikel aus {OUTPUT_CSV}.\")\n",
    "    else:\n",
    "        all_articles = []\n",
    "\n",
    "    \n",
    "    print(f\"\\nStarte Crawling. Letzter Ticker war: {last_processed_ticker if last_processed_ticker else 'Keiner'}\")\n",
    "\n",
    "    # 2. Schleife durch alle Unternehmen\n",
    "    for index, row in df_nasdaq.iterrows():\n",
    "        ticker = row['Ticker']\n",
    "        \n",
    "        # Springen zu dem Ticker, wo wir aufgehört haben\n",
    "        if not start_crawling:\n",
    "            if ticker == last_processed_ticker:\n",
    "                start_crawling = True\n",
    "                print(f\"Fortsetzung gefunden: Starte nach {ticker}.\")\n",
    "            continue\n",
    "        \n",
    "        company_name = row['Company']\n",
    "        \n",
    "        # 3. News abrufen\n",
    "        try:\n",
    "            news_data = fetch_news_for_company(company_name, ticker, MAX_RETRIES)\n",
    "            all_articles.extend(news_data)\n",
    "            \n",
    "            # 4. Fortschritt speichern und warten\n",
    "            save_progress(ticker) # Speichert den Ticker nach erfolgreicher Verarbeitung\n",
    "            time.sleep(WAIT_TIME_SECONDS) \n",
    "            \n",
    "        except Exception:\n",
    "            # Wird bei 429-Fehler ausgelöst, bricht die Schleife ab, Fortschritt ist gespeichert\n",
    "            print(\"Crawling beendet wegen API-Limit.\")\n",
    "            break \n",
    "            \n",
    "\n",
    "    # 5. Daten konsolidieren und speichern\n",
    "    if all_articles:\n",
    "        df_results = pd.DataFrame(all_articles)\n",
    "        \n",
    "        # Deduplizierung basierend auf der URL und Ticker\n",
    "        initial_count = len(df_results)\n",
    "        df_results.drop_duplicates(subset=['url', 'ticker'], inplace=True) \n",
    "        final_count = len(df_results)\n",
    "        \n",
    "        print(\"\\n--- ZUSAMMENFASSUNG ---\")\n",
    "        print(f\"Gesamtanzahl Artikel (nach Deduplizierung): {final_count}\")\n",
    "        print(f\"Anzahl entfernter Duplikate: {initial_count - final_count}\")\n",
    "        \n",
    "        df_results.to_csv(OUTPUT_CSV, index=False)\n",
    "        print(f\"Daten erfolgreich gespeichert in: {OUTPUT_CSV}\")\n",
    "    \n",
    "    # Aufräumen des Status-Files, wenn alle Unternehmen durchlaufen wurden\n",
    "    if start_crawling and (index + 1) == len(df_nasdaq):\n",
    "        os.remove(STATUS_FILE)\n",
    "        print(\"Alle Unternehmen verarbeitet. Statusdatei entfernt.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STARTE TEIL 1 MIT KEY ***6ab6 ---\n",
      "  -> Suche News für: ADBE (Adobe Inc.)...\n",
      "!!! LIMIT ERREICHT (429) für 6ab6. Abbruch dieses Teils.\n",
      "\n",
      "!!! API-Limit für Key ***6ab6 erreicht. Beende diesen Teil.\n",
      "\n",
      "--- STARTE TEIL 2 MIT KEY ***efd9 ---\n",
      "  -> Suche News für: INTU (Intuit)...\n",
      "  -> Gefunden: 96 Artikel.\n",
      "  -> Suche News für: ISRG (Intuitive Surgical)...\n",
      "  -> Gefunden: 12 Artikel.\n",
      "  -> Suche News für: KDP (Keurig Dr Pepper)...\n",
      "  -> Gefunden: 15 Artikel.\n",
      "  -> Suche News für: KLAC (KLA Corporation)...\n",
      "  -> Gefunden: 8 Artikel.\n",
      "  -> Suche News für: KHC (Kraft Heinz)...\n",
      "  -> Gefunden: 16 Artikel.\n",
      "  -> Suche News für: LRCX (Lam Research)...\n",
      "  -> Gefunden: 25 Artikel.\n",
      "  -> Suche News für: LIN (Linde plc)...\n",
      "  -> Gefunden: 95 Artikel.\n",
      "  -> Suche News für: LULU (Lululemon)...\n",
      "  -> Gefunden: 92 Artikel.\n",
      "  -> Suche News für: MAR (Marriott International)...\n",
      "  -> Gefunden: 92 Artikel.\n",
      "  -> Suche News für: MRVL (Marvell Technology)...\n",
      "  -> Gefunden: 15 Artikel.\n",
      "  -> Suche News für: MELI (Mercado Libre)...\n",
      "  -> Gefunden: 6 Artikel.\n",
      "  -> Suche News für: META (Meta Platforms)...\n",
      "  -> Gefunden: 98 Artikel.\n",
      "  -> Suche News für: MCHP (Microchip Technology)...\n",
      "  -> Gefunden: 9 Artikel.\n",
      "  -> Suche News für: MU (Micron Technology)...\n",
      "  -> Gefunden: 100 Artikel.\n",
      "  -> Suche News für: MSFT (Microsoft)...\n",
      "  -> Gefunden: 99 Artikel.\n",
      "  -> Suche News für: MSTR (MicroStrategy)...\n",
      "  -> Gefunden: 96 Artikel.\n",
      "  -> Suche News für: MDLZ (Mondelez International)...\n",
      "  -> Gefunden: 12 Artikel.\n",
      "  -> Suche News für: MNST (Monster Beverage)...\n",
      "  -> Gefunden: 2 Artikel.\n",
      "  -> Suche News für: NFLX (Netflix, Inc.)...\n",
      "  -> Gefunden: 33 Artikel.\n",
      "  -> Suche News für: NVDA (Nvidia)...\n",
      "  -> Gefunden: 100 Artikel.\n",
      "  -> Suche News für: NXPI (NXP Semiconductors)...\n",
      "  -> Gefunden: 6 Artikel.\n",
      "  -> Suche News für: ORLY (O'Reilly Automotive)...\n",
      "  -> Gefunden: 2 Artikel.\n",
      "  -> Suche News für: ODFL (Old Dominion Freight Line)...\n",
      "  -> Gefunden: 4 Artikel.\n",
      "  -> Suche News für: ON (Onsemi)...\n",
      "  -> Gefunden: 96 Artikel.\n",
      "  -> Suche News für: PCAR (Paccar)...\n",
      "  -> Gefunden: 2 Artikel.\n",
      "  -> Suche News für: PLTR (Palantir Technologies)...\n",
      "  -> Gefunden: 95 Artikel.\n",
      "  -> Suche News für: PANW (Palo Alto Networks)...\n",
      "  -> Gefunden: 39 Artikel.\n",
      "  -> Suche News für: PAYX (Paychex)...\n",
      "  -> Gefunden: 3 Artikel.\n",
      "  -> Suche News für: PYPL (PayPal)...\n",
      "  -> Gefunden: 100 Artikel.\n",
      "  -> Suche News für: PDD (PDD Holdings)...\n",
      "  -> Gefunden: 19 Artikel.\n",
      "  -> Suche News für: PEP (PepsiCo)...\n",
      "  -> Gefunden: 99 Artikel.\n",
      "  -> Suche News für: QCOM (Qualcomm)...\n",
      "  -> Gefunden: 99 Artikel.\n",
      "  -> Suche News für: REGN (Regeneron Pharmaceuticals)...\n",
      "  -> Gefunden: 7 Artikel.\n",
      "  -> Suche News für: ROP (Roper Technologies)...\n",
      "  -> Gefunden: 7 Artikel.\n",
      "  -> Suche News für: ROST (Ross Stores)...\n",
      "  -> Gefunden: 4 Artikel.\n",
      "  -> Suche News für: SHOP (Shopify)...\n",
      "  -> Gefunden: 97 Artikel.\n",
      "  -> Suche News für: SOLS (Solstice Advanced Materials)...\n",
      "  -> Gefunden: 18 Artikel.\n",
      "  -> Suche News für: SBUX (Starbucks)...\n",
      "  -> Gefunden: 99 Artikel.\n",
      "  -> Suche News für: SNPS (Synopsys)...\n",
      "  -> Gefunden: 68 Artikel.\n",
      "  -> Suche News für: TMUS (T-Mobile US)...\n",
      "  -> Gefunden: 20 Artikel.\n",
      "  -> Suche News für: TTWO (Take-Two Interactive)...\n",
      "  -> Gefunden: 17 Artikel.\n",
      "  -> Suche News für: TSLA (Tesla, Inc.)...\n",
      "  -> Gefunden: 100 Artikel.\n",
      "  -> Suche News für: TXN (Texas Instruments)...\n",
      "  -> Gefunden: 23 Artikel.\n",
      "  -> Suche News für: TRI (Thomson Reuters)...\n",
      "  -> Gefunden: 99 Artikel.\n",
      "  -> Suche News für: TTD (Trade Desk (The))...\n",
      "  -> Gefunden: 6 Artikel.\n",
      "  -> Suche News für: VRSK (Verisk Analytics)...\n",
      "  -> Gefunden: 10 Artikel.\n",
      "  -> Suche News für: VRTX (Vertex Pharmaceuticals)...\n",
      "  -> Gefunden: 5 Artikel.\n",
      "  -> Suche News für: WBD (Warner Bros. Discovery)...\n",
      "  -> Gefunden: 99 Artikel.\n",
      "  -> Suche News für: WDAY (Workday, Inc.)...\n",
      "  -> Gefunden: 1 Artikel.\n",
      "  -> Suche News für: XEL (Xcel Energy)...\n",
      "  -> Gefunden: 6 Artikel.\n",
      "  -> Suche News für: ZS (Zscaler)...\n",
      "  -> Gefunden: 64 Artikel.\n",
      "\n",
      "--- ZUSAMMENFASSUNG ---\n",
      "Gesamtanzahl Artikel (nach Deduplizierung): 2089\n",
      "Daten erfolgreich angehängt/gespeichert in: gesammelte_nasdaq_news_doublekey.csv\n",
      "Crawling abgeschlossen. Fortschrittsdatei entfernt.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# --- 1. KONFIGURATION ---\n",
    "# WICHTIG: Ersetzen Sie die Platzhalter durch Ihre tatsächlichen NewsAPI-Keys\n",
    "API_KEYS = [\n",
    "    \"41f7159b90304d589a6156bf6f726ab6\", \n",
    "    \"a80f40f0df954696b206f77b1e91efd9\" \n",
    "]\n",
    "\n",
    "# Dateipfade\n",
    "CSV_DATEIPFAD = \"WikiNasdaq_100_constituents.csv\" \n",
    "OUTPUT_CSV = \"gesammelte_nasdaq_news_doublekey.csv\"\n",
    "PROGRESS_FILE = \"crawling_progress.txt\" # Speichert den Ticker des letzten erfolgreich verarbeiteten Unternehmens\n",
    "\n",
    "# Die korrekten Spaltennamen aus Ihrer CSV-Datei\n",
    "TICKER_COLUMN = 'Ticker'\n",
    "NAME_COLUMN = 'Company'\n",
    "\n",
    "\n",
    "# --- 2. HILFSFUNKTIONEN ---\n",
    "\n",
    "def fetch_news_for_company(company_name, ticker, api_key):\n",
    "    \"\"\"Ruft NewsAPI-Headlines für ein Unternehmen mit einem spezifischen Key ab.\"\"\"\n",
    "    \n",
    "    query = f'\"{company_name}\" OR \"{ticker}\"'\n",
    "    url = \"https://newsapi.org/v2/everything\"\n",
    "    \n",
    "    # Abfrage der letzten 7 Tage\n",
    "    seven_days_ago = (datetime.now() - pd.Timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    params = {\n",
    "        'q': query,\n",
    "        'language': 'en',\n",
    "        'sortBy': 'publishedAt',\n",
    "        'apiKey': api_key,\n",
    "        'from': seven_days_ago,\n",
    "        'pageSize': 100 \n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"  -> Suche News für: {ticker} ({company_name})...\")\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status() \n",
    "        data = response.json()\n",
    "        \n",
    "        articles = data.get('articles', [])\n",
    "        print(f\"  -> Gefunden: {len(articles)} Artikel.\")\n",
    "        \n",
    "        company_news = []\n",
    "        for article in articles:\n",
    "            company_news.append({\n",
    "                'ticker': ticker,\n",
    "                'company_name': company_name,\n",
    "                'title': article.get('title'),\n",
    "                'description': article.get('description'),\n",
    "                'published_at': article.get('publishedAt'),\n",
    "                'source_name': article.get('source', {}).get('name'),\n",
    "                'url': article.get('url')\n",
    "            })\n",
    "            \n",
    "        return company_news\n",
    "        \n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if e.response.status_code == 429:\n",
    "            # Spezifische Behandlung des Limit-Fehlers (429)\n",
    "            print(f\"!!! LIMIT ERREICHT (429) für {api_key[-4:]}. Abbruch dieses Teils.\")\n",
    "            # Wirft einen spezifischen Fehler, der in process_part abgefangen wird\n",
    "            raise Exception(\"API_LIMIT_REACHED\") \n",
    "        \n",
    "        print(f\"!!! HTTP Fehler bei {ticker} ({api_key[-4:]}): {e.response.status_code}. Wird übersprungen.\")\n",
    "        return []\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"!!! Allgemeiner Fehler bei {ticker}: {e}. Wird übersprungen.\")\n",
    "        return []\n",
    "\n",
    "def save_progress(ticker):\n",
    "    \"\"\"Speichert den Ticker des zuletzt erfolgreich verarbeiteten Unternehmens.\"\"\"\n",
    "    with open(PROGRESS_FILE, 'w') as f:\n",
    "        f.write(ticker)\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"Lädt den Ticker, ab dem das Crawling fortgesetzt werden soll.\"\"\"\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        with open(PROGRESS_FILE, 'r') as f:\n",
    "            return f.read().strip()\n",
    "    return None\n",
    "\n",
    "def process_part(df_part, api_key_index, all_articles):\n",
    "    \"\"\"Verarbeitet einen DataFrame-Teil mit einem spezifischen API-Key.\"\"\"\n",
    "    api_key = API_KEYS[api_key_index]\n",
    "    \n",
    "    # Prüfen, ob bereits Fortschritt aus früheren Läufen gespeichert wurde\n",
    "    last_processed_ticker = load_progress()\n",
    "    start_crawling = True if last_processed_ticker is None else False\n",
    "    \n",
    "    key_suffix = api_key[-4:]\n",
    "\n",
    "    print(f\"\\n--- STARTE TEIL {api_key_index + 1} MIT KEY ***{key_suffix} ---\")\n",
    "    \n",
    "    for index, row in df_part.iterrows():\n",
    "        # Lesen der Daten mit den konfigurierten Spaltennamen\n",
    "        ticker = row[TICKER_COLUMN]\n",
    "        company_name = row[NAME_COLUMN]\n",
    "        \n",
    "        # Logik zum Wiederaufsetzen nach einem Absturz/Limit\n",
    "        if not start_crawling:\n",
    "            if ticker == last_processed_ticker:\n",
    "                # Das letzte verarbeitete Element wurde gefunden, mit dem nächsten fortsetzen.\n",
    "                start_crawling = True\n",
    "                print(f\"*** Fortschritt geladen. Starte ab dem nächsten Unternehmen nach {last_processed_ticker}.\")\n",
    "            continue\n",
    "        \n",
    "        # Daten abrufen\n",
    "        try:\n",
    "            news_data = fetch_news_for_company(company_name, ticker, api_key)\n",
    "            all_articles.extend(news_data)\n",
    "            \n",
    "            # Nur bei Erfolg den Fortschritt speichern\n",
    "            save_progress(ticker)\n",
    "            \n",
    "        except Exception as e:\n",
    "            if str(e) == \"API_LIMIT_REACHED\":\n",
    "                print(f\"\\n!!! API-Limit für Key ***{key_suffix} erreicht. Beende diesen Teil.\")\n",
    "                return False # Signalisiert, dass das Limit erreicht wurde\n",
    "            raise # Wirft andere Fehler weiter\n",
    "            \n",
    "        # Wichtig: Zeitverzögerung zwischen Anfragen (reduziert die Wahrscheinlichkeit eines 429)\n",
    "        time.sleep(1.5) \n",
    "        \n",
    "    return True # Signalisiert, dass der Teil erfolgreich abgeschlossen wurde\n",
    "\n",
    "# --- 3. HAUPTPROGRAMM ---\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # 1. CSV einlesen und Validierung\n",
    "    try:\n",
    "        df_nasdaq = pd.read_csv(CSV_DATEIPFAD)\n",
    "        if len(API_KEYS) < 2:\n",
    "             print(\"FEHLER: Bitte beide API-Keys im Skript konfigurieren.\")\n",
    "             sys.exit(1)\n",
    "        \n",
    "        if TICKER_COLUMN not in df_nasdaq.columns or NAME_COLUMN not in df_nasdaq.columns:\n",
    "             print(f\"FEHLER: Die CSV-Datei muss die Spalten '{TICKER_COLUMN}' und '{NAME_COLUMN}' enthalten. Bitte in der Konfiguration prüfen.\")\n",
    "             sys.exit(1)\n",
    "             \n",
    "    except FileNotFoundError:\n",
    "        print(f\"FEHLER: Die Datei '{CSV_DATEIPFAD}' wurde nicht gefunden.\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Laden der CSV-Datei: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    total_companies = len(df_nasdaq)\n",
    "    split_point = total_companies // 2 \n",
    "    \n",
    "    # 2. DataFrame aufteilen\n",
    "    df_parts = [df_nasdaq.iloc[:split_point], df_nasdaq.iloc[split_point:]]\n",
    "    all_articles = []\n",
    "    \n",
    "    # 3. Teile nacheinander mit den dedizierten Keys verarbeiten\n",
    "    for i in range(len(API_KEYS)):\n",
    "        df_part = df_parts[i]\n",
    "        \n",
    "        # Starte die Verarbeitung des Teils. \n",
    "        success = process_part(df_part, i, all_articles)\n",
    "        \n",
    "        # Wenn das Limit erreicht wurde, versuchen wir den nächsten Key (falls vorhanden).\n",
    "        if not success:\n",
    "            continue\n",
    "    \n",
    "    # 4. Daten konsolidieren und speichern\n",
    "    if all_articles:\n",
    "        df_results = pd.DataFrame(all_articles)\n",
    "        \n",
    "        # Deduplizierung basierend auf der URL \n",
    "        df_results.drop_duplicates(subset=['url'], inplace=True) \n",
    "        \n",
    "        print(\"\\n--- ZUSAMMENFASSUNG ---\")\n",
    "        print(f\"Gesamtanzahl Artikel (nach Deduplizierung): {len(df_results)}\")\n",
    "        \n",
    "        # Speichern im \"append\" Modus, um Fortschritte zu sichern\n",
    "        # 'header=...' stellt sicher, dass der Header nur einmal geschrieben wird\n",
    "        write_header = not os.path.exists(OUTPUT_CSV) or os.stat(OUTPUT_CSV).st_size == 0\n",
    "        df_results.to_csv(OUTPUT_CSV, index=False, mode='a', header=write_header)\n",
    "        print(f\"Daten erfolgreich angehängt/gespeichert in: {OUTPUT_CSV}\")\n",
    "        \n",
    "        # Beim erfolgreichen Abschluss des gesamten Crawlings die Fortschrittsdatei löschen\n",
    "        if os.path.exists(PROGRESS_FILE):\n",
    "             os.remove(PROGRESS_FILE)\n",
    "             print(\"Crawling abgeschlossen. Fortschrittsdatei entfernt.\")\n",
    "    else:\n",
    "        print(\"\\nKeine neuen Artikel gefunden oder Fehler aufgetreten.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
