[
    {
        "Ticker": "PLTR",
        "Company": "Palantir Technologies",
        "Source_URL": "https://www.palantir.com/",
        "Content_Type": "Website Content",
        "Raw_Text": "Our software powers real-time, AI-driven decisions in critical government and commercial enterprises in the West, from the factory floors to the front lines.\nOur Platforms\nAutomate operations, from the factory floor to the front lines\n/0.1\nAIP\n/0.1\nAchieve AI-driven combat superiority, from space to mud\n/0.2\nGotham\n/0.2\nBuild and manage Ontology-powered software, with a complete developer platform\n/0.3\nFoundry\n/0.3\nAutonomously deploy, monitor, and manage software across any environment\n/0.4\nApollo\n/0.4"
    },
    {
        "Ticker": "PLTR",
        "Company": "Palantir Technologies",
        "Source_URL": "https://www.palantir.com/platforms/aip/",
        "Content_Type": "Website Content",
        "Raw_Text": "Build AI apps, actions, and agents in Workflow Builder — an intuitive workspace designed with next-gen AI builders in mind.\nReady your AI-driven workflows for production and iteratively improve them with end-to-end evaluation tooling.\nOntology SDK anchors software development in the operational truth of the enterprise.\nAIP Now makes it easy to explore pre-built AI applications, examples, and builder starter packs."
    },
    {
        "Ticker": "PLTR",
        "Company": "Palantir Technologies",
        "Source_URL": "https://www.palantir.com/about/",
        "Content_Type": "Website Content",
        "Raw_Text": "Why We’re Here\nWe believe in augmenting human intelligence, not replacing it.\nWith good data and the right technology, people and institutions today can still solve hard problems and change the world for the better.\nIn 2003, when we looked at the available technology, we saw products that were too rigid to handle novel problems, and custom systems that took too long to deploy and required too many services to maintain and improve.\nWe saw automated approaches that failed against adaptive adversaries, and all-or-nothing access controls that forced organizations to make unacceptable trade-offs between collaborating and securing sensitive data from misuse.\nWe saw a need for a different kind of technology, and we knew it would take a different kind of company to build it. That’s why we founded Palantir.\nWhat we do\nWe make products for human-driven analysis of real-world data\nWe’re focused on creating the world’s best user experience for working with data, one that empowers people to ask and answer complex questions without requiring them to master querying languages, statistical modeling, or the command line.\nTo achieve this, we build platforms for integrating, managing, and securing data on top of which we layer applications for fully interactive human-driven, machine-assisted analysis.\nWe build our company around mission-driven engineering\nWe’re engineers, not academics.\nAt our offices around the world, we’ve assembled a team that combines practical expertise in distributed systems infrastructure, big data processing, user experience design, and data science. Whatever their role, each Palantirian combines an uncompromising engineering mindset with an unwavering focus on executing in service of the mission.\nWe meet the problems where they live\nOur customers have data and a deep understanding of the problems they face. We have proven products and an engineering mindset.\nWe send our engineers into the field to work directly with our customers—deploying our products, integrating their data, optimizing their workflows, and producing operational results in weeks, not years. By establishing a true partnership, we help customers get the most out of our products and engineering expertise.\nWe design technology to help institutions protect liberty\nAnalytic technology, especially in the hands of powerful institutions that hold large volumes of data, can pose serious risks to privacy and civil liberties.\nThat's why we build privacy-protective capabilities into our products, help customers understand how to use them responsibly, and work with advocacy groups and the policy community on how technology can be used to protect privacy interests today and in the future. We have always been, and continue to be, committed to helping organizations get value out of their data while protecting sensitive information from misuse and abuse.\nWe go where we're needed most\nWe are engineers on a mission.\nWe seek out the most critical problems we can find—the ones that pose threats not only to many of the world’s most important institutions, but to the people they serve as well. Some institutions have the resources to pay for our products, and some don’t. Whatever the situation, our approach with all our clients is the same: to establish a partnership that transforms the way they use data in pursuit of their goals.\nWhere we’re going\nOrganizations around the world are using Palantir to help them do their most important work.\nWith Palantir, investigators are uncovering human trafficking rings, finding exploited children, and unraveling complex financial crimes.\nHumanitarian response organizations are directing resources more effectively to communities affected by natural disasters. Prosecutors are building stronger cases against insider traders. Public health officials are tracking and containing the spread of deadly diseases. Information security specialists are defending against the theft of intellectual property, while oversight authorities are auditing user activity to protect sensitive data from misuse or abuse. And this is just the beginning.\nWe are working to build a future in which public institutions, commercial enterprises, and non-profit organizations can use data to function as they were designed—to fulfill the mandates with which they’ve been entrusted, to deliver value to customers, and to distribute aid to those most in need."
    },
    {
        "Ticker": "PLTR",
        "Company": "Palantir Technologies",
        "Source_URL": "https://www.palantir.com/blog/",
        "Content_Type": "Website Content",
        "Raw_Text": "Palantir Blog\nPalantir is a software company that builds the world’s leading platforms for data-driven operations and decision-making. We develop long-term partnerships with organizations, working together to help them realize the value in their data and transform how they operate.\nFeatured Posts\nDeploying Full Spectrum AI in Days with AIP Bootcamps\nPalantir Artificial Intelligence Platform (AIP) enables new and existing customers to dramatically accelerate high-value AI use cases. To meet the intense requirements of the moment, we have introduced AIP Bootcamps. In these immersive, hands-on-keyboard sessions, participants can expect to go from zero to use case in just one to five days.\nThe results speak for themselves.\nPalantir Global\nLa eficacia y ética de la Inteligencia Artificial deben pasar de lo fatuo a lo operacional\nPalantir et la Souveraineté Numérique en France: Préserver l’autonomie dans un monde en mutation\nPalantir und Digitale Souveränität: Bewahrung der Handlungsfähigkeit in einer Welt in Bewegung\nProtección de Datos en Palantir Foundry: Un enfoque íntegro de la privacidad y la gobernanza"
    },
    {
        "Ticker": "PLTR",
        "Company": "Palantir Technologies",
        "Source_URL": "https://www.palantir.com/platforms/foundry/",
        "Content_Type": "Website Content",
        "Raw_Text": "Gartner Names Palantir a Visionary in the Magic Quadrant for Data Integration Tools\n— Gartner MQ™, Q3 2022\nThe Foundry Ontology is the heart of Palantir Foundry. It integrates the semantic, kinetic, and dynamic elements of your business — empowering your teams to harmonize and automate decision-making in complex settings.\nExecute faster with universal logic representing the objects, actions, and processes of your business.\nActivate the Power of Your Data and Analytics.\nWeave your data and analytics directly into the daily decision-making happening across your core business and operational teams. Capture decisions for continuous learning.\nCollaboration, Supercharged.\nAchieve real-time collaboration between your data, analytics, and operational teams. Integrate decision-making in a common logic layer, driving meaningful action as conditions evolve.\nThe Best Way to Build.\nCompose AI-powered workflows on a scalable architecture that re-uses your ontology’s multi-modal objects, actions, and processes.\nNo Duplication.\nIncorporate data and models from your enterprise architecture — without duplicating the underlying assets or fracturing existing sources of truth.\nFor more than a decade, we’ve embedded alongside our customers to build Foundry backwards, starting from the most critical operational decisions. We’ve encoded this tradecraft into our product.\nToday, some of the world’s most important institutions use Foundry to build safer cars, secure global supply chains, accelerate cancer research, and more.\nPalantir and Jacobs Smart Algorithms unlocked 20% plant-wide power savings, eliminated operational fines, and reduced greenhouse gas emissions — all at an already-optimized water treatment plant.\nSonnedix has deployed Palantir Foundry at their Sonnedix Talayuela solar plant in Spain — and several other solar energy sites — to radically digitize production processes and reduce plant downtime\nUsing Foundry, PG&E has developed a model that combines equipment health data, geospatial location, and network topology to help predict when the utility should conduct preventive maintenance on distribution transformers.\nThe utilities industry has a highly complex IT/OT landscape. In his 2023 FoundryCon keynote, Todd Inlander, CIO at Southern California Edison (SCE), discussed the value of digital transformation across the industry. At SCE, Palantir Foundry powers workflows that help with wildfire prevention, customer notifications, emergency response, and daily meteorology analysis, leveraging large-scale ML-driven weather forecasting, AI-enabled inspections, and IoT sensors.\nSkywise leverages AI to analyze disparate data sources, offering new insights for the entire aviation industry. With over 25,000 users, Skywise provides airlines with an AI OS to address Aircraft Operations Challenges.\nTrafigura leverages Palantir Foundry to power their Supply Chain Carbon Emissions Platform, with a consortium approach that enables participants across global energy & commodities supply chains to model lifecycle carbon intensities and allows industry participants to collaborate for enhanced visibility and reporting.\nAthinia™ provides a secure data analytics platform for collaborating on relevant information from participants across the semiconductor industry that can help materials suppliers and device-makers uncover novel insights that improve semiconductor manufacturing by using AI and ML.\nNCATS securely aggregates and intelligently harmonizes de-identified data from thousands of hospitals and clinics to power advanced research.\nIn this paper, machine learning researchers were able to leverage the collective data to effectively identify patients suffering from \"Long COVID,\" despite the absence of accurate diagnosis coding. N3C closely governs data usage and access for this and hundreds of other projects via Purpose-based Access Controls (PBAC).\nConcordance Healthcare Solutions is using Palantir Foundry to build an ecosystem that can disrupt this fragmented network.\nCastrol partners with Palantir to smooth supply chain shocks.\n\"The combination of flexibility and velocity enables us to move ahead of the disruption and get back from crisis situation into business as usual really quickly, and that’s important — especially in difficult times when resources are limited.\" — Nicola Buck, SVP Marketing at bp and CMO at Castrol\nCardinal Health partners with Palantir to securely integrate clinical data and access dynamic purchase decision insights.\nFoundry’s AI/ML capabilities empower health care systems by securely integrating and modeling diagnosis, clinical, and purchasing data, resulting in a clinically-integrated supply chain solution that can respond to changes in real time.\nSarcos is pushing the future of AI and robotics by combining the strength of Sarcos robotic systems with the edge capabilities of Palantir Foundry AI OS.\nPalantir partners with Doosan Infracore to drive the heavy machinery manufacturer’s digital transformation and deliver high-quality, market-relevant products to its customers.\nCleveland Clinic today leverages Palantir's Virtual Command Center to help improve care delivery across all avenues of operations for the hospital, including streamlining bed assignment and discharge management to reduce wait times and reduce length of stay; optimizing staff allocation to provide more balanced care; and increasing the utilization of critical resources such as operating room block time.\nPalantir is enabling Tampa General to take their organization to the leading edge of innovation in healthcare by integrating key data sources and applying advanced analytics in Foundry to deliver insights to caregivers and decision-makers across the health system — from improving operations to enhancing clinical care to accelerating research insight generation.\nSOMPO's Real Data Platform — powered by Foundry — provides a secure environment for shift workers to view and manage tenant care data, including tailored sensor alerts that measure critical vital signs. Now, data gathering to make care plans that once took 30 minutes can be made at a glance.\nFinancial organizations use AI to accelerate transaction monitoring and improve risk analysis & compliance.\nTransaction monitoring is a critical line of defense against financial crimes & noncompliance. Foundry’s AI-powered transaction monitoring has enabled one global bank to resolve alerts 60% faster at 90% lower cost, and to perform multi-jurisdiction client searches 90% faster with increased consistency.\nSwiss Re uses Foundry to connect data repositories into a single analytical data foundation and gain advanced insights into risk while maintaining strong data security & governance. With this visibility and the ability to build sophisticated tools within the platform, Swiss Re is improving the resilience of their clients and society at large.\nA major bank turned to Foundry to power Next Best Offer (NBO) Marketing initiatives, and accelerated the time needed to launch a campaign from months to a single day.\n“Palantir is rooted in building data-driven intelligence applications for complex, high-value government and commercial use cases.”\n— The Forrester Wave™: AI/ML Platforms, Q3 2022\n“Palantir has demonstrated its clear leadership in AI on a global scale from both a market share and revenue perspective...we’re excited to see how Palantir continues to separate itself from the competition by solving the toughest business challenges out there with their platforms.”\n— Ritu Jyoti, IDC’s Group VP, AI and Automation\n— Gartner MQ™, Q3 2022"
    },
    {
        "Ticker": "PLTR",
        "Company": "Palantir Technologies",
        "Source_URL": "https://www.palantir.com/mission-manager/",
        "Content_Type": "Website Content",
        "Raw_Text": "Mission Manager\nHelping government agencies securely expedite software vendor onboarding, deployment, and management.\nThe Ecosystem\nQuickly onboard innovative, traditional, and non-traditional software vendors into Palantir's secure Kubernetes-based ecosystem.\nGovernment administrators retain complete oversight and management of all deployed services and software versions.\nTailored to Your Program\nMission Manager offers the flexibility to adapt to the specific needs of your government program.\nReach out to us for more information on how we can help meet your mission requirements.\nThe Benefits\nCyber Security\nEnhance Security and Efficiency Across Your Operations\nAutomated Cybersecurity and Risk Management: Streamline your manual approval processes and ensure robust, automated security controls.\nRelieve ISSO and AO Burden: Minimize the effort and steps in onboarding new applications.\nAccelerate Delivery: Expedite mission-critical deployments.\nZero-Trust Multitenant Compute: Ensure robust security across multiple tenants.\nContinuous Monitoring and Delivery\nAchieve Real-Time Insights and Operational Efficiency\nApollo Insights: Gain real-time visibility into CVEs, STIGs, and more.\nRapid Onboarding and Offboarding: Efficiently manage vendor workloads.\nFaster High-Side Delivery: Enable application availability in mission-critical environments in a matter of days.\nComplete Oversight and Transparency: Maintain full visibility into deployments and risk footprint for operational readiness and informed decision-making.\nDistributed Data Architecture\nEnhancing Data Integration and Edge Capabilities\nOntology / SDK: Leverage a comprehensive ontology and SDK for enhanced data management.\nPeering and Federation: Extend capabilities seamlessly to the edge.\nImmediate Connectivity to Existing Data Layers: Ensure seamless integration with your current data infrastructure.\nCentralized Identity Management and Data Security\nStreamlined Identity and Access Protection Across Your Organization\nMultipass for ICAM: Implement robust identity and access management using Multipass.\nOIDC Integration: Seamless vendor integration with existing IdPs through OIDC.\nCommon Data Access: Simplify and secure data access across your organization.\nAdvanced Access Control: Utilize Attribute-Based Access Control (ABAC) and Role-Based Access Control (RBAC) to secure data.\nDeploy Everywhere, Operational Today\nVersatile and Secure Deployment for Immediate Operational Readiness\nUniversal Deployment: Compatible with all cloud providers and on-prem environments.\nBroad Security Compliance: Meets stringent requirements such as FedRAMP High, IL5, IL6, and higher network ATOs.\nProven in Operations: Trusted by agencies like CDAO, SOCOM, and Army Futures Command.\nOpen Standards\nLeveraging Open Standards for Seamless Integration and Operation\nKubernetes: Optimize operations with container orchestration.\nHelm Charts: Simplify Kubernetes deployments.\nREST APIs: Seamless integration with existing systems.\nOIDC: Streamline identity management with OpenID Connect.\nOAuth: Secure authorization for your applications."
    },
    {
        "Ticker": "PLTR",
        "Company": "Palantir Technologies",
        "Source_URL": "https://www.palantir.com/docs/",
        "Content_Type": "Website Content",
        "Raw_Text": "By clicking “Accept All Cookies”, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts. More Info\nPrivacy Preference Center\nYour Privacy\nStrictly Necessary Cookies\nTargeting Cookies\nYour Privacy\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.\nMore information\nStrictly Necessary Cookies\nAlways Active\nThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nTargeting Cookies\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising."
    },
    {
        "Ticker": "PLTR",
        "Company": "Palantir Technologies",
        "Source_URL": "https://www.palantir.com/pcl/",
        "Content_Type": "Website Content",
        "Raw_Text": "Privacy & Civil Liberties Engineering\nPalantir is a mission-focused company.\nOur team is dedicated to working for the common good and doing what's right, in addition to being deeply passionate about building great software and a successful company.\nPalantir was founded on the conviction that it's essential to preserve fundamental principles of privacy and civil liberties while using data.\nOur earliest work in counter-terrorism required us to ask whether we could meaningfully strengthen national security in the US without weakening constitutional privacy protections. In response, we invested financial and intellectual capital to build technology that is now trusted by the world's most stringent — and skeptical — data protection regimes.\nOur culture of open and critical discussion around the implications of our technology ensures that we remain true to that initial conviction, even as the nature of data and the environments where we operate evolve.\nFrom their first day, new Palantirians are trained to ask: \"Do I want to live in the kind of world that the technology we're building would enable?\"\nTo help our engineers and business leaders answer this question affirmatively, we:\nAdhere to a set of principles that guide our technical and business decisions.\nInvest in building technology that promotes responsible and effective data and AI usage.\nEmploy and empower a team of Privacy and Civil Liberties Engineers.\nEngage with independent experts in privacy law, policy, and ethics.\n↳ Technologies\nWe build software platforms that help our customers integrate and analyze their own data in ways that are consistent with legal and ethical considerations. We build products across three focus areas: core data governance, responsible AI, and civilian protection in defense applications."
    },
    {
        "Ticker": "PLTR",
        "Company": "Palantir Technologies",
        "Source_URL": "https://www.palantir.com/palantir-explained/",
        "Content_Type": "Website Content",
        "Raw_Text": "By clicking “Accept All Cookies”, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts. More Info\nPrivacy Preference Center\nYour Privacy\nStrictly Necessary Cookies\nTargeting Cookies\nYour Privacy\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.\nMore information\nStrictly Necessary Cookies\nAlways Active\nThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nTargeting Cookies\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising."
    },
    {
        "Ticker": "PLTR",
        "Company": "Palantir Technologies",
        "Source_URL": "https://www.palantir.com/platforms/foundry/foundry-ontology/",
        "Content_Type": "Website Content",
        "Raw_Text": "One Ontology. Three Layers of Capability.\nHigh-fidelity objects. Real-time actions. Multi-step simulations. Foundry Ontology is a groundbreaking new way to interact with your business.\nSemantic Layer\nDynamic Objects & Links\nIntegrate disparate data and model sources into a real-time, interactive view of the “nouns” of your business — including its objects, entities, relationships, and events.\nMulti-Modal Properties\nGenerate object properties from models, structured data, streaming data, geospatial data, and any other data or model source. Configure model-derived properties for even richer semantic detail.\nOntology Primitives\nRapidly configure the properties, behaviors, and interdependencies of common real-world concepts — such as Scheduling — with pre-defined configuration patterns. Primitives let you set up high-fidelity objects in a matter of clicks, using a low-code interface.\nKinetic Layer\nAI-Driven Actions & Functions\nRepresent the behavior of your business in a real-time kinetic graph — from actions made in transactional systems to those tied to models in industry-specific tools. Foundry Ontology links actions to your semantic objects, forming the basis for AI-guided operations.\nProcess Mining & Automation\nMine actions and processes, reveal hidden action flows and inefficiencies, and quantify the business impact of changes. Monitor processes in real time and bind models from the ontology to your processes for continuous optimization.\nAction Orchestration\nExecute actions across the systems that run your enterprise in a stable, governed way by assigning writeback procedures to kinetic actions. Foundry Ontology mediates changes to data and models in external environments, including across edge, tasking, and transactional systems for real-time business process workflows.\nReal-Time Monitoring\nEmpower your non-technical teams to monitor your ontology’s semantic and kinetic elements. Low-code tooling makes it easy to author rules on objects, actions, and processes — including on objects with billions or trillions of streaming data points for real-time process monitoring and alerting.\nDynamic Layer\nAI-Guided Decisions\nBind models to your objects and actions to guide decisions and automate processes. Models can reason across both the semantic and kinetic variables of your business, allowing them to compute globally optimal recommendations.\nMulti-step Simulations\nCreate a live link between strategy and operations by simulating decisions across a range of metrics — such as profitability, production, or customer value. Simulations can be branched, chained across underlying models, and iterated on, allowing your teams to explore all possibilities in response to new events and changing conditions.\nDecision Capture & Learning\nCapture decisions made within workflows and in response to simulations as new data, with complete lineage. Programmatically feed decision data back to AI/ML, closing the loop between operations and analytics and improving the predictive power of your simulations.\nOntology Wielding & Hydration\nOntology Wielding\nLeverage your ontology’s semantic and kinetic graphs to simulate operations, automate processes, and federate decision-making across operators, devices, and environments.\nOntology APIs\nLeverage a full range of APIs that read from and enrich the Ontology to build your own custom apps or fuel third-party tools.\nAutomated API generation that can both read from and write to the Ontology means you can build powerful applications that adapt to shifting needs.\nApplication Building\nFor novel applications and use cases, Foundry comes with app building capabilities for users from all backgrounds.\nWorkshop empowers a broad range of users to create Ontology-aware applications with flexible building blocks from no-code, low-code, and fully customizable code-based widgets. Leverage the full breadth of your Ontology’s objects and actions for deep interactivity, a tailored user experience, and operational efficacy.\nSlate allows app builders to construct dynamic, bespoke applications at speed, allowing for full customization using HTML, CSS, and JavaScript. Seamless integration with the Ontology allows for deep visibility into the relationships and interdependencies between Objects and Actions, helping you build custom applications that fully fit any unmet operational need.\nObject-Aware Analytics\nGroundbreaking analytic applications in Foundry are fully object-driven, allowing non-technical users to self-serve in the language of the business for even the most technically demanding workflows.\nQuiver enables you to quickly visualize and analyze time series and object data through a visual point-and-click interface and a powerful charting library. Without writing any code, you can build and publish analyses and share them with your colleagues to keep everyone on the same page.\nContour allows for ad-hoc data analysis on large volume datasets, such as performing joins on 100,000 objects or aggregations on 50,000 rows. Analyze your data in a point-and-click interface and build interactive and sharable dashboards to further explore this data — all without writing any code.\nObject Explorer makes it easy to drill down into specific objects in the Ontology, both individually and in aggregate. Compare and contrast object sets, perform bulk actions, open them in compatible apps (like Quiver), and export them.\nFusion is Foundry’s built-in spreadsheet application. Directly query Foundry datasets and use standard spreadsheet functions to analyze this data. Leverage Fusion’s writeback capabilities to submit results back into Foundry for downstream usage.\nVertex allows you to visualize and quantify cause and effect across the digital twin of your real-world organization. Build, curate, and publish graphs for use across your organization or build new system graphs.\nFoundry Marketplace Developer Suite\nBuild and launch data products as SaaS.\nFoundry Marketplace Developer Suite enables organizations to rapidly develop and deploy packages of data-backed workflows built in Foundry to a consumer-facing storefront, with integrated CI/CD, multi-layer security for granular controls and strict governance across customer workspaces, and easy discoverability for internal and external consumers to install published data products.\nProcess Mining & Automation Suite\nRevolutionize processes with the Ontology.\nProcess Mining & Automation Suite allows users to rapidly mine and transform their processes, unlocking the ability to drill down into process inefficiencies, perform root cause analysis, evaluate alternatives with ML/algorithm-agnostic forecasting and simulation, and drive change with writeback and orchestration across disparate systems.\nIndustry Applications\nOut of the box, leverage a library of object-aware applications built to serve the needs of different industries, including:\nOntology Hydration\nCombine data, models, and transactional systems with intuitive tooling built to empower technical and non-technical users alike.\nPipeline Builder\nFoundry’s flagship Ontology Hydration capability, for robust scalable pipelines at speed.\nBuild production-grade pipelines with Pipeline Builder using a dynamic point-and-click interface. Start by defining endpoint schema for Ontology object types and properties and describing the pipeline to match inputs to endpoints. Pipeline Builder’s back end will then automatically write transform code to build the desired pipelines.\nPipeline Builder works with all types of data (structured, unstructured, IoT, geospatial et al.) across all modalities (batch, micro-batch, streaming, et al.).\nNative Federation\nIncorporate existing datasets in external systems into objects — without duplicating the underlying data.\nIntegrate directly with data platforms, streamlining the hydration of existing data assets into the Ontology.\nFederation ensures that the Ontology can harness the full power of your enterprise architecture, without fracturing existing sources of truth.\nModel Hydration\nCapture the kinetics of your organization. Interoperable model connectors, integrations for externally hosted models, and built-in tooling make model hydration seamless.\nBring your externally developed models into Foundry with a suite of interoperable connectors, making them easily consumable by other users, pipelines, and operational applications.\nBuilt-in tooling streamlines the process of binding models to Ontology Objects, Actions, and Processes.\nThe platform includes full versioning, security, and lineage for integrated models — as well as model management, evaluation, and deployment via Model Objectives."
    },
    {
        "Ticker": "PLTR",
        "Company": "Palantir Technologies",
        "Source_URL": "https://www.palantir.com/access-insights-palantir-overview/",
        "Content_Type": "Website Content",
        "Raw_Text": "When you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.\nMore information\nThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising."
    },
    {
        "Ticker": "PLTR",
        "Company": "Palantir Technologies",
        "Source_URL": "https://www.palantir.com/impact/supply-chain-emissions/",
        "Content_Type": "Website Content",
        "Raw_Text": "Trafigura + Palantir\nTo act on emissions, you first need visibility of them across your supply chains.\nBoth are possible with Foundry.\nA Consortium Approach\n- Partner\n- Trafigura\n- Problem Space\nCommodities\nAs the world transitions towards a net-zero economy, increasing accountability for emissions demands that organizations have visibility of where improvements can be made across their global supply chains.\n- Solution\nTrafigura adopted Palantir Foundry to create a Global Carbon Data Consortium platform. This will enable them to integrate data from across supply chains and work with their partners to simulate and implement lower emissions pathways across the commodities lifecycle.\n- Problems Solved\n↳ Integration\n↳ Supply chain visibility\n↳ Decision-orchestration\nInformed and data-driven decision-making\nDetermine product-level carbon intensity\nReplicate real-world supply chains by drawing on primary and industry data\nOptimize for carbon intensity\nSimulate an unlimited number of alternative supply chains to assess their carbon intensities"
    },
    {
        "Ticker": "PLTR",
        "Company": "Palantir Technologies",
        "Source_URL": "https://www.palantir.com/impact/sompo/",
        "Content_Type": "Website Content",
        "Raw_Text": "SOMPO's Real Data Platform\nPowered by Palantir Foundry, SOMPO's operating system is designed to increase security, health, and wellbeing. The RDP drives impact across four key pillars\nWellbeing\nImproving elderly care in Japan and abroad by customizing care plans & streamlining the supply chain\nResilience\nMaking Japan and other countries more resilient by integrating insurance with real-world operations\nIntegrated Internet of Things (IoT)\nIncorporating and safely connecting data across devices, workflows, and industries\nNational Healthcare Infrastructure\nCreating a connected healthcare infrastructure that enables secure collaboration between research, government, and private industry in Japan and abroad\nImproving Elder Care in Japan\nProblem Space\nWith 60,000 elder care facilities in Japan, a country that leads in global population longevity, inefficiencies amount to approximately $35 billion, a drain on social cost, care facilities, and impacting quality of care.\nSOMPO Care sought to minimize costly inefficiencies by integrating hundreds of different data types stored in disparate systems to empower care decisions, informed by data and cutting-edge technology.\nImpact\nThe RDP provides a secure environment for shift workers to view and manage tenant care data - including tailored sensor alerts that measure critical vital signs like sleep, breathing, and pulse rate- using a mobile device, enabling real-time log changes. Data gathering to make care plans that once took 30 minutes can now be made at a glance.\n- Partner\n- SOMPO Care\n- Problems Solved\n↳ Utilize a single source of truth\n↳ Harness the power of sensor data\n↳ Move from reactive to proactive with tailored alerts\n↳ Drive social change by increasing access to cutting edge technology\n- Testimonial\n\"We had invested in various operational systems and sensor technologies, but the data was siloed in many different places and not fully utilized. That’s when we came across Foundry.\"\n↳ Satoshi Kasai, CEO of Nursing Care and Seniors Business, SOMPO"
    },
    {
        "Ticker": "PLTR",
        "Company": "Palantir Technologies",
        "Source_URL": "https://www.palantir.com/impact/swiss-re/",
        "Content_Type": "Website Content",
        "Raw_Text": "Swiss Re + Palantir\nIdentifying and mitigating risks for a more resilient society.\nA world of increasingly interconnected risks\nAs one of the world’s leading reinsurers, Swiss Re has been in the business of understanding risk for over 150 years. Amidst global challenges such as climate change, cyberattacks, and population increases, the company deals with more interconnected risks than ever on its mission to make the world more resilient.\nTo maintain agility and enable real-time, data-driven decisions in a world where unprecedented events have become the norm, Swiss Re needed a central, unified technical foundation from which to manage complex data at scale, identify connections between seemingly disparate trends and events, and operationalize these insights for their business and their clients, all in a manner compliant with applicable laws, rules, and regulations.\nA single analytical data foundation for managing risk\nToday, Swiss Re uses Foundry to connect their organizations and business functions to a single analytical data foundation, all in a manner compliant with applicable laws, rules, and regulations. This allows them to leverage their data at even the most granular level, enabling advanced insights into the array of risks the company and its clients face, from COVID-19 to supply chain disruptions.\nWith a better understanding of risk exposures and the ability to build sophisticated tools with Foundry, Swiss Re is improving the resilience of their clients and society at large. For example, using Foundry, Swiss Re has developed a climate risk score tool, which leverages troves of data and computing power to determine the potential impacts of climate change on a given portfolio. Swiss Re deploys this tool both for their own business and for their clients, enhancing their respective understanding of the perils of a warming planet and empowering them to mitigate — and potentially prevent — the associated impacts.\nCrucially, Foundry also represents a solution that meets the stringent data security and governance needs of Swiss Re and its clients. In the highly regulated, heavily scrutinized insurance space, deploying a platform that enabled Swiss Re to fulfill its complex regulatory and contractual obligations was of paramount importance, and Foundry was up to the challenge.\nImpact\nAnalytical Data Models (ADMs) provide a single version of truth to experts, actuaries, and portfolio managers as they sift through millions of multi-dimensional data points\nOver 200 projects leverage in-platform data to drive business value and increase efficiency, using more than six petabytes of data\nMore than one out of every three employees is an active user of Foundry, including every second underwriter\nFoundry allows Swiss Re to cut the time to produce reports and complex analytics from days to minutes"
    },
    {
        "Ticker": "PLTR",
        "Company": "Palantir Technologies",
        "Source_URL": "https://www.palantir.com/docs/foundry/aip/overview/",
        "Content_Type": "Website Content",
        "Raw_Text": "Palantir's Artificial Intelligence Platform (AIP) connects AI with your data and operations. AIP is designed to drive automation across operational processes, providing a comprehensive suite of tools that can be used by everyone in an organization, from developers to frontline users.\nAIP's builder tools like AIP Logic, AIP Agent Studio, and AIP Evals enable the development of production-ready AI-powered workflows, agents, and functions on top of the Ontology and developer toolchain. Additionally, AIP transforms the application environment by allowing sandboxed, autoscaling applications to integrate generative AI seamlessly within existing security, audit, and resource management frameworks.\nTogether with Foundry (Palantir's data operations platform) and Apollo (Palantir's mission control for autonomous software deployment), AIP forms an operating system that can deliver a full range of AI-driven products, from LLM-powered web applications to mobile applications using vision-language models to edge applications that embed localized AI.\nThe remainder of this page provides a brief overview of key AIP benefits. For more details about AIP's capabilities, we recommend reviewing the AIP documentation, including the AIP application reference.\nPalantir AIP can integrate seamlessly with your organization's existing data on a Foundry enrollment. This enables you to build and interact with LLM-powered agents and workflows that leverage data from a wide array of data sources and formats.\nAIP incorporates all of Palantir's advanced security measures for the protection of sensitive data in compliance with industry regulations. AIP provides robust access controls, encryption, and auditing capabilities to maintain data integrity and transparency. Moreover, built-in governance tools help organizations maintain accountability and historical lineage in AI operations.\nTo learn more about how LLMs securely process user prompts in the platform, review FAQs: Security and Privacy of Palantir's AIP leveraging third-party-hosted LLMs ↗ by selecting Palantir AIP FAQs.\nAIP provides a comprehensive suite of tools for building, training, and deploying large language models. Supporting a range of different large language models allows data scientists and engineers to work with their preferred tools and pick the models that are best suited for each use case. Additionally, AIP offers version control and collaboration features, enabling teams to manage models efficiently throughout their lifecycle.\nDesigned to handle large-scale data operations, AIP ensures that AI models can be deployed and scaled according to organizational needs. The platform's architecture supports distributed computing, allowing for high-performance processing and real-time analytics, which are crucial for mission-critical applications. AIP also provides granular control of resource use and limitation setting.\nTo learn more about the tools that enable developers to monitor and optimize the performance of the agents and applications that are built with the platform, review the AIP Observability documentation.\nTrust is critical when building AI workflows for production deployment; for LLMs, trust comes from explainability and transparency, as well as from rigorous evaluations. AIP provides tools for generating detailed audit trails, explanations, and evaluations of model decisions, helping users understand and trust the outcomes. This trust is vital for organizations to safely deploy AI in real-world situations and make informed and ethical decisions based on AI insights.\nNote: AIP feature availability is subject to change and may differ between customers.\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.\nMore information\nThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising."
    },
    {
        "Ticker": "PLTR",
        "Company": "Palantir Technologies",
        "Source_URL": "https://www.palantir.com/docs/foundry/devops/overview/",
        "Content_Type": "Website Content",
        "Raw_Text": "Foundry DevOps enables you to rapidly develop and deploy packages of data-backed workflows built in Foundry, leveraging your organization’s ontology, AI models, pipelines, data connections, or end-to-end use cases.\nKey features of Foundry DevOps include:\nFoundry DevOps is a good fit for use cases such as:\nLearn how to create products and manage products.\nThe Marketplace storefront facilitates easy discoverability to install published data products.\nKey features of Marketplace include:\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.\nMore information\nThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising."
    },
    {
        "Ticker": "PLTR",
        "Company": "Palantir Technologies",
        "Source_URL": "https://www.palantir.com/docs/foundry/announcements/",
        "Content_Type": "Website Content",
        "Raw_Text": "REMINDER: Sign up for the Foundry Newsletter to receive a summary of new products, features, and improvements across the platform directly to your inbox. For more information on how to subscribe, see the Foundry Newsletter and Product Feedback channels announcement.\nShare your thoughts about these announcements in our Developer Community Forum ↗.\nDate published: 2025-11-11\nJupyterLab® and RStudio® Code Workspaces now provide an AIP agent accessible from a workspace's sidebar, enabling access to any of AIP's supported large language models (LLMs) to help you develop and deploy code in Foundry based on your specific use case. This experimental feature is available for all Foundry enrollments with AIP enabled.\nAn AIP agent helps you write code and generate visualizations in JupyterLab® and RStudio® Code Workspaces.\nTo get started, open your workspace, select the </> icon at the bottom of the left sidebar, and enter a prompt in the Ask a question... text box to initiate the agent. The agent will provide coding guidance or generate complete files for you based on its available tools. To configure the tools an agent can access to help it perform essential operations in your workspace, select the wrench icon to render all available Tools and opt the agent out of those which are not relevant for your use case. Agents can perform a wide range of tasks through its tools, such as author files, write and run code snippets, search for and install libraries, or execute terminal commands.\nConfigure the tools available to the AIP agent in your workspace.\nUse the agent's Settings menu to rename conversation threads and view the system prompt. The agent will not persist your chat history after you shut it down or restart the workspace, so make sure to sync any code or model outputs you want to save before ending your session.\nYou can rename chat threads using the agent's Settings menu.\nYou can alternate the LLM your agent uses by selecting the name of the current model from the bottom of the prompt text box. Model behavior may vary across providers, so you can experiment with different models to find the approach that works best for your specific use case. Learn more about prompt engineering best practices.\nYou can use a workspace's AIP agent to:\nWe will continue to refine the agent's capabilities and expand its toolkit as we gather feedback during its initial experimental release. Additionally, support for writing Foundry models will be available in the coming weeks.\nJupyter®, JupyterLab®, and the Jupyter® logos are trademarks or registered trademarks of NumFOCUS. RStudio® is a trademark of Posit™. All third-party trademarks (including logos and icons) referenced remain the property of their respective owners. No affiliation or endorsement is implied.\nDate published: 2025-11-11\nThe new Machinery widget, an analysis and real-time monitoring tool that provides operational insights for your configured Machinery processes, is available on all enrollments the week of November 10. This new capability enables teams to visualize process flows, track key metrics, and identify performance issues without requiring additional configuration beyond your existing Machinery setup.\nThe new Machinery widget at a glance.\nThe new Machinery widget natively supports multi-process graphs, allowing you to track metrics across multi-object-type process implementations. The widget is available in Workshop modules or as a stand-alone view in the Machinery application with limited features.\nConfiguration is streamlined through automatic derivation of subprocess object sets using search arounds from parent processes. This means that you only need to configure one object input for each root process. If you have an application process with many linked review subprocesses, you can provide 100 application objects, and all related child objects will be automatically identified through configured link types.\nFour metric views are preconfigured and can be customized by application builders; historical count, current count, historical duration, and current duration. Application builders can also add custom metric views to suit specific analytical needs. Users can switch between these views, hover over nodes to reveal all available metrics, and pin specific nodes for continuous monitoring across the graph visualization.\nThe new Machinery widget optimizes space usage using contextual zoom. When zoomed out, it will show many graph elements, but only a single metric. When zoomed in, nodes reveal additional information and metric cards show up to three available metrics.\nContextual zoom reveals additional information and metrics.\nTwo analysis modes enable process investigation beyond visualization. Path explorer analyzes individual process paths and their frequency distribution, allowing you to select specific paths to filter outputs and understand exactly how objects flow through your workflow.\nAnalyze individual process paths and their frequency distribution with path explorer mode.\nDuration distribution identifies performance outliers through the visualization of time spent in selected states across all objects. This allows the isolation of individual buckets or ranges of objects with undesirable behavior, such as spending excessive time in particular transitions or states. Both analysis modes update output object sets dynamically, enabling iterative investigation of process performance issues.\nUse the duration distribution mode to identify outliers through visualization of time spent in states across selected objects.\nMultiple graph features adapt visualizations to different use cases where bottleneck identification is critical. Transition nodes simplify complex graphs by replacing actions and automations with implicit state transitions, providing a cleaner state-transition perspective. Additionally, subprocesses can be replaced with their implicit state transitions for visibility into transition metrics on the currently focused process.\nThe Machinery v2 widget automatically detects and removes objects that are deviating from the process definition, helping to remove noise from the performance analysis. Non-conforming objects can be made visible and explicitly included or isolated in the output. When visible, deviating states and transitions are visually highlighted, with metrics computed across all input objects rather than just conforming ones. This is valuable when investigating why certain processes deviate from expected patterns.\nWe want to hear about your experiences using Machinery and welcome your feedback. Share your thoughts with Palantir Support channels or on our Developer Community ↗ using the machinery\ntag ↗.\nDate published: 2025-11-11\nStarting the week of November 10, Workflow Builder will be rebranded as Workflow Lineage, better reflecting its role as an interactive workspace for visualizing, understanding, and managing application dependencies and their underlying processes.\nThe newly renamed Workflow Lineage home page.\nAll existing features and functionalities remain unchanged, and you can continue to use Workflow Lineage as usual. You should see the new name reflected across Foundry and platform communications. If you have any questions about this change, share them with Palantir Support channels or on our Developer Community ↗ .\nDate published: 2025-11-06\nTracing, logging, and run history views for functions, actions, automations, and language models are now available in Workflow Lineage for all users. Additionally, starting the week of November 10, all in-platform logs (including those from the Ontology and AIP workflows) can be exported to a real time streaming dataset, allowing for powerful custom analysis of your telemetry.\nOntology and AIP workflows now come out of the box with first-class tracing, logging, and run history views for all functions, actions, automations, and language models:\nTelemetry highlights include the following:\nTo start observing your Ontology and AIP workflows, follow the steps below:\nThe trace view for a function workflow execution.\nAs stated in the log permissions and configure logging documentation, users with the Information security officer or Enrollment administrator role can manage the Log observability settings for an organization in Control Panel.\nLet us know what you think about our new observability capabilities for Ontology and AIP workflows. Contact our Palantir Support channels, or leave your feedback in our Developer Community ↗ .\nDate published: 2025-11-06\nPeer Manager enables you to view and monitor jobs associated with an established peering connection that synchronizes objects and links between Foundry enrollments in real-time as well as mediates changes made across ontologies. The application will be generally available across all enrollments the third week of November.\nPeering enables organizations to establish secure, real-time Ontology data synchronization across distinct Foundry enrollments. Peer Manager is the central home for administering peering in Foundry. From Peer Manager, space administrators can create peer connections, monitor peering jobs, and configure data to peer.\nAfter you create a peer connection, you can use Peer Manager's home page to garner information about your new connection and all other connections configured between your enrollment and other enrollments. Peer connections support the import and export of Foundry objects and their links as well as object sets configured in Object Explorer.\nThe Peer Manager home page provides an overview of all configured Peer Connections.\nSelect a connection to launch its Overview window, where you can track the health of each peer connection by viewing the status of individual peering jobs.\nPeer Manager's Overview window offers a unified view of the status and health of peering jobs within a connection.\nSelect Ontology from the top ribbon to peer objects across an established connection, where Peer Manager enables you to peer all or a selection of properties on the object.\nPeer Manager's Ontology window enables you to peer object types and their links across a peer connection.\nThe ability to configure Artifact peering will be available in Peer Manager by the end of 2025. Contact Palantir Support with questions about peering or Peer Manager on your enrollment.\nDate published: 2025-11-06\nPipeline Builder now offers the ability to create external pipelines using third-party compute engines, with Databricks as the first supported provider. This capability is in beta.\nExternal pipelines require virtual table inputs and outputs from the same source as your compute. When using external pipelines, compute is orchestrated by Foundry and pushed down to the source system for execution.\nFoundry’s external compute orchestration provides you with the flexibility to choose the most appropriate technology for your workload, use case, and architecture requirements. Pipelines built with external compute can also be composed together with Foundry-native compute pipelines using Foundry’s scheduling tools, allowing you to easily orchestrate complex multi-technology pipelines using the exact right compute at every step along the way.\nWith this improvement, you can now push down compute to Databricks using either code-based Python transforms or point-and-click Pipeline Builder boards. Learn more about creating external pipelines in Pipeline Builder.\nEnabling push down compute in Pipeline Builder.\nExternal pipeline with pushdown compute in Pipeline Builder.\nAs we continue to add features to Pipeline Builder, we want to hear about your experiences and welcome your feedback. Share your thoughts with Palantir Support channels or our Developer Community ↗ using the pipeline-builder tag ↗.\nDate published: 2025-11-06\nIceberg ↗ and Delta ↗ tables can now be imported as virtual tables into JupyterLab® code workspaces, providing more flexibility when working with externally stored data at large scales. Delta and Iceberg tables are open source table formats that enable reliable, scalable, and efficient management of large datasets, including tables stored in Databricks.\nJupyterLab® code workspaces now support read and write capabilities for Iceberg and Delta tables, and provide table-specific code snippets in the Data panel to facilitate development.\nA highlighted code snippet in the Data panel.\nThis feature enables running interactive Python notebooks against data stored and cataloged externally to Foundry in Iceberg and Delta tables, supporting a wide range of data science, analytics, and machine learning workflows.\nLearn more about virtual tables and Code Workspaces.\nJupyter®, JupyterLab®, and the Jupyter® logos are trademarks or registered trademarks of NumFOCUS. All third-party trademarks (including logos and icons) referenced remain the property of their respective owners. No affiliation or endorsement is implied.\nDate published: 2025-11-04\nWidget sets created in Custom Widgets can now be included as content in Marketplace products.\nWhen you add a Workshop module that uses a widget set to a Marketplace product, the widget set is automatically packaged. Widget sets can also be manually packaged independently, allowing you to build Workshop modules on top of them.\nIf a widget set had Ontology API access enabled in the source environment, it will be installed with access disabled by default. After installation, you must manually enable Ontology API access on the widget set if needed.\nPublished Marketplace product containing a Workshop module that uses a widget set.\nAs we continue to develop new features for custom widgets, we want to hear about your experiences and welcome your feedback. Share your thoughts with Palantir Support channels or our Developer Community ↗ and use the custom-widgets ↗ tag.\nDate published: 2025-11-04\nDataset rollback is now available in Data Lineage, giving you greater control over your data pipelines. Whether you encounter an outage, errors in your pipeline logic, or unexpected upstream data, dataset rollback provides a fast, reliable way to revert your datasets to a stable state. In addition, you can now queue snapshots, allowing datasets to snapshot automatically on their next build.\nDataset rollbacks provide several key benefits:\nTo get started with dataset rollback, open your dataset in Data Lineage and select a previous successful transaction in the History tab. You can roll back your dataset to that transaction by selecting Roll back to transaction.\nThe Roll back to transaction option, listed in a selected transaction's Overview tab.\nTo queue a snapshot on your dataset's next build, open a dataset in Data Lineage and select Force snapshot In the History tab in the bottom panel.\nThe Force snapshot option in the History tab.\nNote that you will need to acknowledge that this action cannot be undone before proceeding.\nEditor\nrole can perform rollbacks to ensure secure operations.Dataset rollback allows you to build, experiment, and iterate on your pipelines with confidence; the ability to revert to a stable state is available whenever you need it.\nWe want to hear about your experience and welcome your feedback as we develop more features in Data Lineage. Share your thoughts with Palantir Support channels or on our Developer Community ↗ using the data-lineage\ntag ↗.\nDate published: 2025-11-04\nOntology Manager now offers an improved rebasing and conflict resolution experience that gives you greater flexibility and control when managing branch changes. You can now rebase at any point without creating a proposal, view changes from both Main\nand your branch simultaneously, and resolve merge conflicts using multiple approaches—either through the Conflicts tab in the Save dialog or directly in the Ontology Manager interface for conflict resolution. This enhanced workflow prevents situations where unresolvable errors block your progress. This feature is available the week of November 3 across all enrollments.\nWhile you introduce changes on your branch, Main\ncan also update with new changes made by others. Rebasing incorporates the latest changes from Main\ninto your current branch to keep it up to date.\nResolve merge conflicts by choosing between changes from Main\nor your current branch directly in Ontology Manager.\nDuring a rebase, Ontology Manager enters a new state where you can view and access changes from both Main\nand your branch. You may resolve merge conflicts by choosing between changes from Main\nor your current branch from the Conflicts tab in the save dialog. Alternatively, you can resolve conflicts by editing the ontology resource directly. This flexibility prevents situations where users become stuck due to unresolvable errors after conflict resolution.\nComplex cases of schema migrations or datasource replacements are not yet handled by this rebasing experience. Refer to the known limitations section of the documentation for an alternative solution. We are actively working to resolve these limitations.\nAs we continue to develop new features for Foundry Branching, we want to hear about your experiences and welcome your feedback. Share your thoughts with Palantir Support channels or our Developer Community ↗ and use the foundry-branching ↗ tag.\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.\nMore information\nThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising."
    },
    {
        "Ticker": "PLTR",
        "Company": "Palantir Technologies",
        "Source_URL": "https://www.palantir.com/palantir-is-not-a-data-company/",
        "Content_Type": "Website Content",
        "Raw_Text": "Palantir is Not a Data Company\n(Palantir Explained, #1)\nPublished: 11 November 2020\nEditor's Note: This is the first post in Palantir Explained, a series that explores a range of topics, including our approach to privacy, security, AI/ML safety, and more.\nPalantir has often been described as a secretive company. There is some truth to this. For many years, we primarily served institutions with exceptional confidentiality expectations in fields like defense and intelligence. We often had little choice but to remain silent about our work, even when misunderstandings about the nature of our business appeared in the media or in the public sphere.\nNow that we serve clients in a wider range of sectors, we have an opportunity to be more open. This is particularly true for sectors like healthcare, where Palantir’s software is used to process personal data. People have a right to understand how Palantir technology works, and how our customers use it.\nCommon misconceptions recur, particularly around the assumption that Palantir can use or transfer client data for its own purposes, or can join data from different clients together to sell on. This is not how we operate and never has been. In this blog post, we will explain more about the nature of Palantir’s business.\nThis blog post will be the first in a series, Palantir Explained, that will explore a range of topics, including our approach to privacy, security, AI/ML safety and more. We hope you find these blogs useful and informative; and we welcome feedback. If you find these blog posts inspiring and would like to join us in solving the challenges described, have a look at the opportunities we have available — we’d love to hear from you.\nPalantir’s Business Model\nPalantir is not a “data broker” or “data aggregator.”\nUnlike many tech companies, our business model is not based on the monetization of personal data. We do not collect, store, or sell personal data. We don’t use personal data to train proprietary AI or machine learning models to share or resell to other customers. We never facilitate the movement of data between clients, except where those specific clients have entered into an agreement with each other.\nPalantir is a software company.\nWe build digital infrastructure for data-driven operations and decision-making. Our products serve as the connective tissue between an organization’s data, its analytics capabilities, and operational execution. Palantir’s platforms tie these together by bringing the right data to the people who need it, allowing them to take data-driven decisions, conduct sophisticated analytics, and refine operations through feedback. We license this software to organizations, who receive secure and unique instances of our platforms in which to conduct their own work on their own data.\nThis infrastructure helps organizations bring the right data together at the right time to answer complex questions and make intelligent decisions. This is particularly valuable when existing systems are fragmented, and essential information is held in silos that can’t communicate with each other. Healthcare organizations, for instance, have used our software to tackle challenges like efficiently allocating PPE supplies when thousands of hospitals across the country have radically different and constantly changing levels of supply and demand for each item of PPE.\nOur software platforms allow organizations to better manage the data that they already lawfully control.\nThese platforms are used in some of the most sensitive and secure information environments in the world, necessitating world-class data protection and governance. As such, we have an exceptionally strong information security record.\nWith regards to customer data, we act as a data processor, not a data controller. Our software and services are used under direction from the organizations that license our products: these organizations define what can and cannot be done with their data; they control the Palantir accounts in which analysis is conducted; and any Palantir engineers that assist them in their work follow these directions. We do not and cannot reuse or transfer our clients’ data for our own purposes. Attempting to profit from customer data in this way would be illegal and would undermine the trust that is necessary to work in the sensitive environments in which we have built our business.\nTechnology companies should be scrutinized, especially when their technology is used by governments. To that end, we hope this series of blog posts will provide readers with more insight into our business and a deeper understanding of some of the key features of our technology. In our next post, we will discuss Purpose-Based Access Controls. Please feel free to reach out at media@palantir.com."
    },
    {
        "Ticker": "PLTR",
        "Company": "Palantir Technologies",
        "Source_URL": "https://www.palantir.com/palantir-is-still-not-a-data-company/",
        "Content_Type": "Website Content",
        "Raw_Text": "Palantir Is Still Not a Data Company\n(Palantir Explained, #7)\nPublished: 06 June 2025\nEditor’s Note: This is the seventh post in Palantir Explained, a series that explores a range of topics, including our approach to privacy, security, AI/ML safety, and more. In this post, we revisit some of the themes discussed in our first installment from 2020, addressing recurrent misconceptions about who we are and what we do. While the intervening years have witnessed many considerable disruptions, changes, and events, one point of consistency is that Palantir is still not a data company.\nWho We Are and What We Do\nPalantir makes software which organizations use to better manage their data, improve their operations, and serve the people who rely on them.\nWe’re proud that essential organizations — including those delivering life-saving assistance, improving health outcomes, manufacturing aircraft fleets, and securing and defending the West — depend on our software platforms to deliver their most vital mission outcomes and further institutional trust within the communities they serve.\nContrary to some media reports, we are not a surveillance company. We do not sell personal data of any kind. We don’t provide data-mining as a service. Palantir is a software company. Unlike many technology companies, our business model is not based on monetizing personal data. Instead, we develop and license software platforms that enable our customers to integrate and analyze their own data assets to make better decisions. Privacy and data security are fundamental to Palantir and have been built into the software’s architecture from the start.\nWe make digital infrastructure that enables organizations to operate in complex data environments. We help our customers — across the public, private, and non-profit sectors — overcome common challenges associated with fractured data landscapes, in which their data is split across different systems and formats.\nOur software provides our customers with the capabilities to integrate those sources into a common platform in which they can build more effective data management, analytics, and operations. Many of our customers also use our platforms to build or deploy AI tools to further enhance their operations in responsible, reliable, and impactful ways.\nPalantir has a deep and longstanding commitment to protecting privacy and civil liberties. We were the first company to establish a dedicated Privacy & Civil Liberties Engineering Team over a decade ago, and we have a longstanding Council of Advisors on Privacy & Civil Liberties comprised of leading experts and advocates. These functions sit at the heart of the company and help us to embody Palantir’s values both through providing rights-protective technologies and fostering a culture of responsibility around their development and use.\nTransparency and Engagement\nWe strive to be transparent about the security and privacy controls in the technologies that we design, build, and deploy, publishing blogs, videos, documentation, and engaging at events. Anyone can sign up for our AI platform, Palantir AIP, and test it out for themselves. In fact - we encourage this to see firsthand how our software works!\nPalantir’s software is built at every stage to uphold, not undermine, legal and regulatory protections as well as the ethics and standards that help institutions govern the appropriate uses of powerful technologies. A key part of this is oversight. Our software features, among other complementary capabilities, audit logs and transparent data flows, which enable regulators and governance bodies to ensure that the software has been used legally and responsibly.\nIn fact, it is the strength of these protections that has underpinned our success in the age of AI. These integral safeguards help to ensure that AI models can be used in safe, controlled, tested, and privacy-protective ways. Wherever possible, we strive to provide tools which exceed the protections that the law requires.\nOur Longstanding Support of the U.S. Government\nWe are extremely proud to support key American institutions. These include longstanding contracts with agencies like the IRS and NIH, where we have a history of excellence and the efficient delivery of essential digital infrastructure. Central to our mission is supporting liberal democratic societies and their allies by providing vital institutions with software that enables them to fulfill their responsibilities and maintain the trust of the citizens they serve — a commitment that transcends political shifts and administrations.\nEach of these customers gets a unique instance of our software, over which they have control. They define which data is integrated, who has access to it, and how it is used. Palantir does not have unfettered access to data our customer provide us. Nor does it share, transfer, monetize, or otherwise use such data for its own business purposes.\nIf any Palantir engineer has temporary access to data, this is at the explicit instruction of the customer, who grants individual access to their instance of the software, and assigns the permissions necessary to carry out discrete technical tasks. This access is then revoked once the project is complete.\nEnabling Secure Collaboration Between Organizations\nMany institutions — including U.S. federal government agencies — rely on our software because it helps them deliver effective and reliable outcomes. However, the fact that two agencies run on the same software does not imply generalized data sharing between them (just as the fact that two agencies both use spreadsheets does not allow them to access each others spreadsheets).\nWhere limited sharing or collaboration between agencies does need to take place, this is always directed by the agencies in question, for a specific purpose, explicitly in accordance with our legal obligations, and protected by deep policy and technical safeguards. This is done to better meet the service obligations of the people for whom these institutions are responsible.\nMisconceptions and Incorrect Reporting\nMisconceptions can arise because our products are complicated, used in a huge variety of industries (often with confidentiality requirements), and support nuanced and technical functions. This category of software is unfamiliar to the average person. However, it is absolutely possible to accurately describe it to people who are curious.\nThe relative complexity of software is not an excuse for critics to trade on people’s fears with false or misleading claims. These organizations have a responsibility to inform people and to allow them to make their own intelligent opinions. To support this, we are always willing to engage with the media and provide clarifications on any points of uncertainty - we just ask that these organizations seek the truth in good faith.\nData integration and the modernization of software is an essential process that every functional institution must go through. The effectiveness of our democratic institutions - and therefore the upholding of trust in democracy itself - depends on it. We’ve been helping key organizations do this for more than two decades.\nIf this mission excites you, join us! Apply here today: palantir.com/careers"
    },
    {
        "Ticker": "PLTR",
        "Company": "Palantir Technologies",
        "Source_URL": "https://www.palantir.com/docs/foundry/aip-observability/run-history/",
        "Content_Type": "Website Content",
        "Raw_Text": "To see the run history for a Function, Action or automation, navigate to the resource, then select the Run history tab. This provides a complete view of all executions over the past seven days.\nThe Run history table includes:\nThe run history displays executions from the past seven days, sorted by timestamp.\nYou can filter the results by:\nTo inspect a specific execution, select the View log details option to access the full trace and debugging information.\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.\nMore information\nThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising."
    },
    {
        "Ticker": "PLTR",
        "Company": "Palantir Technologies",
        "Source_URL": "https://www.palantir.com/docs/foundry/aip-observability/service-logs-and-debugging/",
        "Content_Type": "Website Content",
        "Raw_Text": "To access detailed logging information, navigate to the Details view after selecting the View log details option for a specific execution.\nThe service logs provide:\nINFO\n, WARN\n, ERROR\n, DEBUG\n, and TRACE\nmessages.To filter for specific log levels, use the log levels selector at the top of the table:\nAvailable log levels:\nTo see the full details of any log entry, select the Content field:\nEffective logging helps you debug issues quickly and understand your Function's behavior in production. Follow these best practices:\nWe recommend including identifiers and relevant data that can help you understand what has happened:\nCopied!1// TypeScript v1 example - Good logging practices 2console.log(\"Processing order\", orderId, \"for user\", userId); // Include relevant IDs 3console.log(\"Retrieved\", results.length, \"items from Ontology\"); // Include counts/metrics 4console.warn(\"Retry attempt\", attemptNumber, \"of\", maxRetries, \"for operation\", operationId); // Include retry context 5console.error(\"Failed to process order\", orderId, \"Error:\", error.message); // Include error details\nNever log sensitive information that could compromise security:\nCopied!1// ❌ Don't do this 2console.log(\"User credentials\", username, password); 3console.log(\"API response\", fullApiResponse); // May contain sensitive data 4 5// ✅ Do this instead 6console.log(\"Authentication attempt for user\", username); 7console.log(\"API call completed with status\", response.status);\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.\nMore information\nThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising."
    },
    {
        "Ticker": "PLTR",
        "Company": "Palantir Technologies",
        "Source_URL": "https://www.palantir.com/docs/foundry/marketplace/install-product/",
        "Content_Type": "Website Content",
        "Raw_Text": "When you have found a product you would like to install, Marketplace will guide you through the process of mapping any required inputs to create the product's output content.\nTo begin installing a product, select the blue Install button in the top right corner of the Marketplace interface. If you already have access to an existing installation, this button will instead prompt you to Open, though you can select Install again if you prefer to install the product again (for instance, if you would like to create a new version with different inputs). If the product version is recalled by the publisher, the Install button will be disabled.\nAn installation draft will then be created and you will be presented with a guided installation process.\nThe first part of the installation process allows you to configure the name and installation location of a product.\ntest\nto a product created as a test.Inputs are entities (such as objects or pipelines) relied upon by the content of a product. Inputs can be mapped to a product manually, using linked products, or by choosing an existing folder or legacy Foundry Template.\nThe Inputs overview page provides information about the inputs that are currently unmapped; these are referred to as \"missing inputs\".\nYou can choose content from an existing folder or a legacy Foundry Template to automap as inputs.\nAfter the Inputs overview page, you will have a chance to select resources to fulfill each required input.\nOn the page for each resource, the Configuration tab allows you to set up each input, for instance by specifying column mappings between the input and the product.\nThe Dependents tab provides information on why an input is needed by showing what content requires this input in order to install. For example, a Workshop application may require specific object types.\nTo deploy Marketplace products before all resources are available, you can create temporary stub resources in a specified folder to continue with the installation. Dataset inputs are currently supported, with additional input types expected to be supported in the near future.\nThe option to generate placeholder inputs will be visible on the Inputs overview page if there are any supported but not yet fulfilled inputs. Selecting Generate placeholder inputs will generate placeholder inputs for every suitable input.\nAlternatively, to generate a single placeholder input, select ... next to the input selector, then select Generate placeholder.\nOnce the actual resources become available, you can remap these inputs by navigating to the Installation page and selecting Edit.\nYou can remap any existing placeholder input with the actual resource by selecting Change.\nNot all products will require input mapping; for example, if a product provides only datasets as content, it may not need any inputs to be mapped. If input mapping is not required for your product, this step will be hidden.\nThe Content page provides a summary of all resources that will be installed, such as applications, functions, link types, and action types.\nThe Content page contains a toggle labeled Prefix ontology entities; similar to using an installation suffix to customize the name of the project in which your content will be installed, you can use the Prefix ontology entities toggle to customize the names of all object types, link types, and action types with a user-specified prefix (for example, DEV\n, which would prefix ontology entities with [DEV]\n).\nWhen installing a Production mode product, you will have an additional installation step to specify release channels and maintenance windows for automatic upgrades.\nRead more about these settings in upgrades.\nThe Review page surfaces any validation errors that must be resolved before installation. For instance, the installer might need to map a missing column in the clean dataset used as a required input before the installation will succeed.\nOnce all inputs have been mapped, you will be able to kick off an installation using the Install option. This submits the installation draft and a job starts running to create the resources. You will then be redirected to the job page, where you can see the job's progress.\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.\nMore information\nThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising."
    },
    {
        "Ticker": "PLTR",
        "Company": "Palantir Technologies",
        "Source_URL": "https://www.palantir.com/docs/foundry/foundry-devops/create-products/",
        "Content_Type": "Website Content",
        "Raw_Text": "This page contains instructions for creating a new product in Foundry DevOps. Users can browse and install available products via the Marketplace storefront. Read more about use cases for which you may want to create a product.\nTo create a new product, select a store in which to publish your product. To select an existing store or create a new one, select the Change Store option in the top-right.\nNew stores are saved to a project and inherit the permissions of that project. Specifically, anyone with edit access to a store's project can create new products and edit existing products in that store, and anyone with view or edit access to the store's project can install products from that store.\nOnce you've chosen a store, select New product to begin creating your product.\nProvide a name for your draft product in the Set product title... input box in the top-left.\nChoose Add output to select the outputs to include in your draft product; these outputs are the resources that Marketplace recreates when users install your product. You can choose Add files to select most resource types within the Compass filesystem. If a resource is not available to select through Compass, then DevOps provides its own selection option, such as Add ontology resources.\nDevOps automatically identifies resource dependencies, so you should add the furthest downstream resources first. For example, if you want to package a Workshop application and four object types, you should only add the Workshop application.\nDevOps also enables you to add outputs in bulk. Select Add from folder if you have a well-defined project that contains all the outputs you want to include in your product. You can use Add from Data Lineage and Add from Workflow Lineage to add resources from a graphical dependency view.\nAfter you add outputs to your draft product, select a resource to launch its Details panel on the right side of your screen.\nSelect the Dependencies tab to review the output's dependencies, which DevOps also surfaces as inputs.\nDevOps automatically surfaces output dependencies as Inputs as you add outputs to your draft product. Users who install your finished product must provide resources to satisfy each input.\nWhen designing your product, you can promote inputs for inclusion as outputs. Select the ellipsis icon before choosing Move to output on the input row to promote it to an output.\nYou can also move inputs to outputs in bulk by selecting multiple inputs and choosing Move {N} to outputs from the popup at the bottom of your screen.\nAs a general rule, if you want installers to provide their own version of a resource, such as their own dataset or object type, then you should list that requirement as an input. If you want your product to instead provide a resource for your installers, then you should promote the input to be an output.\nIf you iteratively promote all inputs to outputs, then users who install your product will not need to map anything during installation.\nYou cannot move certain input types, such as parameters or groups, to outputs since DevOps requires their configuration for installation.\nTo view more information about a specific input, select the input to launch the Details panel. The Dependents tab includes information on which content resources require this input. From here, you can also configure presets for an input, which allow you to restrict the options of an input and provide a default to installers.\nDevOps provides multiple options to help you manage products with a large number of outputs. Use the Group by Folder option in the Outputs panel to preview outputs in their destination folders if you enable folder structure to replicate the organization of packaged resources during installation.\nYou can also select the filter icon in the ribbon of both the Inputs and Outputs panels to filter the resources they display. For example, this enables you to display only those resources that have an error message or are of a specific type.\nSelect the Linked products tab from the left panel of your product to preview all available linked products, which can provide inputs for your products.\nYou can also use the Group by linked products option in the Inputs table to preview the inputs provided by each of the upstream linked products.\nSelect the Documentation tab from the left panel of your product to add information visible to users who are browsing products in the storefront. Use the Description section to write a short description of your product before adding an image to serve as its Thumbnail.\nYou can add additional Images as a preview of your product's content. Optionally, enter a longer Product description using Markdown ↗ syntax that contains detailed product use instructions.\nSelect the Settings tab to configure additional product settings.\nEnable folder structure by toggling on Folder structure to have DevOps create folders during installation that match the original organization.\nDevOps packages folders up until the lowest common ancestor, which on installation will be replaced by the installation project.\nUse Installation mode to set the default settings for any installations of your product. These settings can be changed per installation.\nUse Build settings to determine whether or not DevOps builds datasets and models automatically during the Marketplace installation job. This ensures that DevOps hydrates all datasets across the newly deployed resources upon job completion.\nOnce you are satisfied with your product, select Publish to make the product available in the storefront. This launches the Review Changes and Publish popup, where you can review changes in your product and add a Changelog description. Publishing may take up to a few minutes depending on your product's size.\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.\nMore information\nThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising."
    },
    {
        "Ticker": "PLTR",
        "Company": "Palantir Technologies",
        "Source_URL": "https://www.palantir.com/docs/foundry/announcements/2023-11/",
        "Content_Type": "Website Content",
        "Raw_Text": "REMINDER: You can now sign up for the Foundry Newsletter to receive a summary of new products, features, and improvements across the platform directly to your inbox. For more information on how to subscribe, see the Foundry Newsletter and Product Feedback channels announcement.\nWe are excited to announce the release of the Foundry Newsletter and Product Feedback channels, available now for sign-up by navigating to User Settings > Notifications > Updates & News.\nThe Foundry Newsletter will deliver a summary of new products, features and improvements across the platform, directly to your inbox. The first (GA) Foundry Newsletter will be sent to subscribers mid-November 2023. You can also opt-in to the newly-released Product Feedback channel, which provides opportunities to connect directly with Palantir engineers seeking targeted user input. This update presents an exciting opportunity to have your voice heard and play a role in shaping ongoing developments across the Foundry ecosystem.\nNewsletters and other content shared through these opt-in subscriptions will be sent to the email address associated with the Foundry user account. Note that notifications information, as well as email addresses, are stored solely within the boundaries of the Foundry enrollment and not collected centrally for Notifications communications.\nSteps to subscribe or change your notification preferences are as follows:\nAccount settings\nNotifications\nNote for platform administrators: Platform administrators should also register their email addresses in the Foundry Control Panel settings for Contact information in order to receive important communications related to platform administration, user support, service disruption announcements, and security updates that are designed for platform administrators and separate from the Foundry Newsletter and Product Feedback channels (designed for all users) described above.\nDate published: 2023-11-30\nThe Approvals inbox has now been integrated into Control Panel to support Control Panel-related workflows, allowing a seamless and centrally-managed approval request review of your sensitive operations. This Control Panel integration provides version control for security configurations, enabling administrators to trace the history and rationales behind modifications made, while an added layer of visibility guarantees that teams can easily monitor and assess updates to security settings, thereby contributing to the maintenance of a secure and compliant environment. Starting now, you can review network ingress configuration requests, with additional workflows in active development.\nApprovals tab features an inbox with only Control Panel-related workflows, starting with ingress configuration change requests.\nStarting December 4, an approval will be required to enact network ingress changes. This policy change seeks to ensure a more secure and controlled process for managing ingress configurations while adhering to the platform's existing permissioning model.\nOnly security officers can open change requests for network ingress configuration, and by default, change requesters can approve their own changes. For a minority of enrollments that require elevated security, the workflow will be configured to require approval from a second security officer.\nNetwork ingress configuration managed by Approvals within Control Panel.\nApprovals' integration with Control Panel aims to bolster security and control over sensitive workflows and ensure that crucial operations are administered with appropriate oversight. The Approvals inbox will improve by adding new workflows, making sensitive task management more organized and enhancing collaboration.\nFor more information on how to use the approvals inbox, review Control Panel approvals. Alternatively, to learn more about managing access to the platform, review Configure network ingress documentation.\nDate published: 2023-11-28\nWorkshop application builders can now build reusable application components using Embedded Module widgets, now generally available. This new capability unlocks a powerful composition primitive that enhances the maintainability and reusability of use cases.\nFor example, consider a Workshop module that has many pages, hundreds of widgets, and thousands of variables. By leveraging the Embedded Module widget and its variable sharing capabilities, this large Workshop module can be divided into smaller, separate embedded modules. Builders can develop these focused embedded modules independently and later combine them into one Workshop module. Any parts of the module which are duplicated, for instance, a filter widget in combination with some display widget, may be separated to another module which is embedded in multiple places.\nWithin the Foundry apps section of the widget selection page, locate the Workshop: Embedded Module widget as illustrated below.\nThe Embedded Module widget is located on the widget selection menu.\nConfigure your widget by selecting a module for which the module interface variable definitions will be displayed. Then, map parent module variables to child module variables.\nConfiguration section for the Embedded Module widget for variable mapping.\nFor more detail, review the Embedded Module widget documentation.\nSeveral additional improvements to embedded modules are planned, including:\nLoop layouts: Currently in beta, these layouts will extend embedded module capabilities by allowing looping over an object set and displaying a module for each object in the set. Read the Loop layouts documentation to learn more.\nModule interface variable experience: An improved module interface variable experience for more straightforward configuration and usage with embedded modules is in active development.\nDate published: 2023-11-28\nVersion 2.29.0 (SP29) of the Foundry Connector 2.0 for SAP Applications add-on, used to connect Foundry to SAP systems, is now available.\nThis latest release features minor bug fixes and several enhancements, including:\nStarting with SP29, the add-on installation packages can be downloaded directly from within Foundry. To access SP29:\nWe recommend sharing this with your organization's SAP Basis team.\nFor more on downloading the add-on, consult the documentation.\nDate published: 2023-11-28\nWe are excited to announce that you now have the ability to create model inference history datasets in Foundry. These datasets capture all inference requests (inputs) and inference results (outputs) handled by a live deployment in a modeling objective, simplifying a wide range of workflows, including drift detection, continuous retraining, performance evaluation, and usage analysis.\nModel inference history datasets track requests and responses for live deployments. This capability provides valuable feedback for production use cases where the model developer is interested in understanding how the model is being used by real customers and users.\nFor example, consider the use of a live deployment to serve recommendations on a website. The model inference history records user visits and the recommendations provided to each user. With this, developers can analyze the dataset to determine the effectiveness of the recommendations for specific users. In this case, the dataset creates value by enabling usage analysis which can inform decisions about optimization and resource allocation.\nWhile current model evaluation processes support continuous retraining and performance evaluation, these existing capabilities do not provide user feedback of model usage. As a result, and in contrast to model inference history datasets, these capabilities are more useful for those cases in which there is existing training and testing data, as opposed to live data.\nTo create a model inference history dataset, navigate to the Deployments page of your modeling objective, select the live deployment, and choose Create dataset under the Model Inference History section. We highly recommend adding security markings, as inputs and outputs may contain sensitive information.\nModel inference history is created from the Deployments page of your modeling objective.\nOnce created, the dataset will record essential information such as timestamps, user IDs, request UUIDs, and JSON representations of inputs and outputs.\nTo temporarily enable or disable a model inference history, navigate to the Model Inference History card and use the Enable Recording toggle. To permanently disable a model inference history, select the Remove button next to the dataset in the Model Inference History card.\nFor more information on this topic, refer to the model inference history documentation.\nDate published: 2023-11-15\nWe are announcing Lightweight API, an alternative to Spark transforms that capitalizes on the unparalleled performance of container transforms to speed up transforms for small-to-medium-sized datasets that do not need to rely on Spark. As an increasing number of data transforms can now be run on a single node, Lightweight API allows you to eliminate the considerable overhead from the orchestration of distributed parallelism and instead rely on single-node alternatives for authoring data pipelines.\nTo turn your Pandas transform into a Lightweight Pandas transform:\n@lightweight\non top of your existing decorators, as in the following code snippet:Copied!1from transforms.api import transform_pandas, Input, Output, lightweight 2 3@lightweight 4@transform_pandas( 5 Output('/Project/folder/output'), 6 df=Input('/Project/folder/input') 7) 8def compute(df): 9 return df[df['Name'].str.startswith(\"A\")].sort_values(by=\"Age\")\nThe benefits of Lightweight API are demonstrated when comparing the benchmarks of optimizing a complex transformation and a trivial transformation. For context, the two benchmark graphs displayed below demonstrate a controlled optimization comparison of the mean end-to-end running time across seven repetitions of five compute engines executing the same transformation logic on 19 different dataset sizes. The bands represent standard error. Each transformation was provisioned 8 vCPUs and 32 GBs of RAM. The premature ending of lines denotes an out-of-memory (OOM) condition being reached. Polars was used in streaming mode accessible through .polars(lazy=True).\nThe benchmark image below shows a reasonably complex pipeline containing an explode operation followed by multiple joins and a group-by, natively implemented using the APIs of Pandas, Polars, and PySpark.\nBenchmark graph showing the optimization of a complex transform\nIn this case, the startup overhead is nearly cut in half when using the Spark-less Lightweight backend. Additionally, both single-node optimized data processing libraries Pandas ↗ and Polars ↗ remain significantly faster up until around 10 million rows, as they do not wait for the results of network-bound shuffles as Spark does. However, for larger datasets and more complex operations, Spark's query plan scales to be much more efficient and is thus recommended in these use cases.\nIn contrast, applying a simpler pipeline to even larger data indicates that Spark does not always manage to outperform Polars, even on immensely large datasets.\nBenchmark graph showing the optimization of a trivial transform\nThe above chart highlights a key observation that the implemented transformation logic is close to trivial, containing only a single filter operation on a column by which the dataset is not partitioned. In this setup where the pipeline is simple (such as fewer operators and memory-friendly joins), Lightweight Polars' streaming mode ↗ vastly outperforms Spark and is recommended for all dataset sizes. As a reminder, when a @transform\nis decorated with @lightweight\n, Spark methods are not accessible.\nWe are actively developing Lightweight API. As such, some features including unmarking workflows or external transforms are not yet supported. The following features are coming soon:\nIn cases of smaller-scale datasets or simpler processing logic, we recommend using Lightweight transforms for faster computation.\nFor more details, review the documentation on Lightweight API and transform_polars or give it try by installing the Lightweight transforms examples Marketplace product from your stack’s Reference Resources.\nDate published: 2023-11-13\nPalantir Foundry now supports semantic search in the Ontology with the introduction of vector properties and functions. Users can now create a vector property type for objects in the Ontology and use a K-nearest neighbors (KNN) function on objects (FoO) to enable semantic search over objects. This feature makes it possible to use the outputs of models leveraging the Ontology for more accurate results.\nSemantic search capabilities in Foundry enable a range of use cases, including improved experience for documentation base users searching for a specific topic. With semantic search support, a user's query can be matched with its semantic meaning and return related objects. Additionally, with access to the Ontology, these results can then be piped back into the LLM to generate a useful user-friendly prompt boosting existing AIP builder features.\nSemantic search is a search technique that takes into account the context of a query to return more accurate and relevant search results. Unlike keyword search, which simply matches whole phrases from a query regardless of context, semantic search incorporates the meaning behind the user's search terms.\nSupport for semantic search in Foundry is facilitated by various enhancements made to the Ontology, including:\nGenerate a vector property from a float array obtained from a Foundry modeling objective to then allow searches using KNN in a FoO function for similar vectors. For a detailed end-to-end semantic search workflow example, refer to the semantic search workflow tutorial.\nThe new vector type can be used to capture embeddings from models into the Ontology. These can be created on any float array dataset field of fixed dimension. There are a few similarity functions to compare vectors that can be chosen based on use case and model output.\nFrom within Ontology Manager, you can configure the vector property, starting with a type, choosing the dimension (output length of model that this is based on), and then selecting the similarity function that decides how you want to compare this vector property type with other objects that have the same vector property type.\nThe addition of K-Nearest Neighbors (KNN) Function on Object (FoO) support simplifies the process of creating a function to semantically query an object type. A live model-generated vector or existing vector from an object can be used to perform the KNN search over the object type. Review additional information on KNN FoO functions in the documentation.\nFor more information on topics related to this announcement, refer to following documentation:\nDate published: 2023-11-13\nWe have simplified and sped-up the process of enforcing usage of a specific Spark module version (also known as \"pinning\") on your desired code repository. You can now select the Spark module version directly from within in-platform from the Settings > Runtime override tab of your repository in Code Repositories. Previously, this action required a manual and time-consuming CDConfig process.\nAs a reminder, the use of pins is intended to solely be a temporary approach. We always recommend using the most recent version of Spark to benefit from the latest performance and security enhancements.\nTo pin a Spark version, open your repository in Code Repositories and navigate to the Settings > Runtime overrides tab.\nConfigure your module versions in the Runtime overrides tab of your code repository's Settings view\nYou can Create pins and select specific Versions of Spark that you would like to pin on All branches or Specific branches. An Expiration date must be specified and cannot be more than 90 days from the current date.\nOnce you Save a pin, you can see confirmation in the Runtime overrides tab as depicted in the image above. Existing pins are displayed and can be Edited or Archived where they will no longer be in effect. Archived pins are tagged with an Expired\nlabel and can be restored if necessary.\nThe pin can also be viewed on the Build Preview > View details page:\nCode repositories will display a \"Pinned\" tag listing the Spark module version when the feature is enabled\nThis feature currently supports pinning Spark module to repositories. Our team is working to include pinning support for the following:\nFor more details on this new feature, review the Pin Spark modules in-platform documentation.\nDate published: 2023-11-02\nOrganization permissions are migrating in two phases from Platform Settings to Control Panel in order to improve legibility, reduce confusion, and provide greater flexibility in permission setups for platform administrators. Currently, the controls which govern how users experience Foundry, including the management of permissions, membership, marking categories, and group memberships are managed in the Platform Settings interface but will be consolidated to Control Panel by this move.\nPhase 1 (Starting the week of November 20th): Platform Settings workflows/permissions added to Control Panel roles.\nPhase 2 (Expected Q1 2024): Remaining members of Platform Settings permissions are migrated to the Control Panel roles that contain the workflows/permissions. Once users have been fully migrated, legacy Organization platform settings will solely be available in Control Panel. The details of phase 2 changes will be communicated in a future announcement.\nReview the following notice carefully to understand how permissions updates will affect you and your Organization.\nWe have seen instances where users are confused by which permissions are granted in Control Panel vs. Platform Settings. Consolidating into a single UI improves clarity and reduces the complexity of administrative workflows.\nThe current permissions system is inflexible and burdens top-level administrators with excessive workload due to their exclusive access to certain tasks. Roles in Control Panel allow for customization out of the box, and permissions for these workflows can be granted in a granular manner via custom roles.\nBy the end of Phase 2, all existing permissions will be automatically migrated and result in no loss of access or privileges. However, there are a few important changes to note:\nTo shift away from using Platform Settings permissions, we first have to grant these permission/workflows to roles in Control Panel. One new role will be created, and some existing roles will have new workflows added, resulting in existing role members gaining additional permissions inline with the description of the role.\nWe recommend you review the changes below against your Organizations' role memberships in advance for considerations of security and update roles in Control Panel where necessary.\nGraphical chart representing role mappings to Control Panel roles in phase 1 changes\nManage\npermissions and other permissions granted in Platform SettingsOrganization administrator acquires Manage permissions, membership, marking categories and control Expand Access/Apply Organization.\nUsers with the Organization administrator default role will gain the ability to perform workflows previously granted via Platform Settings (Manage permissions, Manage membership, Manage marking categories, and View group membership). The change maintains our philosophy that Organization administrator is a highly privileged role and should have the ability to manage all aspects of the Organization.\nExpand access will remain separate and not be automatically granted to Organization administrators. Organization administrators will however be able to manage the users and groups who have this capability by navigating to Control Panel > Organization Permissions tab.\nUsers with the Organization settings viewer default role will gain the ability to View group membership for the Organization. Currently, this Control Panel role grants read-only access to Organization permissions in Control Panel, but more read-only workflows will be added in the coming months.\nUsers with the Data governance officer default role will gain the ability to Manage marking categories for the organization.\nA new default role is being added to Control Panel called Users and groups administrator. Users added to this role will gain the ability to Manage membership for the Organization.\nA new Marking Permissions tab will be added to the Organization Permissions section in Control Panel. This is a one-to-one replacement for the Apply Organization and Expand access sections in Platform Settings.\nNew Marking permissions tab on Control Panel for Organization administrators to review Organization permissions and existing permissions\nIn Phase 2, existing permission grants will be automatically migrated over to Control Panel roles and the interface on Platform Settings will be fully deprecated and removed. Additional communication about phase 2 details will be provided once phase 1 is underway. At the start of Phase 1, we encourage administrators to:\nFor more information on how permissions and its primitives work in Foundry, review the documentation on Enrollments and Organizations in Foundry.\nDate published: 2023-11-02\nThe Python language version of the Ontology SDK is now generally available. Developers can use Developer Console to generate a Python package with object types, link types, and action types from their Ontology, and install the package using either Conda or pip.\nThe Ontology SDK can be used to load data, perform aggregations, and apply actions. The following is a code snippet example:\nThe Python Ontology SDK is generated using Developer Console. Refer to the Ontology SDK documentation for more details, or follow our walkthrough on how to use the Python SDK with Jupyter notebook.\nDeveloper Console also includes dynamic documentation customized to the content of your application. In-platform API documentation is also available for TypeScript and cURL.\nDeveloper Console includes documentation for each object type included in the SDK\nOpen Developer Console from your Foundry workspace navigation bar and create an application or open one you have already created. On the Application SDK page, you will find a list of all the generated versions of the SDK. Start by selecting Getting started docs located on the top right.\nDeveloper Console includes documentation for each object type included in the SDK\nAccess the Application SDK from under the Settings menu of the Developer Console navigation bar and begin with Getting started docs\nFor more information, review Ontology SDK documentation.\nDate published: 2023-11-02\nAutomate, generally available the week of November 20th, is the new, fully backwards-compatible product that replaces Objects Monitoring to become the single entry point for all business automation in Foundry. All existing object monitors will automatically be migrated to the new application and continue to function without change.\nAutomate's summary page at a glance\nAutomate allows users to newly set time conditions in addition to object conditions, and link them to actions or notifications that automatically execute when conditions are met. With many improvements and extensions over Object Monitoring, Automate offers a fully redesigned user interface for creating automations, more options to configure conditions and effects, and native integrations with other applications in the platform.\nIn addition to Automate's ability to create automations that never expire, you can now configure a variety of new conditions, effects, and native integrations to provide you more automation flexibility.\nWe plan on extending Automate's capabilities and embedding it even further with other Foundry applications, with a highlight on improving:\nFor more information, review the Automate documentation or learn how to Get started with Automate.\nDate published: 2023-11-30\nNew Repository Guidance | Foundry now provides helpful suggestions for files to edit when creating a new repository in Code Repositories. This feature aims to streamline the process of setting up a new repository by recommending relevant files to modify.\nDate published: 2023-11-30\nEnhanced SDK Generator Selection | Users can now choose from all available generator versions for each package manager, providing more flexibility and control over SDK generation. A new dialog has been introduced for selecting generator versions and specifying which packages to generate, improving the overall user experience.\nDate published: 2023-11-30\nCloud Identities Extension Now Generally Available | The Cloud Identities extension in Control Panel is now generally available. Cloud identities allow you to authenticate to cloud provider resources without the use of static credentials.\nDate published: 2023-11-30\nCustom Base Layers Support in Workshop Map Widget and Foundry Maps | This update enables custom base layers in the Workshop Map Widget and Foundry Maps, providing users with more flexibility in map visualization. The \"enable mapbox base layer\" toggle in the Map section of the Control Panel now applies to all maps, and when disabled, maps will use the first custom base layer configured. Additionally, Workshop now offers a \"custom\" base style option.\nDate published: 2023-11-30\nEnhanced Time Series Transforms | Quiver now offers improved time series transforms with parameterizable inputs, units support, unit override, and unit conversion. These features apply to time series transforms used both natively and in the transform table, providing a more flexible and efficient user experience.\nDate published: 2023-11-30\nEnhanced object storage filter for Ontology cleanup | The Ontology Manager application now includes an improved object storage filter to streamline the ontology cleanup process. Users can more efficiently identify and remove unnecessary objects, resulting in a cleaner and more organized ontology.\nDate published: 2023-11-30\nCloud Identity Permissions Management | Introducing a new permissions manager for cloud identities in the Control Panel, allowing users to easily manage access and permissions for their cloud resources.\nDate published: 2023-11-28\nImproved widget navigation in Slate | Users can now easily navigate to widgets on a long canvas in Slate by selecting the widget in the widget list. The canvas will automatically scroll to the selected widget, providing a more efficient and user-friendly experience.\nDate published: 2023-11-28\nFull-width editor panels now available | The Slate application now features full-width editor panels, enabled by default on all stacks. This update provides users with a more spacious and comfortable editing experience.\nDate published: 2023-11-28\nEnhanced dataset preview for local development | Users can now preview datasets with files during local development, providing a more efficient way to test and validate their work. This update includes improved file preview capabilities and supports multiple input and output files.\nDate published: 2023-11-28\nCross-window messaging in Slate | Slate now supports cross-window messaging, allowing users to send and receive messages between browser windows using the slate.sendMessage\naction and slate.getMessage\nevent. This feature provides a more flexible and generic way to communicate between windows.\nDate published: 2023-11-28\nInstalling TypeScript Packages via Task Runner | Installation through the Task Runner is now enabled for TypeScript Functions repositories with functions-typescript template versions of at least 0.523.0\n.\nTo use this feature, search for a package using the Libraries tab of your code repository. After selecting a package, the new Add and install library option can be selected to automatically install the package using the task runner. This will automatically update both the package.json and package-lock.json files in your repository.\nFor more information, read the documentation.\nDate published: 2023-11-28\nEnhanced npm package search and installation | The search and installation workflow for npm packages in TypeScript Functions for Code Repositories now benefits from enhanced functionality. Previously, the only place to find npm packages was in the external public registry. Now, an enhanced Typescript developer experience allows users to search for any published npm package from Artifacts Repositories or directly from https://npmjs.com within their Typescript Functions Code Repositories, and subsequently import them to their Code Repositories. Additionally, users can now publish their custom typescript packages directly to Artifacts Repositories, enabling developers to import any library's functionality directly to Foundry! To learn more, read the documentation.\nDate published: 2023-11-28\nUpdated Workshop home page | Workshop now features a newly revamped home page, offering enhanced features that enable users to effortlessly create ready-to-use applications from a variety of templates. Users can conveniently input their own data and choose properties that will be smoothly integrated into the module. The current collection of templates features an inbox, map, and metrics dashboard, with more exciting options coming soon!\nDate published: 2023-11-28\nImproved large graph performance in Vertex | Users will experience a significant performance improvement when drag-selecting on large graphs with approximately 1000 nodes in the Vertex application. This enhancement provides a smoother and more responsive experience for users working with large datasets.\nDate published: 2023-11-28\nEnhanced safety for deleting edits in Ontology Manager | The Ontology Manager now provides additional safeguards by preventing users from deleting all edits on an Object type with an \"Active\" status in Object Storage V2. This ensures data integrity and reduces the risk of accidental data loss.\nDate published: 2023-11-28\nBoard collapse | Visualization boards can now be collapsed to free up path space and improve performance for large paths. To collapse a board, select the Hide board contents option in the top right of the board. All transforms that the board applies to the path will be kept but any visualizations will be hidden.\nDate published: 2023-11-28\nEnhanced security for data transformations | We have improved the security posture within Foundry's data transformation architecture by implementing an updated job spec restriction policy. This update provides tighter controls for input datasets referenced in code authoring and code workbooks transforms. If a transform is affected by this update, it will fail, and the user will receive a detailed message explaining the root cause. For more information, refer to the documentation on Project references and permissions.\nDate published: 2023-11-28\nNew page templates available | New layout templates for pages are now available in Workshop! On creation of a new page, five new pre-styled layout options will be available for selection including a details, grid, inbox, overview, and settings view. Layout options may be previewed prior to selection by hovering over their respective icons.\nDate published: 2023-11-15\nBusiness metrics are visible in the overview page of the Resource Management app | Users can now highlight key business metrics, including data scale and average daily users. Metrics can be configured per enrollment or usage account and are visible at the top of the Overview tab or in the Usage accounts tab, respectively.\nDate published: 2023-11-13\nEnhanced Resource Selection in Actions | Action parameter configurations now support a Foundry Resource Selector display option for String type parameters, allowing users to easily select resources within their workflows. This makes it much simpler to build workflows that use both objects and resources in creative ways.\nDate published: 2023-11-13\nRestriction on Primary Key Changes for Object Storage V2 | Ontology Management now restricts users from changing the primary key of an OSv2-backed object type that has received edits, as this is not supported in Object Storage V2. This change ensures data consistency and prevents potential issues.\nDate published: 2023-11-13\nEnhanced Schema Migrations Interface in Ontology Manager | The Ontology Manager application now features a redesigned schema migrations UI when making breaking schema changes in OSV2. Users can easily apply a set of suggested migrations, streamlining the process and improving the user experience.\nDate published: 2023-11-13\nInclude Attachments documentation in developer console | When generating documentation for the Typescript OSDK, there is now a section on working with Attachments if the relevant ontology object types, actions, and functions include any attachment properties.\nDate published: 2023-11-13\nEnhanced Developer Console Function Selection | The Developer Console now allows the selection of Functions with unsupported inputs/outputs, providing a more flexible experience. Unsupported types are documented, and failures are surfaced in the CLI logs.\nDate published: 2023-11-13\nModel Inference History for Live Deployments | The Model Inference History is a dataset in Foundry that captures all inference requests (inputs) and inference results (outputs) sent to a Modeling Live Deployment. This feature enhances the user experience by providing a comprehensive record of all inferences made, allowing for better tracking and analysis of model performance.\nDate published: 2023-11-13\nImproved Telemetry Service Stability | The Foundry Telemetry Service has been optimized, resulting in a significant decrease in QoS and Transaction Conflict exceptions. Users can expect a more stable and reliable experience with a 91% reduction in these exceptions.\nDate published: 2023-11-13\nStrict Mode Enabled in Slate | Slate now operates in \"strict mode ↗\" as all scripts run as ES6 modules, ensuring better code quality and error handling. This update may affect variable naming, as JavaScript keywords are now strictly reserved.\nDate published: 2023-11-13\nEnhanced Cross-Window Messaging in Slate | Slate now supports more flexible cross-window messaging. This new feature allows users to send messages between different browser windows, providing a more integrated and seamless user experience. This implementation is highly versatile, essentially serving as a wrapper around the window.postMessage\nfunction and a listener for incoming events. Users can now use the slate.sendMessage action to specify their messages and post anything they want and selectively trigger specific Slate actions when a message is received.\nDate published: 2023-11-13\nPipeline Origin Indication for Object Types | Object types created in Pipeline Builder now display a banner indicating the pipeline they were created in, even when editing is enabled in Ontology Manager. The banner also shows whether editing in Ontology Manager has been enabled. For object types created in Pipeline Builder that are not targeting building a dataset but are populating the object type directly, the Pipeline Builder pipeline will be displayed instead of a backing dataset.\nDate published: 2023-11-08\nEnhanced Ontology integration in Pipeline Builder | Pipeline Builder now supports Ontology output types, ensuring seamless integration with downstream production applications. Users can now add an object type or a link type output directly within Pipeline Builder, eliminating the need to link a dataset output to an object in Ontology Manager.\nDate published: 2023-11-08\nEnhanced executor profiling for debug jobs | Improved the profiling of executor-side performance for debug jobs with periodic executor profiling. By default, executors are profiled every 5 minutes for one minute, providing better visibility to identify and address performance regressions. You can download HTML files containing flame graphs for each executor and profiling interval from the files view in the output dataset's Details tab.\nDate published: 2023-11-08\nEnhanced model creation experience | The Model Assets creation process has been significantly improved, offering a variety of guided options for creating different model types. Additionally, the model submission dialog in the Modeling Objectives application now supports more source types and direct sandbox deployment for immediate testing of new model submissions.\nDate published: 2023-11-08\nDeveloper Console: Update groupby\nsyntax | The Foundry Developer Console now uses the group_by\nsyntax instead of groupby\nfor improved consistency and readability.\nDate published: 2023-11-08\nEnhanced preview speed with input sampling | Pipeline Builder now supports input sampling, allowing users to choose a percentage of their input data for faster previews. This feature significantly increases preview speed while maintaining full input computation upon building.\nDate published: 2023-11-08\nEnhanced geospatial data preview in Pipeline Builder | Users can now preview geospatial transformations on a map directly within Pipeline Builder using the geospatial preview board in the bottom left panel. To use this feature, select a series of geospatial datapoints in your preview table, then right-click and choose Open geo preview.\nDate published: 2023-11-06\nOpt-in to SDK Beta Features | Users can now opt-in to beta SDK features in the Foundry Developer Console. This allows users to test and experiment with new features, such as Time Series properties, before they are rolled out to everyone. To enable beta features, simply toggle the \"Enable Beta Features\" option in the package settings.\nDate published: 2023-11-06\nConditional selection in Notepad templates | Conditional selection enables users to selectively include content in a generated document based on the value of a string template input. Configure a rule with an \"if\" section to set the condition, and a \"then\" section to determine the consequence, opting to either hide or show the contents of the conditional section.\nDate published: 2023-11-02\nPipeline Builder search panel | The search panel has been updated to provide a more user-friendly experience while searching the pipeline graph and transform paths. New features include more explicit tooltips and labels, the ability to use Enter\nand Shift + Enter\nto navigate between results, and the option to open the search panel via Ctrl + F\n(Windows) / cmd + f\n(MacOS).\nUsers can combine multiple conditions, and each condition can be toggled to search node name, description/text boards, column references, schema or property names, transform names, and parameter references.\nDate published: 2023-11-02\nPipeline Builder proposals now support comments | Comments can now be added to proposals to help facilitate discussion among users about the proposed changes.\nDate published: 2023-11-02\nTime series unit overrides and conversions | Users can now manually override the unit of a time series using a transform table column or a string provider. When used together with the Time series unit transform, users can propagate the unit of one time series to multiple time series.\nAdditionally, users can now convert the values of a series data using available unit conversions. The available unit conversions are dependent on the base unit of the series, or the override unit if specified.\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.\nMore information\nThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising."
    },
    {
        "Ticker": "PLTR",
        "Company": "Palantir Technologies",
        "Source_URL": "https://www.palantir.com/docs/foundry/platform-overview/development-life-cycle/",
        "Content_Type": "Website Content",
        "Raw_Text": "At Palantir, we strive to continuously evolve production-grade software to meet the needs of our customers and improve the capabilities we offer. We incorporate cutting-edge technologies and frameworks while remaining grounded in the real-world problems our customers face every day. In practice, this means that Palantir can develop new capabilities in close partnership with customers.\nThis page outlines the various development life cycle phases of capabilities in the Palantir platform, including the definitions of these statuses, their availability, support levels, and the expectations we set for those statuses between Palantir and our customers.\nWhile this page explains the typical execution of the product development life cycle at Palantir, some attributes of a life cycle phase may vary based on a given product or feature.\nThis table provides an overview of the different development statuses and their core attributes. For more information, see the linked detail sections on each status.\n| Status | Development | Availability | User support | Public documentation |\n|---|---|---|---|---|\n| Experimental: Prototype or exploratory feature | 🟢 Active | 🔴 Very limited availability | 🔴 None | 🔴 None |\n| Beta: Early-stage feature with a typically slower rollout | 🟢 Active | 🟡 Opt-in availability | 🟢 Yes | 🟢 Yes |\n| Generally available (GA): Production feature | 🟢 Active | 🟢 Broadly available | 🟢 Yes | 🟢 Yes |\n| Legacy: Production feature without active development | 🟡 Critical fixes only | 🟢 Broadly available * | 🟢 Yes | 🟢 Yes |\n| Sunset: Feature slated for deprecation, but no date is scheduled | 🟡 Critical fixes only | 🟡 Existing customers only | 🟢 Yes | 🟢 Yes |\n| Planned deprecation: Deprecation date and migration path communicated | 🟡 Critical fixes only | 🟡 Existing customers only | 🟢 Yes | 🟢 Yes |\n| Deprecated: Feature is no longer available | 🔴 None | 🔴 None | 🔴 None | 🔴 None |\n* Installed but disabled for new customers by default. New customers can enable the feature through Control Panel.\nNew features in Foundry begin in an experimental or prototype phase.\nThese features and products are in the early stages of development and are typically worked on in collaboration with a small set of customers. The goal of the experimental phase is to demonstrate and validate a new capability as quickly as possible. Features in the experimental phase may not be publicly documented, as we prioritize speed of development to explore the space of solutions expansively. Experimental products are expected to change frequently and may not progress to other life cycle stages.\nOnce a feature's purpose and approach has been validated with one or more customer partners, it is made available to a broader group of customers in a beta phase.\nThe goals of the beta phase are to invest heavily in the long-term maintainability of a feature while measuring overall value and usage. For this reason, beta features may be rolled out slowly to customers to gather feedback during the development process. Sometimes, beta features are available to a wide range of customers; at other times, beta features may be limited to partners with specific business needs or feedback opportunities that advance the development process.\nBeta features are documented publicly to help enable customer partners to use this early-stage functionality, even if the features are not yet available in every enrollment. When a feature is in the beta phase, it will be labeled as such in documentation.\nThere is no guarantee that beta features or products will progress to the next stages of development, though the vast majority do move on to general availability.\nGenerally available is the term used to describe the vast majority of features and products in Foundry. GA features are enabled for customers by default and constitute a core part of the platform. When features become GA, other parts of the platform may build on top of them to enable tight-knit integrations.\nIn the GA phase, there is a continuous feedback loop between Palantir engineers and the broad set of customers using the feature. Feedback is triaged, prioritized, and fed into Foundry's product roadmap, and updates are delivered to customers rapidly using Apollo ↗, Palantir's platform for continuous delivery.\nAlthough GA features are widely available, it is possible in some cases that your enrollment may not have every feature enabled as some features have dependencies on specific types of infrastructure or may require specific contractual agreements. For example, some GA features in Foundry are only available in Palantir's managed SaaS environment and are not supported for self-hosted installations. Keep in mind that when a new feature or application is announced as generally available, there may be a delay of a week or more before it becomes available within a specific Foundry enrollment.\nUnless otherwise specified, any publicly documented feature is generally available, and you can rely on GA features to be fully supported. The removal of any GA feature from the platform will follow the sunset and planned deprecation processes outlined below.\nIn the legacy phase of development, features and products enter a stage where work is considered complete and no additional feature development is expected.\nLegacy applications are widely installed across the Palantir customer base and available for continued production use. As in GA, products in the legacy status remain fully supported. For new customers, they are typically available on an opt-in basis. Despite legacy features not being actively developed, feedback is still encouraged as it can inform our work on new applications and tools.\nAll products and features in the legacy phase will be labeled as such and thoroughly documented publicly. At this point, no deprecation date is planned or expected.\nAs development progresses, an existing feature or application in Foundry may reach the end of its usefulness or purpose, or it may be superseded by another functionality. The original vision for the feature may be narrower than the problem space it has grown to address, or new tools or features may result in more robust or scalable solutions to a problem. At this point, we start to discourage use of the feature, and new customers will not be able to enable it for use in their enrollment.\nIn our public documentation, we will label products that are in the sunset phase and will suggest other tools and applications to replace them. As in GA, products in the sunset phase remain fully supported. Deprecation is planned for the future, but no exact date is confirmed. As soon a deprecation date is identified, the feature will move into the planned deprecation phase, with the date and any relevant migration details communicated.\nProducts entering the planned deprecation phase are preparing for retirement from the platform. As products or features enter the planned deprecation phase, Palantir works to provide proactive communication around the scheduled retirement date and any migration steps, if needed. Products in planned deprecation remain fully supported. However, new enrollments will not be able to enable features in planned deprecation.\nIn planned deprecation, a feature does have a confirmed deprecation date, and we are in the process of removing the feature and its integrations from our platform. Like products in the sunset phase, we will label products in the planned deprecation in our public documentation and in the platform. Our public documentation will also offer suggestions for workflow migrations that will fit your use case.\nWhen we move a feature into planned deprecation, you can expect time to migrate your workflows to new tools and applications before the scheduled deprecation date. We will use our platform Upgrade Assistant to notify administrators of the upcoming deprecation and provide a clear deadline for complying with workflow migrations. For application-level deprecations, the intent to deprecate and the final deprecation notice will be proactively shared with the registered platform administrator's contact details as well as publicly on the Foundry Announcements page. Progress towards feature deprecation is tracked quantitatively to ensure all customers are able to migrate to a replacement before a feature is finally removed from the platform.\nWhen a product completes the required tasks during the planned deprecation phase, it is considered fully deprecated. In this phase, the feature or product is no longer supported or available in the platform and all customers have been migrated away from relevant workflows. Customers will no longer have access to related API endpoints or other integrations, and support is no longer available. All public documentation of the feature is removed and no longer discoverable.\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.\nMore information\nThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising."
    }
]