[
    {
        "Ticker": "META",
        "Company": "Meta Platforms",
        "Source_URL": "https://www.meta.com/",
        "Content_Type": "Website Content",
        "Raw_Text": "Die Verwendung von Cookies durch Meta in diesem Browser erlauben?\nWir verwenden Cookies und √§hnliche Technologien, um Inhalte in Meta-Produkten bereitzustellen und zu verbessern. Dar√ºber hinaus verwenden wir sie, um mithilfe der durch Cookies auf und au√üerhalb von Meta Quest empfangenen Informationen die Sicherheit zu verbessern sowie um Meta-Produkte f√ºr Personen, die ein Konto haben, bereitzustellen und zu verbessern.\n‚Ä¢\nErforderliche Cookies: Diese Cookies sind notwendig f√ºr die Nutzung von Meta-Produkten und die ordnungsgem√§√üe Funktion unserer Websites.\n‚Ä¢\nCookies anderer Unternehmen: Wir verwenden diese Cookies, um dir Werbeanzeigen au√üerhalb von Meta-Produkten zu zeigen und Funktionen wie Karten oder Videos in Meta-Produkten anbieten zu k√∂nnen. Hierbei handelt es sich um optionale Cookies.\nDu bestimmst, welche optionalen Cookies wir verwenden d√ºrfen. In unserer Cookie-Richtlinie erf√§hrst du mehr √ºber Cookies und wie wir sie verwenden. Dort kannst du deine Auswahl au√üerdem jederzeit √ºberpr√ºfen oder √§ndern.\nInfos zu Cookies\nWas sind Cookies?\nMehr dazu\nWarum verwenden wir Cookies?\nMehr dazu\nWas sind Meta-Produkte?\nMehr dazu\nDeine Cookie-Auswahl\nMehr dazu\nCookies anderer Unternehmen\nCookies nach Kategorie ausw√§hlen\nSo verwenden wir diese Cookies\nWenn du diese Cookies erlaubst:\nWenn du diese Cookies nicht erlaubst:\nAndere M√∂glichkeiten, um deine Informationen zu kontrollieren\nPersonalisiere dein Werbeerlebnis in der Konten√ºbersicht\nWeitere Informationen zu Onlinewerbung\nCookies √ºber die Browser-Einstellungen kontrollieren"
    },
    {
        "Ticker": "META",
        "Company": "Meta Platforms",
        "Source_URL": "https://www.meta.com/de-de/about/",
        "Content_Type": "Website Content",
        "Raw_Text": "Shoppe brandneue Ger√§te und erweitere deine Welt\nLerne unsere F√ºhrungskr√§fte kennen\nDie F√ºhrungskr√§fte von Meta leiten unser Unternehmen bei der Weiterentwicklung von Mixed Reality und KI und tragen dazu bei, die n√§chste Evolutionsstufe der digitalen Verbindung zu verwirklichen.\nWir verwenden Cookies und √§hnliche Technologien, um Inhalte in Meta-Produkten bereitzustellen und zu verbessern. Dar√ºber hinaus verwenden wir sie, um mithilfe der durch Cookies auf und au√üerhalb von Meta empfangenen Informationen die Sicherheit zu verbessern sowie um Meta-Produkte f√ºr Personen, die ein Konto haben, bereitzustellen und zu verbessern.\nDu bestimmst, welche optionalen Cookies wir verwenden d√ºrfen. In unserer Cookie-Richtlinie erf√§hrst du mehr √ºber Cookies und wie wir sie verwenden. Dort kannst du deine Auswahl au√üerdem jederzeit √ºberpr√ºfen oder √§ndern.\nWenn du ein Facebook-Konto hast, kannst du mithilfe dieser Tools festlegen, wie unterschiedliche Daten zur Personalisierung von Werbeanzeigen verwendet werden.\nUm dir bessere Werbung zu zeigen, verwenden wir Informationen, die Werbetreibende und andere Partner uns zu deinen Aktivit√§ten au√üerhalb von Produkten der Meta-Unternehmen, zum Beispiel auf deren Websites und Apps, bereitstellen. In deinen Einstellungen f√ºr Werbung kannst du festlegen, ob wir diese Informationen verwenden d√ºrfen, um dir Werbung zu zeigen.\n√úber das Meta Audience Network k√∂nnen Werbetreibende dir Werbeanzeigen in Apps und auf Websites au√üerhalb der Produkte der Meta-Unternehmen zeigen. Um relevante Werbeanzeigen auszuliefern, bestimmt das Audience Network zum Beispiel anhand deiner Werbepr√§ferenzen, an welchen Werbeanzeigen du interessiert sein k√∂nntest. Diese kannst du in deinen Einstellungen f√ºr Werbung verwalten.\nIn den Werbepr√§ferenzen kannst du festlegen, ob wir dir Werbung zeigen sollen, und ausw√§hlen, welche Informationen wir daf√ºr verwenden d√ºrfen.\nDu kannst dir deine Aktivit√§ten au√üerhalb von Facebook ansehen. Dies ist eine Zusammenfassung deiner Interaktionen mit Unternehmen und Organisationen, die diese mit uns teilen. Als Interaktion gilt zum Beispiel, wenn du eine App √∂ffnest oder eine Website besuchst. Unternehmen und Organisationen verwenden unsere Business-Tools wie Facebook Login oder das Meta-Pixel, um diese Informationen mit uns zu teilen. So k√∂nnen wir dir ein individuelleres Nutzungserlebnis auf Meta-Produkten bieten. Erfahre mehr √ºber Aktivit√§ten au√üerhalb von Facebook, wie wir sie verwenden und wie du sie verwalten kannst.\nCookies sind kleine Textdateien, die zum Speichern und Empfangen von Kennungen in einem Webbrowser verwendet werden. Wir verwenden Cookies und √§hnliche Technologien, um Meta-Produkte anzubieten und um Informationen, die wir √ºber Nutzer erhalten, etwa zu ihren Aktivit√§ten auf anderen Websites und in anderen Apps, nachvollziehen zu k√∂nnen.\nSolltest du kein Konto haben, verwenden wir keine Cookies, um Werbeanzeigen f√ºr dich zu personalisieren. Informationen zu deinen Aktivit√§ten, die wir erhalten, verwenden wir lediglich f√ºr die Sicherheit und Integrit√§t unserer Produkte.\nIn unserer Cookie-Richtlinie erf√§hrst du mehr √ºber Cookies und wie wir sie verwenden.\nMithilfe von Cookies k√∂nnen wir die Meta-Produkte anbieten, sch√ºtzen und optimieren, beispielsweise indem wir Inhalte personalisieren, Werbeanzeigen individuell zuschneiden und ihre Performance messen, sowie ein sichereres Nutzungserlebnis erm√∂glichen.\nWelche Cookies wir verwenden, kann sich aufgrund von Optimierungen und Aktualisierungen der Meta-Produkte von Zeit zu Zeit √§ndern. Unabh√§ngig davon verwenden wir Cookies zu folgenden Zwecken:\nIn unserer Cookie-Richtlinie erf√§hrst du mehr √ºber Cookies und wie wir sie verwenden.\nZu den Meta-Produkten z√§hlen die Facebook-, Instagram- und Messenger-App sowie weitere in unserer Datenschutzrichtlinie aufgef√ºhrten Funktionen, Apps, Technologien, Software oder Dienste, die Meta anbietet.\nWeitere Infos zu Meta-Produkten findest du in unserer Datenschutzrichtlinie.\nDu kannst bestimmen, inwiefern wir optionale Cookies verwenden d√ºrfen:\nDu kannst diese Auswahl jederzeit in deinen Cookie-Einstellungen einsehen oder √§ndern."
    },
    {
        "Ticker": "META",
        "Company": "Meta Platforms",
        "Source_URL": "https://www.meta.com/blog/connect-2025-day-one-keynote-ai-glasses-ray-ban-display-neural-band-metaverse-news/",
        "Content_Type": "Website Content",
        "Raw_Text": "Mark Zuckerberg and Diplo just ran off the Connect stage ‚Äî literally ‚Äî to kickstart the after party, and we‚Äôre here to recap all the news you might‚Äôve missed. But first, let‚Äôs talk about another dynamic duo that‚Äôs taking the world by storm: glasses and AI.\nIt‚Äôs no secret that we‚Äôre working to bring personal superintelligence to everyone. And glasses are the ideal form factor to deliver it. It‚Äôs the only form factor that lets your AI see what you see, hear what you hear, and talk with you throughout the day. AI glasses can help improve your memory, heighten your senses, and let you seamlessly communicate with others ‚Äî all while staying fully present in the moment.\nWe design our AI glasses with three core values in mind:\n- They need to be awesome glasses, full stop. They should be light, feel comfortable, and look great.\n- The technology needs to get out of the way. We want to give you powerful tools when you want them that fade into the background when you don‚Äôt.\n- We take superintelligence seriously. Our glasses are engineered to get better over time. As the AI improves, your glasses do, too.\nAnd because good things come in threes, we‚Äôve got three major AI glasses announcements to share.\nRay-Ban Meta (Gen 2): Extended Battery Life, 3K Video, & More\nThe next generation of Ray-Ban Meta glasses is here. With up to 2x the battery life compared to its predecessor, Ray-Ban Meta (Gen 2) offers up to eight hours of mixed use as well as ultra HD 3K video recording ‚Äî more than double the previous number of pixels ‚Äî for sharper, smoother, move vivid content capture.\nWe announced conversation focus, a new feature coming soon that‚Äôll help you hear better in noisy environments. If you‚Äôre eating at a hot new restaurant, commuting on the train, or catching your favorite DJ‚Äôs latest set, conversation focus uses your AI glasses‚Äô open-ear speakers to amplify the voice of the person you‚Äôre talking to. And it‚Äôs coming to our existing Ray-Ban Meta and Oakley Meta HSTN glasses as a software update, too.\nWe‚Äôre enhancing live AI. As we make battery and energy efficiency optimizations, Meta AI will transition from something you prompt with a wake word to an always-available assistant. While all-day AI is a technical challenge we‚Äôre heads-down focusing on, you can currently use live AI for an hour or two.\nWe‚Äôre introducing three new limited-time-only seasonal colors and a limited-edition Wayfarer Matte Transparent frame with two lens color options to better match your personal style.\nThe next generation of Ray-Ban Meta is available now on meta.com and Ray-Ban.com starting at $379 USD. Learn more.\nOakley Meta Vanguard: The Next Era for Performance AI Glasses\nFor AI glasses to truly go mainstream, we need frames designed for different activities and aesthetics so they match whatever you‚Äôre into. Following up on the success of Oakley Meta HSTN, today we announced Oakley Meta Vanguard ‚Äî Performance AI glasses that combine classic Oakley style with the durability to withstand high-intensity sports and rugged outdoor adventures.\nThe battery can last up to nine hours of mixed use, helping you train for that marathon on a single charge. The camera is centered with a wider 122-degree field of view, and 3K video with stabilization means amazing footage whether you‚Äôre trailrunning or hitting the slopes.\nWe added two new capture modes ‚Äî Hyperlapse and Slow Motion ‚Äî so you can capture your epic adventures in new ways. And not to worry: Those capture modes are available on Ray-Ban Meta (Gen 2) and Oakley Meta HSTNs, too, so you‚Äôve got options.\nThe open-ear speakers are our most powerful yet ‚Äî six decibels louder than Oakley Meta HSTN. Combine that with our advanced wind noise reduction, and they‚Äôre great for running and biking in up to 30mph winds.\nWe‚Äôre integrating Garmin and Strava with the Meta AI app to take your performance to the next level. You‚Äôll be able to get real-time stats on the fly, completely hands-free, and Meta AI will give you workout summaries to help you track your progress over time.\nWe also built a new autocapture feature with Garmin that triggers your glasses to capture footage automatically based on your workout stats and milestones. When you hit a set distance or get a burst of speed, your glasses can grab a hands-free video without you even having to ask. And if you pair the Meta AI app with Garmin or Strava, you can even overlay performance stats on your videos before sharing them for bonus bragging rights.\nWith an IP67 rating, Oakley Meta Vanguard is definitely sweatproof and can even withstand temporary submersion in up to a meter of water. And they‚Äôre designed with swappable Oakley¬Æ PRIZM‚Ñ¢ Shield Lenses, so you can customize your look and optimize for various lighting conditions.\nPre-orders are open now, for $499 USD, and they‚Äôll hit shelves October 21. Learn more.\nMeta Ray-Ban Display: A Window Into the Future, Today\nWe introduced our first AI glasses with a high-resolution monocular display, which combine iconic Ray-Ban style with our brand-new Meta Neural Band.\nThe display itself ‚Äî which is large enough to read text messages or watch a small video ‚Äî is slightly offset to avoid disrupting your view, and it disappears after a few seconds when not in use so it doesn‚Äôt distract you. At 42 pixels per degree (ppd), it‚Äôs higher resolution than any of our consumer VR headsets. And with a custom light engine and waveguide delivering brightness up to 5,000 nits, it works great both indoors and outside ‚Äî even on bright, sunny days.\nAnd each pair of Meta Ray-Ban Display glasses comes with a Meta Neural Band. With 18 hours of battery life and a comfortable, water-resistant form factor, it‚Äôs a true scientific breakthrough, letting you silently control your Meta Ray-Ban Display glasses with subtle hand gestures. We‚Äôve been talking about our research investment in surface electromyography (sEMG) since 2021. We showed the world this tech could work with our Orion product prototype at last year‚Äôs Connect, and we published an update in Nature that details our most recent research findings. We‚Äôre thrilled to unveil this new wristworn product that puts seamless, intuitive control of your AI glasses quite literally at your fingertips.\nMeta Ray-Ban Display will be available for purchase September 30 for $799 USD, which includes the Meta Neural Band. You can sign up for a demo and find your perfect pair at select Best Buy, LensCrafters, and Ray-Ban Store locations in the US. Learn more.\nAI Glasses for Good\nWhether you‚Äôre capturing the moment hands-free while staying fully present, getting more information about the world around you on the fly, or making calls to friends and family, our AI glasses help keep you connected to the people and things that matter most.\nWe were so excited to learn that Blind and Low-Vision users and Blinded Veterans across America are using our AI glasses to experience the world more independently. They‚Äôre using Meta AI to describe their surroundings into audio descriptions, which lets them find things in a room, identify what‚Äôs in front of them, or read food labels in the grocery store. In fact, we‚Äôve learned that VA Blind Rehabilitation Centers are issuing Ray-Ban Meta glasses to support Blind and Low-Vision Veterans. That‚Äôs why the Blinded Veterans Association is developing a guide to teach them how to use the glasses to help with daily living and to navigate the world with greater autonomy, confidence, and connection.\nLast year at Connect, we announced our partnership with Be My Eyes, which connects people who are Blind or have Low Vision with sighted volunteers who then ‚Äúsee‚Äù on their behalf through the Ray-Ban Meta glasses and help talk them through various tasks. Since then, people have used their Ray-Ban Meta glasses to connect with volunteers hands-free to help them shop for groceries, cook a meal at home, pick out a birthday card, and more. With the continued expansion of Ray-Ban Meta into new markets, our Be My Eyes integration is becoming available in more and more new countries and languages with even more coming soon. And we‚Äôre exploring ways to make it even more seamless to call a volunteer, like by tapping a button on some of our glasses. More to come.\nMetaverse Momentum\nWe‚Äôre hard at work advancing the state of the art in augmented and virtual reality, too, and where those technologies meet AI ‚Äî that‚Äôs where you‚Äôll find the metaverse.\nOur vision for the future is a world where anyone anywhere can imagine a character, a scene, or an entire world and create it from scratch. There‚Äôs still a lot of work to do, but we‚Äôre making progress. In fact, we‚Äôre not far off from being able to create compelling 3D content as easily as you can ask Meta AI a question today. And that stands to transform not just the imagery and videos we see on platforms like Instagram and Facebook, but also the possibilities of VR and AR, too.\nFoundational Infrastructure: Meta Horizon Engine & Meta Horizon Studio\nMeta Horizon Engine has been built from scratch and optimized to help bring the metaverse to life. It powers better graphics, faster performance, and more advanced worlds. Developers and creators will be able to use it to easily generate nearly infinite connected spaces with realistic physics and interactions.\nWe introduced Meta Horizon Studio, a new editor and hub for creator capabilities. It includes the generative AI tools we‚Äôve added over the last year that let creators generate mesh, textures, TypeScript, audio, skyboxes, and more with simple text prompts, so they can make high-quality worlds in a fraction of the time. And soon, we‚Äôll add an agentic AI assistant to stitch together different tools and make the creation process even faster and easier.\nUsing Horizon Engine, we built a new Immersive Home for VR. You can customize it by pinning apps in different spots. And you can jump straight from your home to a series of interconnected worlds. Thanks to Horizon Engine, it‚Äôs 4x faster to load and render new worlds, so it takes just a few seconds.\nHorizon Engine also enables much greater concurrency, so more people can enjoy the same world at the same time. We can now support 100+ people in a single space ‚Äî 5x more than our previous engine could handle. Now the vibes match the experience, whether you‚Äôre catching a concert with a huge crowd in our new Arena or meeting up with friends in Horizon Central to explore new worlds.\nAnd eventually, you‚Äôll be able to seamlessly incorporate photorealistic spaces in worlds, too. Last year at Connect, we introduced a demo of Hyperscape, which uses Gaussian Splatting, cloud rendering, and streaming to make these types of spaces viewable on Meta Quest 3 and 3S headsets. And today, we‚Äôre rolling out Hyperscape Capture in Early Access, so you‚Äôll be able to use your Quest to scan a room in just a few minutes and then turn that into an immersive, photorealistic world of your own.\nEntertainment on the Horizon\nQuest continues to have the best slate of VR games, and it keeps getting better. Marvel‚Äôs Deadpool VR, ILM‚Äôs Star Wars: Beyond Victory, Resolution Games‚Äô Demeo x Dungeons & Dragons: Battlemarked, and nDreams Elevation‚Äôs Reach are all launching this fall. And entertainment all-up continues to grow in popularity. VR lets you enjoy your favorite content on a massive screen you can bring with you anywhere. And new formats are emerging that incorporate immersive elements to make the experience more magical.\nThat‚Äôs why we‚Äôre launching a new entertainment hub called Meta Horizon TV.* We‚Äôre partnering with some big names to bring the best in movies, TV, live sports, music, and more direct to your headset. And our upcoming platform investments will eliminate letterboxing, displaying media in a correctly sized window based on its aspect ratio. That means you‚Äôll get an optimal viewing experience, as the director originally intended.\nDisney+ is coming to Quest, including content from ESPN and Hulu, and will be available in the Meta Horizon Store and Meta Horizon TV. And we‚Äôre partnering with Universal Pictures, Nexus Studios, and iconic horror company Blumhouse, so you can watch movies like M3GAN and The Black Phone with immersive special effects you won‚Äôt find anywhere else.\nMeta Horizon TV supports Dolby Atmos sound, with support for Dolby Vision coming later this year. That means rich colors, crisp details, and spatial sound for a more immersive experience than a traditional TV can deliver.\nFinally, as an extension of our partnership with James Cameron‚Äôs Lightstorm Vision, we‚Äôre thrilled to share that an exclusive 3D clip of Avatar: Fire and Ash is available now in Meta Horizon TV for a limited time ‚Äî so go check it out now! This is just the beginning of how fans can experience Pandora like never before on Quest, following the film‚Äôs theatrical release this December.\nFuture Focus\nThe next computing platform continues to come into focus. VR headsets let us immerse ourselves in interactive worlds and experience best-in-class entertainment on a larger than life screen. AI glasses connect us to real-world superpowers as we move throughout the day, tapping into a wealth of information while staying present in the moment. And AR glasses will overlay rich digital content on top of our view of the physical world, opening up the door for new adventures we haven‚Äôt even dreamt of yet.\nHere‚Äôs to another year of progress building the future of human connection. We‚Äôll see you tomorrow for a recap of the day two keynote as Connect continues."
    },
    {
        "Ticker": "META",
        "Company": "Meta Platforms",
        "Source_URL": "https://www.meta.com/blog/meta-ray-ban-display-ai-glasses-connect-2025/",
        "Content_Type": "Website Content",
        "Raw_Text": "The wait is over. Meta Ray-Ban Display hits shelves in the US today! Priced at $799 USD, which includes the Meta Neural Band, these breakthrough AI glasses let you interact with digital content while staying fully present in the physical world.\nDemand for in-person demos of Meta Ray-Ban Display is strong, with appointments in most locations booked out through mid-October. We‚Äôre adding more stores to meet the demand, with new availability opening every day. If you‚Äôre unable to book a demo, you can still visit a store listed in our demo finder to learn more about the product.\nMeta Ray-Ban Display is available at limited brick-and-mortar retailers in the US, including at select Best Buy, LensCrafters, Sunglass Hut, Ray-Ban Stores, and Meta Lab. Availability in select Verizon stores will follow soon after.\nSchedule a demo near you and experience the next paradigm shift in wearable tech.\nToday at Connect, Mark Zuckerberg debuted the next exciting evolution of AI glasses: the all-new Meta Ray-Ban Display and Meta Neural Band.\nMeta Ray-Ban Display glasses are designed to help you look up and stay present. With a quick glance at the in-lens display, you can accomplish everyday tasks‚Äîlike checking messages, previewing photos, and collaborating with visual Meta AI prompts ‚Äî all without needing to pull out your phone. It‚Äôs technology that keeps you tuned in to the world around you, not distracted from it.\nThis new category of AI glasses comes with a full-color, high-resolution display that‚Äôs there when you want it ‚Äî and gone when you don‚Äôt. And it‚Äôs the first product that takes microphones, speakers, cameras, and a full-color display backed with compute and AI ‚Äî and puts it all together in a single device that‚Äôs stylish, comfortable, and easy to wear.\nEach pair comes with Meta Neural Band, an EMG wristband that interprets the natural signals created by your muscle activity to navigate the features of your glasses ‚Äî letting you control experiences intuitively using subtle hand movements, without having to touch your glasses or take out your phone. Meta Neural Band is so effortless, it makes interacting with your glasses feel like magic.\nPricing, Demos, & Details\nStarting at $799 USD, which includes both the glasses and Meta Neural Band, Meta Ray-Ban Display will let you experience, learn about, and interact with the world in a totally new way. It hits shelves September 30 at limited brick-and-mortar retailers in the US, including Best Buy, LensCrafters, Sunglass Hut and Ray-Ban Stores. Availability in select Verizon stores will follow soon after. Expansion to Canada, France, Italy, and the UK is planned for early 2026. We‚Äôre starting with select retailers and regions to make sure customers get the glasses and band that‚Äôs perfect for them, and we‚Äôll expand buying options over time.\nMeta Ray-Ban Display and the accompanying Meta Neural Band come in two colors: Black and Sand. We know style options are important, so in addition to launching in two colors, we‚Äôre launching with two frame sizes: standard and large. All glasses feature Transitions¬Æ lenses, so you can wear them on-the-go, inside and outside, night and day, with up to six hours of mixed-use battery life and up to 30 hours of battery life total thanks to the portable (and collapsible!) charging case. And Meta Neural Band comes in three sizes so you can find the perfect fit.\nBe one of the first to experience the next paradigm shift in wearable tech. Book an in-person demo to try them out, get fitted, and pick the right pair for you.\nStyle Meets Function\nWhen we started building this product with EssilorLuxottica, our goal was simple: make them look and feel great. So we integrated a premium eyewear design, making the form factor just as bold as the technology inside.\nMeta Ray-Ban Display has the iconic DNA of a Wayfarer silhouette, which we made taller and gave a more prominent square shape for a bold aesthetic. We slightly rounded off the edges, which improves comfort, elevates the aesthetics, and increases durability ‚Äî and gave the front of the glasses a slight curve, which helps reduce glare while improving comfort and fit.\nWe designed the over-extension hinges with titanium to enhance wearability, minimize weight, and maximize strength.\nWe used ultra-narrow steelcan batteries ‚Äî a first for this form factor ‚Äî which allowed for thinner temple arms, while supporting extended battery life. We started this battery development with display glasses in mind, but like a lot of our work, it made its way into other products like Oakley Meta HSTN and Ray-Ban Meta (Gen 2) glasses, increasing battery life up to eight hours with typical use.\nAnd we achieved all of this with an impressive weight of just 69 grams.\nDisplay Deep Dive\nBecause everything needed to fit in a fashionable pair of glasses worthy of the Ray-Ban name, we rearchitected our entire display to fit the form factor. The monocular display features a highly custom light engine and geometric waveguide. It has great compute efficiency, sharp contrast, and high brightness, thanks to a custom module that was miniaturized to fit into a fashionable frame. This was key to achieving high resolution without having to increase the frame size.\nWe packed 42 pixels into each degree of the field of view to improve the display. Fun fact: The only wave guide device out there with > 42 pixels per degree (ppd) is a giant headset that isn‚Äôt sold commercially anymore.\nThe photochromatic lenses and our auto-brightness algorithms ensure visuals are crisp and colorful whether you‚Äôre indoors or outside. Of course, it‚Äôs important to feel comfortable wearing your display glasses around others. We balanced the brightness with the need for subtlety, so the private in-lens display only has 2% light leakage. And just like with Ray-Ban Meta glasses, the capture LED lets others know when you use your glasses to take photos and record videos for your gallery or streaming.\nDesigning Meta Ray-Ban Display involved a balance between lighting up the display with visuals and helping you stay connected to the people and things around you. The display is placed off to the side, so it doesn‚Äôt obstruct your view. And it isn‚Äôt on constantly ‚Äî it‚Äôs designed for short interactions that you‚Äôre always in control of. This isn't about strapping a phone to your face. It's about helping you quickly accomplish some of your everyday tasks without breaking your flow.\nEMG: The Science of Seamless Interaction\nEvery new computing platform comes with new ways to interact, and we‚Äôre really excited about our Meta Neural Band, which packs cutting-edge surface electromyography research into a stylish input device. It replaces the touchscreens, buttons, and dials of today‚Äôs technology with a sensor on your wrist, so you can silently scroll, click, and, in the near future, even write out messages using subtle finger movements.\nThe amount of signals the band can detect is incredible ‚Äî it has the fidelity to measure movement even before it‚Äôs visually perceptible.\nThe key breakthrough was developing deep learning algorithms based on data from nearly 200,000 consenting research participants, so this wristband will work right out of the box for nearly anyone. That‚Äôs a huge technical feat because of the level of variance in people‚Äôs muscles. Think of the potential impact it could have for people with spinal cord injuries, limb differences, tremors, or other neuromotor conditions.\nWith thousands upon thousands of motor neurons that could theoretically be mapped to any gesture, EMG technology can go beyond glasses. It could be the best way to control any device.\nAll processing of raw EMG data happens on-device. Only events like a ‚Äúclick‚Äù are sent from the wristband to the glasses so a command can be executed. This on-device machine learning also enables high reliability for haptic feedback and time synchronization.\nWe built the Meta Neural band to be durable, lightweight, and comfortable for all-day wear with up to 18 hours of battery life and an IPX7 water rating. The band features electrodes coated with diamond-like carbon, which are then reinforced by Vectran ‚Äî a woven mesh that‚Äôs as strong as steel when pulled, yet soft enough to bend easily. Vectran is the same material used on the crash pads of the Mars Rover to ensure a smooth landing, so yeah. Science!\nExperiences in Meta Ray-Ban Display\nMeta Ray-Ban Display takes everything you know and love about Ray-Ban Meta glasses and supercharges it to give you more context, more functionality, and more freedom than ever before. We‚Äôve built the experiences in Meta Ray-Ban Display to boost your confidence and give you more clarity in everything you do and see through them.\nWe can now take the beloved features from Ray-Ban Meta to the next level, while adding new ones thanks to the addition of a hi-res display and neural band. Here‚Äôs a few of the experiences that make Meta Ray-Ban Display so special:\n- Meta AI with Visuals: Meta AI on glasses can do so much more when it‚Äôs paired with visuals ‚Äî now it can show you answers and step-by-step how-tos, rather than just reading something back to you. Move through steps easily with a quick swipe of your thumb side to side using the Meta Neural Band.\n- Messaging & Video Calling: Staying connected while staying in the moment is so much easier when your glasses show you short texts, WhatsApp messages, or Reels your friends are sharing. With Meta Ray-Ban Display, you can privately view text and multimedia messages from WhatsApp, Messenger, Instagram, and your phone, hands-free with just a pinch. You can also take live video calls from WhatsApp and Messenger and show friends what you‚Äôre seeing through the glasses.\n- Preview & Zoom: The real-time camera viewfinder and zoom functionality helps you get the perfect shot on the first try ‚Äî and the display makes it easy to select and share your favorite photos and videos.\n- Pedestrian Navigation: Get where you need to go, with phone-free, walking directions. Select your destination and get turn-by-turn walking directions with a visual map of the area shown on your glasses‚Äô display. We‚Äôll launch pedestrian navigation for select cities in beta and will continue to add more over time.\n- Live Captions & Translation: Meta Ray-Ban Display breaks down barriers with its live captioning feature. When prompted, it can display captions for the speech that‚Äôs directed at you, or translate select languages for you in real-time ‚Äî all while you stay present and engaged in the conversation.\n- Music Playback: See what you‚Äôre listening to in real time with the music card shown on the glasses‚Äô display. Swipe left and right with your thumb to navigate music, or pinch your fingers and rotate your wrist to turn the volume, just like you‚Äôre dialing up a speaker IRL.\nMeta Ray-Ban Display also gives you a hands-free, intelligent music experience ‚Äî combining rich on-lens visuals, voice confirmation, and discreet EMG gesture control. You‚Äôve got access to Amazon Music, Apple Music (iOS only), Music Info, Shazam, and Spotify in English everywhere Meta AI glasses are supported.\nComing soon, you‚Äôll be able to browse music content intuitively and play music that matches your view thanks to multimodal AI ‚Äî so if you‚Äôre relaxing on the beach, looking out at the sunset over the ocean, you can ask Meta AI for a track or playlist that fits the scene and Spotify will help you find the best music for the moment. This will make it easier to explore, control, and personalize your listening experience anywhere. And you‚Äôll be able to enjoy podcasts and audiobooks from Audible alongside live radio stations from iHeart, too.\nJust like we‚Äôve done with Ray-Ban Meta Glasses, we‚Äôll continue to build new experiences alongside the community and deliver on more of what they want. And we‚Äôve got an exciting slate of software updates landing in the coming months for Meta Ray-Ban Display ‚Äî a dedicated Instagram Reels app, EMG handwriting, and more.\nAll in the Family\nWith today‚Äôs introduction of Meta Ray-Ban Display, our AI glasses now fall into three distinct categories:\n- Camera AI glasses: We have the world‚Äôs best brands in eyewear with Ray-Ban and Oakley. And we‚Äôll continue to expand to more styles and brands in the future, and deliver more AI-enabled features.\n- Display AI glasses: Meta Ray-Ban Display opens up a new category, anchored on the value of a powerful, contextual display.\n- Augmented reality glasses: AR glasses, like our Orion prototype, feature a large holographic display, high-bandwidth input, and digital experiences that augment the world around you.\nAnd yes, for those keeping score at home: We‚Äôre continuing to make progress on a consumer version of our breakthrough Orion prototype that we showed at Connect last year. Stay tuned... ü§ì\nFuture Forward\nToday marks the start of the next chapter, not only for AI glasses, but for the future of wearable technology and its ability to connect us while expanding our interactions with the world. It‚Äôs time to look forward."
    },
    {
        "Ticker": "META",
        "Company": "Meta Platforms",
        "Source_URL": "https://www.meta.com/blog/",
        "Content_Type": "Website Content",
        "Raw_Text": "Die Verwendung von Cookies durch Meta in diesem Browser erlauben?\nWir verwenden Cookies und √§hnliche Technologien, um Inhalte in Meta-Produkten bereitzustellen und zu verbessern. Dar√ºber hinaus verwenden wir sie, um mithilfe der durch Cookies auf und au√üerhalb von Meta Quest empfangenen Informationen die Sicherheit zu verbessern sowie um Meta-Produkte f√ºr Personen, die ein Konto haben, bereitzustellen und zu verbessern.\n‚Ä¢\nErforderliche Cookies: Diese Cookies sind notwendig f√ºr die Nutzung von Meta-Produkten und die ordnungsgem√§√üe Funktion unserer Websites.\n‚Ä¢\nCookies anderer Unternehmen: Wir verwenden diese Cookies, um dir Werbeanzeigen au√üerhalb von Meta-Produkten zu zeigen und Funktionen wie Karten oder Videos in Meta-Produkten anbieten zu k√∂nnen. Hierbei handelt es sich um optionale Cookies.\nDu bestimmst, welche optionalen Cookies wir verwenden d√ºrfen. In unserer Cookie-Richtlinie erf√§hrst du mehr √ºber Cookies und wie wir sie verwenden. Dort kannst du deine Auswahl au√üerdem jederzeit √ºberpr√ºfen oder √§ndern.\nInfos zu Cookies\nWas sind Cookies?\nMehr dazu\nWarum verwenden wir Cookies?\nMehr dazu\nWas sind Meta-Produkte?\nMehr dazu\nDeine Cookie-Auswahl\nMehr dazu\nCookies anderer Unternehmen\nCookies nach Kategorie ausw√§hlen\nSo verwenden wir diese Cookies\nWenn du diese Cookies erlaubst:\nWenn du diese Cookies nicht erlaubst:\nAndere M√∂glichkeiten, um deine Informationen zu kontrollieren\nPersonalisiere dein Werbeerlebnis in der Konten√ºbersicht\nWeitere Informationen zu Onlinewerbung\nCookies √ºber die Browser-Einstellungen kontrollieren"
    },
    {
        "Ticker": "META",
        "Company": "Meta Platforms",
        "Source_URL": "https://www.meta.com/de-de/about/company-info/",
        "Content_Type": "Website Content",
        "Raw_Text": "UNSERE MISSION\nWir gestalten die Zukunft menschlicher Verbindungen und die Technologie, die sie erm√∂glicht\nUnsere Prinzipien\nMeta baut eine Technologie auf, die dich mit Personen, Themen und Erlebnissen verbindet, die dir wichtig sind. Unsere Prinzipien beschreiben, wof√ºr wir stehen, und leiten uns bei der Entwicklung unserer Produkte.\nMenschen haben das Recht, geh√∂rt zu werden und sich zu √§u√üern. Das bedeutet manchmal auch, dass wir uns f√ºr Menschen einsetzen, deren Meinung wir nicht teilen.\nUnsere Dienste helfen Menschen, sich miteinander zu vernetzen und die Welt n√§her zusammenzubringen.\nWir m√∂chten hilfreiche Technologien allen Menschen zur Verf√ºgung stellen. Da unser Gesch√§ftsmodell auf Werbeanzeigen beruht, k√∂nnen wir unsere Dienste kostenlos anbieten.\nWir verstehen es als unsere Verantwortung, Menschen zusammenzubringen, zu f√∂rdern, sie zu sch√ºtzen und Risiken abzuwenden, damit sie gemeinsam Gro√ües schaffen k√∂nnen.\nUnsere Tools schaffen gleiche Voraussetzungen, damit Unternehmen wachsen, Arbeitspl√§tze schaffen und die Wirtschaft st√§rken k√∂nnen.\nUnsere Geschichte\nAls Facebook 2004 gegr√ºndet wurde, revolutionierte es die Art und Weise, wie Menschen sich miteinander vernetzen und austauschen. Apps wie der Messenger, Instagram und WhatsApp haben Milliarden Menschen neue M√∂glichkeiten der Kommunikation gegeben. Jetzt √ºberwindet Meta die Grenzen der Zweidimensionalit√§t und setzt auf immersive Erlebnisse in Mixed Reality und Innovationen mit KI, um einen wichtigen Beitrag zur Weiterentwicklung sozialer Technologien zu leisten. Sieh dir an, wie Meta im Laufe der Zeit gewachsen ist:\n25. September 2024\nOrion, eine AR-Brille, von der wir glauben, dass sie die fortschrittlichste √ºberhaupt ist, wird pr√§sentiert.\n18. M√§rz 2025\nLlama, unsere Sammlung von Open-Source-KI-Modellen, hat 1 Milliarde Downloads geknackt.\n20. Juni 2025\nDie Oakley Meta Brille, eine neue Kategorie von Performance-AI Glasses, kommt auf den Markt.\nMach mit!\nBei Meta m√∂chten wir, dass du eine Karriere aufbaust, in der du gefordert, wertgesch√§tzt und von den Besten lernst. Schlie√üe dich uns an, um die Zukunft menschlicher Verbindungen zu gestalten und die Technologie, die sie erm√∂glicht.\nBei Meta arbeiten\nDer Meta Careers Blog erm√∂glicht es dir, mehr √ºber unsere Arbeit zu erfahren und pers√∂nliche Insights von den Mitarbeitenden bei Meta zu erhalten, w√§hrend du deine eigene Karriere-Journey navigierst.\nForschung bei Meta\nEgal, ob du ein Postdoktorand auf der Suche nach einer Vollzeit-Forschungsposition bist oder ein Doktorand, der an einem kurzfristigen Praktikum interessiert ist ‚Äì bei Meta kannst du die Begrenzungen verschieben und Ideen in Innovationen verwandeln.\nLerne das Team von Meta kennen\nErfahre mehr dar√ºber, wie es ist, bei Meta zu arbeiten.\nWir verwenden Cookies und √§hnliche Technologien, um Inhalte in Meta-Produkten bereitzustellen und zu verbessern. Dar√ºber hinaus verwenden wir sie, um mithilfe der durch Cookies auf und au√üerhalb von Meta empfangenen Informationen die Sicherheit zu verbessern sowie um Meta-Produkte f√ºr Personen, die ein Konto haben, bereitzustellen und zu verbessern.\nDu bestimmst, welche optionalen Cookies wir verwenden d√ºrfen. In unserer Cookie-Richtlinie erf√§hrst du mehr √ºber Cookies und wie wir sie verwenden. Dort kannst du deine Auswahl au√üerdem jederzeit √ºberpr√ºfen oder √§ndern.\nWenn du ein Facebook-Konto hast, kannst du mithilfe dieser Tools festlegen, wie unterschiedliche Daten zur Personalisierung von Werbeanzeigen verwendet werden.\nUm dir bessere Werbung zu zeigen, verwenden wir Informationen, die Werbetreibende und andere Partner uns zu deinen Aktivit√§ten au√üerhalb von Produkten der Meta-Unternehmen, zum Beispiel auf deren Websites und Apps, bereitstellen. In deinen Einstellungen f√ºr Werbung kannst du festlegen, ob wir diese Informationen verwenden d√ºrfen, um dir Werbung zu zeigen.\n√úber das Meta Audience Network k√∂nnen Werbetreibende dir Werbeanzeigen in Apps und auf Websites au√üerhalb der Produkte der Meta-Unternehmen zeigen. Um relevante Werbeanzeigen auszuliefern, bestimmt das Audience Network zum Beispiel anhand deiner Werbepr√§ferenzen, an welchen Werbeanzeigen du interessiert sein k√∂nntest. Diese kannst du in deinen Einstellungen f√ºr Werbung verwalten.\nIn den Werbepr√§ferenzen kannst du festlegen, ob wir dir Werbung zeigen sollen, und ausw√§hlen, welche Informationen wir daf√ºr verwenden d√ºrfen.\nDu kannst dir deine Aktivit√§ten au√üerhalb von Facebook ansehen. Dies ist eine Zusammenfassung deiner Interaktionen mit Unternehmen und Organisationen, die diese mit uns teilen. Als Interaktion gilt zum Beispiel, wenn du eine App √∂ffnest oder eine Website besuchst. Unternehmen und Organisationen verwenden unsere Business-Tools wie Facebook Login oder das Meta-Pixel, um diese Informationen mit uns zu teilen. So k√∂nnen wir dir ein individuelleres Nutzungserlebnis auf Meta-Produkten bieten. Erfahre mehr √ºber Aktivit√§ten au√üerhalb von Facebook, wie wir sie verwenden und wie du sie verwalten kannst.\nCookies sind kleine Textdateien, die zum Speichern und Empfangen von Kennungen in einem Webbrowser verwendet werden. Wir verwenden Cookies und √§hnliche Technologien, um Meta-Produkte anzubieten und um Informationen, die wir √ºber Nutzer erhalten, etwa zu ihren Aktivit√§ten auf anderen Websites und in anderen Apps, nachvollziehen zu k√∂nnen.\nSolltest du kein Konto haben, verwenden wir keine Cookies, um Werbeanzeigen f√ºr dich zu personalisieren. Informationen zu deinen Aktivit√§ten, die wir erhalten, verwenden wir lediglich f√ºr die Sicherheit und Integrit√§t unserer Produkte.\nIn unserer Cookie-Richtlinie erf√§hrst du mehr √ºber Cookies und wie wir sie verwenden.\nMithilfe von Cookies k√∂nnen wir die Meta-Produkte anbieten, sch√ºtzen und optimieren, beispielsweise indem wir Inhalte personalisieren, Werbeanzeigen individuell zuschneiden und ihre Performance messen, sowie ein sichereres Nutzungserlebnis erm√∂glichen.\nWelche Cookies wir verwenden, kann sich aufgrund von Optimierungen und Aktualisierungen der Meta-Produkte von Zeit zu Zeit √§ndern. Unabh√§ngig davon verwenden wir Cookies zu folgenden Zwecken:\nIn unserer Cookie-Richtlinie erf√§hrst du mehr √ºber Cookies und wie wir sie verwenden.\nZu den Meta-Produkten z√§hlen die Facebook-, Instagram- und Messenger-App sowie weitere in unserer Datenschutzrichtlinie aufgef√ºhrten Funktionen, Apps, Technologien, Software oder Dienste, die Meta anbietet.\nWeitere Infos zu Meta-Produkten findest du in unserer Datenschutzrichtlinie.\nDu kannst bestimmen, inwiefern wir optionale Cookies verwenden d√ºrfen:\nDu kannst diese Auswahl jederzeit in deinen Cookie-Einstellungen einsehen oder √§ndern."
    },
    {
        "Ticker": "META",
        "Company": "Meta Platforms",
        "Source_URL": "https://www.meta.com/de-de/media-gallery/",
        "Content_Type": "Website Content",
        "Raw_Text": "Wir verwenden Cookies und √§hnliche Technologien, um Inhalte in Meta-Produkten bereitzustellen und zu verbessern. Dar√ºber hinaus verwenden wir sie, um mithilfe der durch Cookies auf und au√üerhalb von Meta empfangenen Informationen die Sicherheit zu verbessern sowie um Meta-Produkte f√ºr Personen, die ein Konto haben, bereitzustellen und zu verbessern.\nDu bestimmst, welche optionalen Cookies wir verwenden d√ºrfen. In unserer Cookie-Richtlinie erf√§hrst du mehr √ºber Cookies und wie wir sie verwenden. Dort kannst du deine Auswahl au√üerdem jederzeit √ºberpr√ºfen oder √§ndern.\nWenn du ein Facebook-Konto hast, kannst du mithilfe dieser Tools festlegen, wie unterschiedliche Daten zur Personalisierung von Werbeanzeigen verwendet werden.\nUm dir bessere Werbung zu zeigen, verwenden wir Informationen, die Werbetreibende und andere Partner uns zu deinen Aktivit√§ten au√üerhalb von Produkten der Meta-Unternehmen, zum Beispiel auf deren Websites und Apps, bereitstellen. In deinen Einstellungen f√ºr Werbung kannst du festlegen, ob wir diese Informationen verwenden d√ºrfen, um dir Werbung zu zeigen.\n√úber das Meta Audience Network k√∂nnen Werbetreibende dir Werbeanzeigen in Apps und auf Websites au√üerhalb der Produkte der Meta-Unternehmen zeigen. Um relevante Werbeanzeigen auszuliefern, bestimmt das Audience Network zum Beispiel anhand deiner Werbepr√§ferenzen, an welchen Werbeanzeigen du interessiert sein k√∂nntest. Diese kannst du in deinen Einstellungen f√ºr Werbung verwalten.\nIn den Werbepr√§ferenzen kannst du festlegen, ob wir dir Werbung zeigen sollen, und ausw√§hlen, welche Informationen wir daf√ºr verwenden d√ºrfen.\nDu kannst dir deine Aktivit√§ten au√üerhalb von Facebook ansehen. Dies ist eine Zusammenfassung deiner Interaktionen mit Unternehmen und Organisationen, die diese mit uns teilen. Als Interaktion gilt zum Beispiel, wenn du eine App √∂ffnest oder eine Website besuchst. Unternehmen und Organisationen verwenden unsere Business-Tools wie Facebook Login oder das Meta-Pixel, um diese Informationen mit uns zu teilen. So k√∂nnen wir dir ein individuelleres Nutzungserlebnis auf Meta-Produkten bieten. Erfahre mehr √ºber Aktivit√§ten au√üerhalb von Facebook, wie wir sie verwenden und wie du sie verwalten kannst.\nCookies sind kleine Textdateien, die zum Speichern und Empfangen von Kennungen in einem Webbrowser verwendet werden. Wir verwenden Cookies und √§hnliche Technologien, um Meta-Produkte anzubieten und um Informationen, die wir √ºber Nutzer erhalten, etwa zu ihren Aktivit√§ten auf anderen Websites und in anderen Apps, nachvollziehen zu k√∂nnen.\nSolltest du kein Konto haben, verwenden wir keine Cookies, um Werbeanzeigen f√ºr dich zu personalisieren. Informationen zu deinen Aktivit√§ten, die wir erhalten, verwenden wir lediglich f√ºr die Sicherheit und Integrit√§t unserer Produkte.\nIn unserer Cookie-Richtlinie erf√§hrst du mehr √ºber Cookies und wie wir sie verwenden.\nMithilfe von Cookies k√∂nnen wir die Meta-Produkte anbieten, sch√ºtzen und optimieren, beispielsweise indem wir Inhalte personalisieren, Werbeanzeigen individuell zuschneiden und ihre Performance messen, sowie ein sichereres Nutzungserlebnis erm√∂glichen.\nWelche Cookies wir verwenden, kann sich aufgrund von Optimierungen und Aktualisierungen der Meta-Produkte von Zeit zu Zeit √§ndern. Unabh√§ngig davon verwenden wir Cookies zu folgenden Zwecken:\nIn unserer Cookie-Richtlinie erf√§hrst du mehr √ºber Cookies und wie wir sie verwenden.\nZu den Meta-Produkten z√§hlen die Facebook-, Instagram- und Messenger-App sowie weitere in unserer Datenschutzrichtlinie aufgef√ºhrten Funktionen, Apps, Technologien, Software oder Dienste, die Meta anbietet.\nWeitere Infos zu Meta-Produkten findest du in unserer Datenschutzrichtlinie.\nDu kannst bestimmen, inwiefern wir optionale Cookies verwenden d√ºrfen:\nDu kannst diese Auswahl jederzeit in deinen Cookie-Einstellungen einsehen oder √§ndern."
    },
    {
        "Ticker": "META",
        "Company": "Meta Platforms",
        "Source_URL": "https://www.meta.com/de-de/media-gallery/executives/",
        "Content_Type": "Website Content",
        "Raw_Text": "Wir verwenden Cookies und √§hnliche Technologien, um Inhalte in Meta-Produkten bereitzustellen und zu verbessern. Dar√ºber hinaus verwenden wir sie, um mithilfe der durch Cookies auf und au√üerhalb von Meta empfangenen Informationen die Sicherheit zu verbessern sowie um Meta-Produkte f√ºr Personen, die ein Konto haben, bereitzustellen und zu verbessern.\nDu bestimmst, welche optionalen Cookies wir verwenden d√ºrfen. In unserer Cookie-Richtlinie erf√§hrst du mehr √ºber Cookies und wie wir sie verwenden. Dort kannst du deine Auswahl au√üerdem jederzeit √ºberpr√ºfen oder √§ndern.\nWenn du ein Facebook-Konto hast, kannst du mithilfe dieser Tools festlegen, wie unterschiedliche Daten zur Personalisierung von Werbeanzeigen verwendet werden.\nUm dir bessere Werbung zu zeigen, verwenden wir Informationen, die Werbetreibende und andere Partner uns zu deinen Aktivit√§ten au√üerhalb von Produkten der Meta-Unternehmen, zum Beispiel auf deren Websites und Apps, bereitstellen. In deinen Einstellungen f√ºr Werbung kannst du festlegen, ob wir diese Informationen verwenden d√ºrfen, um dir Werbung zu zeigen.\n√úber das Meta Audience Network k√∂nnen Werbetreibende dir Werbeanzeigen in Apps und auf Websites au√üerhalb der Produkte der Meta-Unternehmen zeigen. Um relevante Werbeanzeigen auszuliefern, bestimmt das Audience Network zum Beispiel anhand deiner Werbepr√§ferenzen, an welchen Werbeanzeigen du interessiert sein k√∂nntest. Diese kannst du in deinen Einstellungen f√ºr Werbung verwalten.\nIn den Werbepr√§ferenzen kannst du festlegen, ob wir dir Werbung zeigen sollen, und ausw√§hlen, welche Informationen wir daf√ºr verwenden d√ºrfen.\nDu kannst dir deine Aktivit√§ten au√üerhalb von Facebook ansehen. Dies ist eine Zusammenfassung deiner Interaktionen mit Unternehmen und Organisationen, die diese mit uns teilen. Als Interaktion gilt zum Beispiel, wenn du eine App √∂ffnest oder eine Website besuchst. Unternehmen und Organisationen verwenden unsere Business-Tools wie Facebook Login oder das Meta-Pixel, um diese Informationen mit uns zu teilen. So k√∂nnen wir dir ein individuelleres Nutzungserlebnis auf Meta-Produkten bieten. Erfahre mehr √ºber Aktivit√§ten au√üerhalb von Facebook, wie wir sie verwenden und wie du sie verwalten kannst.\nCookies sind kleine Textdateien, die zum Speichern und Empfangen von Kennungen in einem Webbrowser verwendet werden. Wir verwenden Cookies und √§hnliche Technologien, um Meta-Produkte anzubieten und um Informationen, die wir √ºber Nutzer erhalten, etwa zu ihren Aktivit√§ten auf anderen Websites und in anderen Apps, nachvollziehen zu k√∂nnen.\nSolltest du kein Konto haben, verwenden wir keine Cookies, um Werbeanzeigen f√ºr dich zu personalisieren. Informationen zu deinen Aktivit√§ten, die wir erhalten, verwenden wir lediglich f√ºr die Sicherheit und Integrit√§t unserer Produkte.\nIn unserer Cookie-Richtlinie erf√§hrst du mehr √ºber Cookies und wie wir sie verwenden.\nMithilfe von Cookies k√∂nnen wir die Meta-Produkte anbieten, sch√ºtzen und optimieren, beispielsweise indem wir Inhalte personalisieren, Werbeanzeigen individuell zuschneiden und ihre Performance messen, sowie ein sichereres Nutzungserlebnis erm√∂glichen.\nWelche Cookies wir verwenden, kann sich aufgrund von Optimierungen und Aktualisierungen der Meta-Produkte von Zeit zu Zeit √§ndern. Unabh√§ngig davon verwenden wir Cookies zu folgenden Zwecken:\nIn unserer Cookie-Richtlinie erf√§hrst du mehr √ºber Cookies und wie wir sie verwenden.\nZu den Meta-Produkten z√§hlen die Facebook-, Instagram- und Messenger-App sowie weitere in unserer Datenschutzrichtlinie aufgef√ºhrten Funktionen, Apps, Technologien, Software oder Dienste, die Meta anbietet.\nWeitere Infos zu Meta-Produkten findest du in unserer Datenschutzrichtlinie.\nDu kannst bestimmen, inwiefern wir optionale Cookies verwenden d√ºrfen:\nDu kannst diese Auswahl jederzeit in deinen Cookie-Einstellungen einsehen oder √§ndern."
    },
    {
        "Ticker": "META",
        "Company": "Meta Platforms",
        "Source_URL": "https://www.meta.com/de-de/blog/",
        "Content_Type": "Website Content",
        "Raw_Text": "Die Verwendung von Cookies durch Meta in diesem Browser erlauben?\nWir verwenden Cookies und √§hnliche Technologien, um Inhalte in Meta-Produkten bereitzustellen und zu verbessern. Dar√ºber hinaus verwenden wir sie, um mithilfe der durch Cookies auf und au√üerhalb von Meta Quest empfangenen Informationen die Sicherheit zu verbessern sowie um Meta-Produkte f√ºr Personen, die ein Konto haben, bereitzustellen und zu verbessern.\n‚Ä¢\nErforderliche Cookies: Diese Cookies sind notwendig f√ºr die Nutzung von Meta-Produkten und die ordnungsgem√§√üe Funktion unserer Websites.\n‚Ä¢\nCookies anderer Unternehmen: Wir verwenden diese Cookies, um dir Werbeanzeigen au√üerhalb von Meta-Produkten zu zeigen und Funktionen wie Karten oder Videos in Meta-Produkten anbieten zu k√∂nnen. Hierbei handelt es sich um optionale Cookies.\nDu bestimmst, welche optionalen Cookies wir verwenden d√ºrfen. In unserer Cookie-Richtlinie erf√§hrst du mehr √ºber Cookies und wie wir sie verwenden. Dort kannst du deine Auswahl au√üerdem jederzeit √ºberpr√ºfen oder √§ndern.\nInfos zu Cookies\nWas sind Cookies?\nMehr dazu\nWarum verwenden wir Cookies?\nMehr dazu\nWas sind Meta-Produkte?\nMehr dazu\nDeine Cookie-Auswahl\nMehr dazu\nCookies anderer Unternehmen\nCookies nach Kategorie ausw√§hlen\nSo verwenden wir diese Cookies\nWenn du diese Cookies erlaubst:\nWenn du diese Cookies nicht erlaubst:\nAndere M√∂glichkeiten, um deine Informationen zu kontrollieren\nPersonalisiere dein Werbeerlebnis in der Konten√ºbersicht\nWeitere Informationen zu Onlinewerbung\nCookies √ºber die Browser-Einstellungen kontrollieren"
    },
    {
        "Ticker": "META",
        "Company": "Meta Platforms",
        "Source_URL": "https://www.meta.com/blog/ray-ban-meta-gen-2-now-available-ai-glasses-extended-battery-life-3k-video/",
        "Content_Type": "Website Content",
        "Raw_Text": "The world‚Äôs best-selling AI glasses* just got even better. Announced today at Connect, Ray-Ban Meta (Gen 2) features up to 2x the battery life and 3K Ultra HD video capture, and it‚Äôs available right now** ‚Äî pick up your pair at meta.com and Ray-Ban.com starting at $379 USD.\nIt‚Äôs been an exciting year for Ray-Ban Meta, with the release of multiple new styles, groundbreaking new features like live AI and live translation, and appearances at Paris Fashion Week and the Met Gala. Ray-Ban Meta glasses have become the #1 selling AI glasses in the world, with millions of units sold since launch. People love them ‚Äî as a hands-free camera for life‚Äôs most important moments, a convenient AI assistant, a real-time translator, and an open-ear audio player that lets you stay engaged with your surroundings, all within iconic styles that are perfect for everyday wear.\nWith Ray-Ban Meta (Gen 2), we‚Äôve maintained all the features you love, but with some key under-the-hood improvements that will make them an even better companion.\nFirst and foremost, we‚Äôve enhanced the battery life. A fully-charged pair of Ray-Ban Meta (Gen 2) glasses can last up to eight hours with typical use. That‚Äôs almost twice the battery life of Ray-Ban Meta (Gen 1), making them perfect for morning-to-night music festivals or your outdoor adventures.\nBetter yet, you can charge Ray-Ban Meta (Gen 2) up to 50% in just 20 minutes, minimizing downtime on all-day excursions. And for longer outings, Ray-Ban Meta (Gen 2) comes with a charging case that provides an additional 48 hours of charging on-the-go. Capture more photos and videos, make more memories, and spend less time charging.\nWe‚Äôve also added 3K Ultra HD video capture to Ray-Ban Meta (Gen 2), which is 2x more pixels than the previous generation. And later this fall, you‚Äôll be able to capture sharper, smoother, and more vivid videos in ultrawide HDR and up to 60 frames per second. Hands-free filming is also getting upgraded with two new capture modes: hyperlapse and slow motion.\nNew capture settings are just one example of how Ray-Ban Meta glasses continue to get better and more capable over time with regular updates and new features. At Connect, we revealed conversation focus, a new feature coming soon to AI glasses. Conversation focus uses AI glasses‚Äô open-ear speakers to amplify the voice of the person you‚Äôre talking to, helping distinguish them from ambient background noise in cafes and restaurants, parks, and other busy places. Conversation focus will soon be available as a software update on Ray-Ban Meta and Oakley Meta HSTN glasses.\nWe‚Äôre also expanding live translation to support German and Portuguese starting today. Soon, you‚Äôll be able to have back-and-forth conversations between six languages ‚Äî even when you‚Äôre in airplane mode, so long as you‚Äôve downloaded the language pack in advance.\nRay-Ban Meta (Gen 2) lets you match your look to your lifestyle, whether you prefer the legendary Wayfarer, the modern Skyler, or the popular Headliner. Starting today, you‚Äôll be able to get Ray-Ban Meta (Gen 2) in some of the most popular style-and-color combinations, plus three new limited-time-only seasonal colors, available now:\n- Ray-Ban Meta Wayfarer (Gen 2) Shiny Cosmic Blue Transitions¬Æ Sapphire Lenses (also available in Large)\n- Ray-Ban Meta Skyler (Gen 2) Shiny Mystic Violet Transitions¬Æ Amethyst Lenses\n- Ray-Ban Meta Headliner (Gen 2) Shiny Asteroid Grey Transitions¬Æ Emerald Lenses\nAnd because you loved it so much last year, we‚Äôre bringing back a clear-frame Wayfarer, this time with a Matte Transparent frame and two lens options to choose from: Brown Mirror Gold or Ruby. You‚Äôll only find the Brown Mirror Gold lenses in select retail locations in limited quantities ‚Äî starting today at Connect and in our Meta Lab Burlingame store, with more to come later this fall. The Ray-Ban Meta Wayfarer (Gen 2) Matte Transparent Transitions¬Æ Ruby Lenses are available now on Ray-Ban.com starting at $509 USD.\nWe‚Äôre also bringing Ray-Ban Meta to more countries than ever before. Starting today, you can now order Ray-Ban Meta (Gen 2) in Switzerland and the Netherlands, and we‚Äôll be bringing them to Brazil soon, so stay tuned.\nWhen you‚Äôre ready to get your hands on Ray-Ban Meta (Gen 2), head on over to meta.com and Ray-Ban.com to get yours today, starting at $379 USD ‚Äî or pick up a pair of Ray-Ban Meta glasses (Gen 1), starting at just $299 USD.\n** Ray-Ban Meta (Gen 2) is available at the time of announcement in most countries where Ray-Ban Meta glasses are sold, and will be available in Mexico and India later this fall."
    },
    {
        "Ticker": "META",
        "Company": "Meta Platforms",
        "Source_URL": "https://www.meta.com/blog/oakley-meta-vanguard-pre-order-performance-ai-glasses/",
        "Content_Type": "Website Content",
        "Raw_Text": "Oakley Meta Vanguard hits shelves today. Designed for high-intensity sports, these Performance AI glasses are the perfect companion, whether you‚Äôre training for a marathon, shredding on the slopes, or cycling at speed. Grab a pair now and enter the world of Athletic Intelligence.\nTogether with Oakley, we‚Äôre excited to announce Oakley Meta Vanguard and welcome the world to the era of Athletic Intelligence ‚Äî amplifying performance to help you push past limits in the moments that matter most.\nEarlier this year, Meta and Oakley introduced a new category of Performance AI glasses for athletes, combining bold aesthetics with cutting-edge tech ‚Äî and that was just the beginning. We‚Äôve been working together with Oakley for years to build an AI glasses product from the ground up ‚Äî one that‚Äôs built for high-intensity sports. And today at Connect, Mark Zuckerberg announced that we‚Äôre expanding the product category with Oakley Meta Vanguard, a brand-new line of Performance AI glasses designed to give athletes a competitive edge. Pre-order yours now online at Meta or Oakley for $499 USD ahead of the October 21 launch.\nWe‚Äôre proud to take our leadership together with EssilorLuxottica in wearable tech to the demanding world of sports, where your equipment has to keep up with extreme action as it amplifies your potential ‚Äî and look good doing it. From the trails to the slopes, Oakley Meta Vanguard is the next evolution of sports technology.\nDesigned for Performance\nWe designed Oakley Meta Vanguard for high-intensity sports with an action-ready camera, integrations with fitness apps, immersive audio to elevate your workouts, and more. They feature Oakley‚Äôs Three-Point Fit system and come with three replaceable nose pads including a low- and high-bridge fit for a secure and customized feel. And they‚Äôre optimized to wear with cycling helmets and hats. These glasses are also our most water-resistant frames with an IP67 dust and water resistance rating ‚Äî meaning there‚Äôs nothing stopping you from a great workout.\nOakley Meta Vanguard‚Äôs wraparound design features Oakley¬Æ PRIZM‚Ñ¢ Lens technology, designed to block out sun, wind, and dust.\nThe open-ear speakers are the most powerful speakers on our AI glasses yet ‚Äî six decibels louder than Oakley Meta HSTN ‚Äî so it's perfect for running down a noisy road or biking in 30mph wind. They also include a five-microphone array optimized to reduce wind noise while on calls, messaging, or using Meta AI with your voice.\nOakley Meta Vanguard has up to nine hours of battery life, or up to six hours of continuous music playback ‚Äî enough to last you through a marathon. The glasses come with a charging case that can provide an additional 36 hours of charge on the go, so you can complete your training with ease. If you‚Äôre in a hurry, you can quickly charge the glasses to 50% in 20 minutes in the charging case.\nWe designed everything about Oakley Meta Vanguard with input from world class athletes, rigorously testing each feature in real conditions to make sure they were up to the challenge.\nA New Way to Train\nWhat's really going to take your performance to the next level is that we're integrating Garmin and Strava, which athletes love to use for training, with the Meta AI app.\nIf you have a compatible Garmin device, you‚Äôll be able to ask Meta AI how you‚Äôre doing or get updates on specific stats in real time, all hands-free and screen-free so you can remain present while you train. Just ask, ‚ÄúHey Meta, what‚Äôs my heartrate?‚Äù or ‚ÄúHey Meta, how am I doing?‚Äù to get real-time insights. The glasses will also be able to light up the status LED in your peripheral vision so that you‚Äôll know at a glance if you‚Äôre on target for the metric you set like heart rate or pace without needing to look down or break your momentum.\nWe‚Äôve also built a new autocapture feature with Garmin so that you can capture videos without you ever breaking your concentration or taking out your phone. The glasses will automatically capture video clips when you hit key distance milestones or ramp up your heart rate, speed, or elevation, so you can stay immersed in your workout knowing you‚Äôll never miss the shot.\nFor those training with Strava, athletes can graphically overlay their performance metrics onto videos and photos captured with Oakley Meta Vanguard and share directly to their Strava community ‚Äî all within the Meta AI app. Footage with overlays can also be shared across other apps like Instagram, Facebook, WhatsApp, and more. The result is more than just a workout recap ‚Äî it‚Äôs a way for athletes to showcase their effort, celebrate milestones, and inspire their community. Plus, when you set up App Connections with Garmin Connect, Apple Health, or Health Connect by Android, you‚Äôll get activity summaries directly in the Meta AI app after each workout, so you can compare your progress over time.\nYour Personal Highlight Reel\nOakley Meta Vanguard lets you capture your marathon moments, record-setting rides, and once-in-a-lifetime ski runs from your own point of view, completely hands-free. It can capture video in up to 3K resolution and has a centered 12-megapixel camera with a 122-degree wide-angle lens, so you can capture your most meaningful and epic moments hands-free. We‚Äôve also added new capture modes like Slow Motion, Hyperlapse, and adjustable video stabilization, so you can document your adventures in new ways. Imagine going down the slopes, finally nailing that trick you‚Äôve been practicing, and capturing it all in slow motion.\nWhether you‚Äôre using that video to step-up your performance or share your wins with the world, Oakley Meta Vanguard ensures that you stay on top of your game.\nOakley Meta Vanguard Collection\nOf course, it‚Äôs good to have options. That‚Äôs why we‚Äôve put together a lineup of four frame and lens color combos so you can pick the perfect pair:\n- Oakley Meta Vanguard Black with PRIZMTM 24K\n- Oakley Meta Vanguard White with PRIZMTM Black\n- Oakley Meta Vanguard Black with PRIZMTM Road\n- Oakley Meta Vanguard White with PRIZMTM Sapphire\nIt‚Äôs easy to replace the lenses on Oakley Meta Vanguard. Whether you need to better match the lighting during your workout or just want to change out your existing lenses, it‚Äôs simple to swap them, anytime. Replacement lens options include PRIZMTM Road, PRIZMTM 24K, PRIZMTM Black, and PRIZMTM Sapphire. Additionally, a PRIZMTM Low Light replacement lens option will be coming soon. Replacement lenses are sold separately and available for $85 USD.\nOakley Meta Vanguard will be available in the US, Canada, UK, Ireland, France, Italy, Spain, Austria, Belgium, Australia, Germany, Sweden, Norway, Finland, Denmark, Switzerland, and the Netherlands. We‚Äôre also working to bring Oakley Meta Vanguard to Mexico, India, Brazil, and the United Arab Emirates later this year. Pre-order yours now online at Meta or Oakley.\nBreaking Personal Records with Oakley Meta Vanguard and Red Bull\nOur friends at Red Bull recently put Oakley Meta Vanguard to the test, with two athletes sporting the Performance AI glasses in their pursuit of a new personal best.\nRed Bull athlete Jamie Huser set out to ride the world‚Äôs longest rail on a wakeboard, and when the 21-year-old launched onto Lake Falerin in his native Switzerland, he did just that ‚Äî racing along a 140-meter-long rail to set a new world record.\nMeanwhile Spain‚Äôs Diego Poncelet is looking forward to his next goal: setting the world‚Äôs fastest downhill skateboard speed. In 2026, the young Spaniard will aim to travel in excess of 150 km/h to achieve his goal. Until then, he‚Äôs seeking out the fastest roads to prepare.\nBoth Huser and Poncelet captured some epic footage of their boundary-pushing exploits, thanks to Oakley Meta Vanguard. And we‚Äôre honored to share the results with you."
    },
    {
        "Ticker": "META",
        "Company": "Meta Platforms",
        "Source_URL": "https://www.meta.com/blog/oakley-meta-ai-glasses-sports-wearables-hstn/",
        "Content_Type": "Website Content",
        "Raw_Text": "- Oakley Meta HSTN Warm Grey with PRIZM‚Ñ¢ Ruby Lenses\n- Oakley Meta HSTN Black with PRIZM‚Ñ¢ Polar Black Lenses\n- Oakley Meta HSTN Brown Smoke with PRIZM‚Ñ¢ Polar Deep Water Lenses\n- Oakley Meta HSTN Black with Transitions¬Æ Amethyst Lenses\n- Oakley Meta HSTN Clear with Transitions¬Æ Grey Lenses\n- Oakley Meta HSTN Black with Clear Lenses\nSince 1975, Oakley has pioneered sport and culture through breakthrough technology and future-forward design. And now, Meta and Oakley are teaming up to deliver a brand-new category of Performance AI glasses.\nOakley Meta glasses are a new product line that will combine Oakley‚Äôs signature design DNA with Meta‚Äôs industry-leading technology to give you deeper insights into your physical capabilities and help you share your biggest wins‚Äîon and off the field. The line will launch in a new global campaign starring Team Oakley athletes: World Cup winner Kylian Mbapp√© and three-time Super Bowl MVP Patrick Mahomes.\n‚ÄúFor 50 years, we‚Äôve been pushing the boundaries of what is possible, obsessed with solving unsolved problems,‚Äù says Caio Amato, Global President of Oakley. ‚ÄúTogether with Meta we are setting new bounds of reality‚ÄîIt is not only about pushing forward performance, but also about amplifying human potential as never before. And this is just the first chapter of a new era for sports.‚Äù\nGlasses have emerged as the most exciting new hardware category of the AI era, hands-down (and hands-free). We‚Äôre proud to lead the market with Ray-Ban Meta glasses, which have sold millions of units since launch. And now, we‚Äôre expanding our partnership with EssilorLuxottica to build upon another iconic, global brand.\nLet‚Äôs dig into what this super team has been cooking up.\nIntroducing Oakley Meta HSTN\nOur first product for athletes and fans alike, Oakley Meta HSTN (pronounced HOW-stuhn) combines bold aesthetics with cutting-edge tech. Capture the action completely hands-free with the built-in camera and share your unique POV. Get pumped up with your favorite playlist, listen to podcasts, and more, thanks to powerful open-ear speakers seamlessly integrated into the frames. And with an IPX4 water resistance rating, you can push yourself to the outer limits of your potential.\n‚ÄúThis marks a bold new chapter in our wearables journey. With Oakley‚Äôs iconic design legacy and Meta‚Äôs breakthroughs in AI and spatial computing, we‚Äôre setting a new standard for the industry,‚Äù notes Rocco Basilico, Chief Wearables Officer at EssilorLuxottica. ‚ÄúIt‚Äôs part of a broader multi-brand, multi-technology strategy that reflects the scale of our ambition: to build a connected eyewear category that spans lifestyles, communities, and use cases. We‚Äôre combining design, utility, and emotion to deepen human connection and unlock new potential through the lens of every brand we touch. And there is far more to come.‚Äù\nOakley Meta HSTN is taking the game to the next level with features that represent the evolution of AI glasses:\n- More Battery Stamina: A fully charged pair of Oakley Meta HSTN glasses can last up to eight hours of typical use and up to 19 hours on standby. You can charge them up to 50% in just 20 minutes. The glasses also come with a charging case that can deliver up to 48 hours of charging on the go.\n- Higher Resolution Camera: Capture your activity and share your achievements in Ultra HD (3K) video.\n- Meta AI Meets Performance AI Glasses: With Meta AI, your personal AI assistant, built in, athletes can get more out of their Oakley Meta HSTNs right out of the box. For example, you can level up the competition in a whole new way when playing a round of golf. Need to know how the wind is going to affect your drive? Ask, ‚ÄúHey Meta, how strong is the wind today?‚Äù and channel your inner Team Oakley Athlete J.R. Smith. Or, like Boo Johnson, record epic moments and post to Stories, hands-free, by saying, ‚ÄúHey Meta, take a video.‚Äù Get answers to a range of questions, whether you‚Äôre improving your game or checking the surf conditions.\nDesign Details\nSelect pairs of Oakley Meta HSTN glasses come with Oakley¬Æ PRIZM‚Ñ¢ Lens technology, one of the most advanced innovations in lens design. By decoding how the brain and eye process light, Oakley engineered a way to enhance vision and amplify contrast across changing light and weather conditions. This revolutionary technology fine-tunes the light spectrum, amplifying color while filtering out visual noise. Proprietary PRIZM‚Ñ¢ dyes manipulate light at a molecular level, enhancing detail and transforming your surroundings into vivid, high-definition clarity. This allows subtle visual cues to come sharply into view, helping wearers see more, react quicker, and perform at their peak.\nOf course, it‚Äôs good to have options. That‚Äôs why we‚Äôve put together a lineup of six frame and lens color combos, all Rx-ready so you can pick the perfect pair:\n- Oakley Meta HSTN Warm Grey with PRIZM‚Ñ¢ Ruby Lenses\n- Oakley Meta HSTN Black with PRIZM‚Ñ¢ Polar Black Lenses\n- Oakley Meta HSTN Brown Smoke with PRIZM‚Ñ¢ Polar Deep Water Lenses\n- Oakley Meta HSTN Black with Transitions¬Æ Amethyst Lenses\n- Oakley Meta HSTN Clear with Transitions¬Æ Grey Lenses\n- Oakley Meta HSTN Black with Clear Lenses\nWe‚Äôre kicking off with a Limited Edition Oakley Meta HSTN that honors 50 years of Oakley design, while charging into a new era of innovation, featuring gold accents and gold 24K PRIZM‚Ñ¢ Polar lenses.\nProduct Availability\nThe Limited-Edition Oakley Meta HSTN will be available for preorder starting July 11 for $499 USD, with the rest of the collection starting at $399 USD dropping later this summer. Oakley Meta HSTN will be available in the US, Canada, UK, Ireland, France, Italy, Spain, Austria, Belgium, Australia, Germany, Sweden, Norway, Finland, and Denmark. We‚Äôre also working to bring Oakley Meta HSTN to Mexico, India, and the United Arab Emirates later this year.\nGet a first glimpse of Oakley Meta HSTN in a global campaign featuring elite athletes from Team Oakley and beyond‚Äîfrom Gabriel Medina checking out the surf before carving lines to athlete J.R. Smith judging the wind on the golf course, Ishod Wair and Boo Johnson skating different spots and capturing it all, and Bicho Carrera operating on another altitude. The next evolution of AI glasses will also debut at marquee sporting events, starting with Fanatics Fest June 20 ‚Äì 22, followed by UFC International Fight Week, June 25 ‚Äì 27, with more to come this year.\nLearn more about Oakley Meta HSTN and sign up to be among the first to know about availability here and at Oakley.com.\nOakley Meta glasses are the latest chapter of a multi-year partnership between Meta and EssilorLuxottica. Meta and Oakley‚Äôs work is rooted in years of joint development, ground-up engineering, and an understanding of what drives athletes. We can‚Äôt wait to see how they change the game."
    },
    {
        "Ticker": "META",
        "Company": "Meta Platforms",
        "Source_URL": "https://www.meta.com/blog/orion-ar-glasses-augmented-reality/",
        "Content_Type": "Website Content",
        "Raw_Text": "Today at Connect, Mark Zuckerberg unveiled Orion‚Äîour first pair of true AR glasses, previously codenamed Project Nazare.\nWith an industry-leading field of view, silicon carbide lenses, intricate waveguides, uLED projectors, and more, Orion is our most advanced and most polished product prototype to date.\nNow, we‚Äôre continuing product optimizations and lowering costs as we drive toward a scalable consumer device that will revolutionize the way people interact with the world.\nFive simple words, spoken five years ago. With them, we planted a flag on our vision of a future where we no longer have to make the false choice between a world of information at our fingertips and the physical world all around us.\nAnd now, five years later, another five words that stand to once again change the game:\nAt Meta, our mission is simple: give people the power to build community and bring the world closer together. And at Reality Labs, we build tools that help people feel connected anytime, anywhere. That‚Äôs why we‚Äôre working to build the next computing platform that puts people at the center so they can be more present, connected, and empowered in the world.\nRay-Ban Meta glasses have demonstrated the power of giving people hands-free access to key parts of their digital lives from their physical ones. We can talk to a smart AI assistant, connect with friends, and capture the moments that matter‚Äîall without ever having to pull out a phone. These stylish glasses fit seamlessly into our everyday lives, and people absolutely love them.\nYet while Ray-Ban Meta opened up an entirely new category of display-less glasses super-charged by AI, the XR industry has long dreamt of true AR glasses‚Äîa product that combines the benefits of a large holographic display and personalized AI assistance in a comfortable, stylish, all-day wearable form factor.\nAnd today, we‚Äôve pushed that dream closer to reality with the unveiling of Orion, which we believe is the most advanced pair of AR glasses ever made. In fact, it might be the most challenging consumer electronics device produced since the smartphone. Orion is the result of breakthrough inventions in virtually every field of modern computing‚Äîbuilding on the work we‚Äôve been doing at Reality Labs for the last decade. It‚Äôs packed with entirely new technologies, including the most advanced AR display ever assembled and custom silicon that enables powerful AR experiences to run on a pair of glasses using a fraction of the power and weight of an MR headset.\nOrion‚Äôs input system seamlessly combines voice, eye gaze, and hand tracking with an EMG wristband that lets you swipe, click, and scroll while keeping your arm resting comfortably by your side, letting you stay present in the world and with the people around you as you interact with rich digital content.\nBeginning today at Connect and continuing throughout the year, we‚Äôre opening up access to our Orion product prototype for Meta employees and select, external audiences so our development team can learn, iterate, and build towards our consumer AR glasses product line, which we plan to begin shipping in the near future.\nWhy AR Glasses?\nThere are three primary reasons why AR glasses are key to unlocking the next great leap in human-oriented computing.\n- They enable digital experiences that are unconstrained by the limits of a smartphone screen. With large holographic displays, you can use the physical world as your canvas, placing 2D and 3D content and experiences anywhere you want.\n- They seamlessly integrate contextual AI that can sense and understand the world around you in order to anticipate and proactively address your needs.\n- They‚Äôre lightweight and great for both indoor and outdoor use‚Äîand they let people see each other‚Äôs real face, real eyes, and real expressions.\nThat‚Äôs the north star our industry has been building towards: a product combining the convenience and immediacy of wearables with a large display, high-bandwidth input, and contextualized AI in a form factor that people feel comfortable wearing in their daily lives.\nCompact Form Factor, Compounded Challenges\nFor years, we‚Äôve faced a false choice‚Äîeither virtual and mixed reality headsets that enable deep, immersive experiences in a bulky form factor or glasses that are ideal for all-day use but don‚Äôt offer rich visual apps and experiences given the lack of a large display and corresponding compute power.\nBut we want it all, without compromises. For years, we‚Äôve been hard at work to take the incredible spatial experiences afforded by VR and MR headsets and miniaturizing the technology necessary to deliver those experiences into a pair of lightweight, stylish glasses. Nailing the form factor, delivering holographic displays, developing compelling AR experiences, and creating new human-computer interaction (HCI) paradigms‚Äîand doing it all in one cohesive product‚Äîis one of the most difficult challenges our industry has ever faced. It was so challenging that we thought we had less than a 10 percent chance of pulling it off successfully.\nUntil now.\nA Groundbreaking Display in an Unparalleled Form Factor\nAt approximately 70 degrees, Orion has the largest field of view in the smallest AR glasses form factor to date. That field of view unlocks truly immersive use cases for Orion, from multitasking windows and big-screen entertainment to lifesize holograms of people‚Äîall digital content that can seamlessly blend with your view of the physical world.\nFor Orion, field of view was our holy grail. We were bumping up against the laws of physics and had to bend beams of light in ways they don‚Äôt naturally bend‚Äîand we had to do it in a power envelope measured in milliwatts.\nInstead of glass, we made the lenses from silicon carbide‚Äîa novel application for AR glasses. Silicon carbide is incredibly lightweight, it doesn‚Äôt result in optical artifacts or stray light, and it has a high refractive index‚Äîall optical properties that are key to a large field of view. The waveguides themselves have really intricate and complex nano-scale 3D structures to diffract or spread light in the ways necessary to achieve this field of view. And the projectors are uLEDs‚Äîa new type of display technology that‚Äôs super small and extremely power efficient.\nOrion is unmistakably a pair of glasses in both look and feel‚Äîcomplete with transparent lenses. Unlike MR headsets or other AR glasses today, you can still see each other‚Äôs real eyes and expressions, so you can be present and share the experience with the people around you. Dozens of innovations were required to get the industrial design down to a contemporary glasses form factor that you‚Äôd be comfortable wearing every day. Orion is a feat of miniaturization‚Äîthe components are packed down to a fraction of a millimeter. And we managed to embed seven tiny cameras and sensors in the frame rims.\nWe had to maintain optical precision at one-tenth the thickness of a strand of human hair. And the system can detect tiny amounts of movement‚Äîlike the frames expanding or contracting in various room temperatures‚Äîand then digitally correct for the necessary optical alignment, all within milliseconds. We made the frames out of magnesium‚Äîthe same material used in F1 race cars and spacecraft‚Äîbecause it‚Äôs lightweight yet rigid, so it keeps the optical elements in alignment and efficiently conducts away heat.\nHeating and Cooling\nOnce we cracked the display (no pun intended) and overcame the physics problems, we had to navigate the challenge of really powerful compute alongside really low power consumption and the need for heat dissipation. Unlike today‚Äôs MR headsets, you can‚Äôt stuff a fan into a pair of glasses. So we had to get creative. A lot of the materials used to cool Orion are similar to those used by NASA to cool satellites in outer space.\nWe built highly specialized custom silicon that‚Äôs extremely power efficient and optimized for our AI, machine perception, and graphics algorithms. We built multiple custom chips and dozens of highly custom silicon IP blocks inside those chips. That lets us take the algorithms necessary for hand and eye tracking as well as simultaneous localization and mapping (SLAM) technology that normally takes hundreds of milliwatts of power‚Äîand thus generates a corresponding amount of heat‚Äîand shrink it down to just a few dozen milliwatts.\nAnd yes, custom silicon continues to play a critical role in product development at Reality Labs, despite what you may have read elsewhere. üòé\nEffortless EMG\nEvery new computing platform brings with it a paradigm shift in the way we interact with our devices. The invention of the mouse helped pave the way for the graphical user interfaces (GUIs) that dominate our world today, and smartphones didn‚Äôt start to really gain traction until the advent of the touch screen. And the same rule holds true for wearables.\nWe‚Äôve spoken about our work with electromyography, or EMG, for years, driven by our belief that input for AR glasses needs to be fast, convenient, reliable, subtle, and socially acceptable. And now, that work is ready for prime time.\nOrion‚Äôs input and interaction system seamlessly combines voice, eye gaze, and hand tracking with an EMG wristband that lets you swipe, click, and scroll with ease.\nIt works‚Äîand feels‚Äîlike magic. Imagine taking a photo on your morning jog with a simple tap of your fingertips or navigating menus with barely perceptible movements of your hands. Our wristband combines a high-performance textile with embedded EMG sensors to detect the electrical signals generated by even the tiniest muscle movements. An on-device ML processor then interprets those EMG signals to produce input events that are transmitted wirelessly to the glasses. The system adapts to you, so it gets better and more capable at recognizing the most subtle gestures over time. And today, we‚Äôre sharing more about our support for external research focused on expanding the equity and accessibility potential of EMG wristbands.\nMeet the Wireless Compute Puck\nTrue AR glasses must be wireless, and they must also be small.wireless compute puck for Orion. It takes some of the load off of the glasses, enabling longer battery life and a better form factor complete with low latency.\nThe glasses run all of the hand tracking, eye tracking, SLAM, and specialized AR world locking graphics algorithms while the app logic runs on the puck to keep the glasses as lightweight and compact as possible.\nThe puck has dual processors, including one custom designed right here at Meta, and it provides the compute power necessary for low-latency graphics rendering, AI, and some additional machine perception.\nAnd since it‚Äôs small and sleek, you can comfortably toss the puck in a bag or your pocket and go about your business‚Äîno strings attached.\nAR Experiences\nOf course, as with any piece of hardware, it‚Äôs only as good as the things you can do with it. And while it‚Äôs still early days, the experiences afforded by Orion are an exciting glimpse of what‚Äôs to come.\nWe‚Äôve got our smart assistant, Meta AI, running on Orion. It understands what you‚Äôre looking at in the physical world and can help you with useful visualizations. Orion uses the same Llama model that powers AI experiences running on Ray-Ban Meta smart glasses today, plus custom research models to demonstrate potential use cases for future wearables development.\nYou can take a hands-free video call on the go to catch up with friends and family in real time, and you can stay connected on WhatsApp and Messenger to view and send messages. No need to pull out your phone, unlock it, find the right app, and let your friend know you‚Äôre running late for dinner‚Äîyou can do it all through your glasses.\nYou can play shared AR games with family on the other side of the country or with your friend on the other side of the couch. And Orion‚Äôs large display lets you multitask with multiple windows to get stuff done without having to lug around your laptop.\nThe experiences available on Orion today will help chart the roadmap for our consumer AR glasses line in the future. Our teams will continue to iterate and build new immersive social experiences, alongside our developer partners, and we can‚Äôt wait to share what comes next.\nA Purposeful Product Prototype\nWhile Orion won‚Äôt make its way into the hands of consumers, make no mistake: This is not a research prototype. It‚Äôs the most polished product prototype we‚Äôve ever developed‚Äîand it‚Äôs truly representative of something that could ship to consumers. Rather than rushing to put it on shelves, we decided to focus on internal development first, which means we can keep building quickly and continue to push the boundaries of the technology and experiences.\nAnd that means we‚Äôll arrive at an even better consumer product faster.\nWhat Comes Next\nTwo major obstacles have long stood in the way of mainstream consumer-grade AR glasses: the technological breakthroughs necessary to deliver a large display in a compact glasses form factor and the necessity of useful and compelling AR experiences that can run on those glasses. Orion is a huge milestone, delivering true AR experiences running on reasonably stylish hardware for the very first time.\nNow that we‚Äôve shared Orion with the world, we‚Äôre focused on a few things:\n- Tuning the AR display quality to make the visuals even sharper\n- Optimizing wherever we can to make the form factor even smaller\n- Building at scale to make them more affordable\nIn the next few years, you can expect to see new devices from us that build on our R&D efforts. And a number of Orion‚Äôs innovations have been extended to our current consumer products today as well as our future product roadmap. We‚Äôve optimized some of our spatial perception algorithms, which are running on both Meta Quest 3S and Orion. While the eye gaze and subtle gestural input system was originally designed for Orion, we plan to use it in future products. And we‚Äôre exploring the use of EMG wristbands across future consumer products as well.\nOrion isn‚Äôt just a window into the future‚Äîit‚Äôs a look at the very real possibilities within reach today. We built it in our pursuit of what we do best: helping people connect. From Ray-Ban Meta glasses to Orion, we‚Äôve seen the good that can come from letting people stay more present and empowered in the physical world, even while tapping into all the added richness the digital world has to offer.\nWe believe you shouldn‚Äôt have to choose between the two. And with the next computing platform, you won‚Äôt have to."
    },
    {
        "Ticker": "META",
        "Company": "Meta Platforms",
        "Source_URL": "https://www.meta.com/blog/reality-labs-surface-emg-research-nature-publication-ar-glasses-orion/",
        "Content_Type": "Website Content",
        "Raw_Text": "Punchcards. Keyboards. The mouse. Touchscreens. For multiple generations, we‚Äôve been adapting to new ways of interacting with computers in order to communicate, create, and get things done. But what if there were a way for our devices to adapt to us, driven by machine learning and AI, with a control scheme that was less robotic, more intuitive, and inherently more human?\nWe‚Äôve seen advancements in this area thanks to computer vision and natural language understanding, which lets us interact using our voice and allows computers to see the world like we do, but what if we could control our computers using the subtle movements of our hands? After all, using our hands is one of the first ways we go about interacting with the world around us. What if a new way to engage with machines were literally at our fingertips?\nIt‚Äôs a future we‚Äôve been exploring at Reality Labs for years. Based on our findings, we believe that surface electromyography (sEMG) at the wrist is the key to unlocking the next paradigm shift in human-computer interaction (HCI).\nAnd it‚Äôs an idea that‚Äôs catching on. Check out the latest issue of Nature, one of the world‚Äôs leading multidisciplinary science journals, for our latest peer-reviewed article that outlines our work in the field and validates the use of sEMG as an intuitive and seamless input that works across most people. First published in 1869, Nature has been home to the likes of Charles Darwin, Jennifer A. Doudna, Sir James Chadwick, Rosalind E. Franklin, and our own Yann LeCun.\nNot only does sEMG enable intuitive and seamless on-the-go interaction with our devices, we‚Äôve also supported the work of external research labs, which has shown that this technology is inherently inclusive because it works for people with diverse physical abilities and characteristics.\nWe successfully prototyped an sEMG wristband with Orion, our first pair of true augmented reality (AR) glasses, but that was just the beginning. Our teams have developed advanced machine learning models that are able to transform neural signals controlling muscles at the wrist into commands that drive people‚Äôs interactions with the glasses, eliminating the need for traditional‚Äîand more cumbersome‚Äîforms of input. You can type and send messages without a keyboard, navigate a menu without a mouse, and see the world around you as you engage with digital content without having to look down at your phone.\nsEMG recognizes your intent to perform a variety of gestures, like tapping, swiping, and pinching‚Äîall with your hand resting comfortably at your side. And thanks to our handwriting recognition technology, you can quickly jot down messages on a hard surface like a desk, table, or even your leg, which opens up new possibilities for discreet communication on the go.\nOur neural networks are trained on data from thousands of consenting research participants, which makes them highly accurate at decoding subtle gestures across a wide range of people without the need for individual calibration. And although our generalized models work well out of the box, even a small amount of personalization based on limited individual data can improve handwriting recognition accuracy by up to 16%‚Äîin other words, an sEMG wristband can adapt to you and deliver better performance over time.\nWe believe this technology is the best that‚Äôs been developed by anyone to let you control your devices in a seamless, intuitive, and adaptable way that can be used by most people.\n- It‚Äôs completely non-invasive, opening up new ways to use muscle signals to interact with computers while solving many of the problems facing other forms of HCI.\n- It‚Äôs convenient, simple, and natural to use‚Äîand it works in situations where alternatives like voice interactions may be impractical or undesirable, like sending a private message out in public.\n- It‚Äôs always available, and removes the need for bulky accessories that pull you out of the moment and distract you from the people and things that matter most.\nPerhaps most exciting, our paper in Nature gives the broader scientific community a blueprint to create neuromotor interfaces of their own. In addition to a set of important design rules and best practices across hardware, experimental design, data requirements, and modeling, we‚Äôre publicly releasing a dataset containing over 100 hours of sEMG recordings from over 300 research participants across three distinct tasks. Along with our previously open sourced sEMG datasets for pose estimation and surface typing, our hope is that today‚Äôs release will help accelerate future work by academics and researchers in the field.\nOver time, sEMG could revolutionize how we interact with our devices, help people with motor disabilities gain new levels of independence while improving their quality of life, and unlock new possibilities for HCI that we haven‚Äôt even dreamt of yet.\nIt may just prove to be the perfect input for virtually any device."
    },
    {
        "Ticker": "META",
        "Company": "Meta Platforms",
        "Source_URL": "https://www.meta.com/blog/james-cameron-lightstorm-vision-partnership/",
        "Content_Type": "Website Content",
        "Raw_Text": "Sci-fi adventures. Star-crossed romances. Deep-sea documentaries. Filmmaker and explorer James Cameron is synonymous with all these genres and more. A pioneer in the merging of cutting-edge technology and blockbuster storytelling, Cameron has been at the forefront of innovation within the film industry for 40 years. And today, we‚Äôre thrilled to announce a new partnership between his venture, Lightstorm Vision, and Meta, to scale the creation of world-class 3D entertainment experiences spanning live sports and concerts, feature films, and TV series featuring big-name IP on Meta Quest‚Äîwhich will be Lightstorm Vision‚Äôs exclusive MR hardware platform.\n‚ÄúFantastic news‚Ä¶ Announcing a comprehensive, multi-year partnership with Meta to revolutionize ALL visual media,‚Äù says Cameron. ‚ÄúRecently, Boz [Andrew Bosworth, Meta CTO and Head of Reality Labs] showed me some of Meta‚Äôs advanced tech. I was amazed by its transformational potential and power, and what it means for content creators globally. I‚Äôm convinced we‚Äôre at a true, historic inflection point. Navigating that future with Meta will ensure ALL of us have the tools to create, experience, and enjoy new and mind-blowing forms of media.‚Äù\nMore people are using virtual and mixed reality headsets than ever before, opening up exciting new possibilities for immersive storytelling. Through our collaboration with Lightstorm Vision, we‚Äôre committed to moving media forward and to improving content creators‚Äô ability to make high-quality stereoscopic content through the use of advanced tooling, including employing AI. And as a member of Stability AI‚Äôs Board of Directors, Cameron is uniquely positioned to contribute to the thoughtful and responsible use of AI in this space.\nOur goal is to make top-tier 3D content more accessible‚Äîboth for creators to produce and for people to enjoy at home and on the go. Together with Lightstorm Vision, we‚Äôll improve access to 3D production tools across the broader entertainment industry. In addition to co-producing original content, we‚Äôll leverage Lightstorm Vision‚Äôs cutting-edge technology and production techniques to revolutionize the creation of stereoscopic media.\nCameron and the team at Lightstorm Vision have been pushing 3D technology forward for more than three decades, earning recognition as the world‚Äôs leading experts in stereoscopy. We‚Äôre honored to be able to work alongside them to help lower the production costs associated with top-tier 3D content and deliver a steady stream of mainstream immersive shows.\nFor more than a century, filmmakers have pushed the boundaries of storytelling. MR headsets represent the next step in the evolution of media, enhancing the viewing experience for millions, and utilizing native 3D in the service of traditional and immersive 360-degree environments, along with interactive elements that put you at the center of the story. We‚Äôre in the midst of a creative revolution as studios and individual auteurs explore this new frontier. We can‚Äôt wait to map the future with Lightstorm Vision and enable the next generation of global 3D content creators."
    },
    {
        "Ticker": "META",
        "Company": "Meta Platforms",
        "Source_URL": "https://www.meta.com/de-de/blog/michael-abrash-richard-newcombe-contextual-ai-reality-labs-research-connect-2025/",
        "Content_Type": "Website Content",
        "Raw_Text": "As he took the stage to help close out the day 2 keynote, Reality Labs Chief Scientist Michael Abrash revisited some predictions he made at Connect in 2016, reflecting on the progress we‚Äôve made in virtual and augmented reality to date.\n‚ÄúThe nine years that have passed since then provide fresh confirmation of Hofstadter‚Äôs Law,‚Äù he said. ‚Äú‚ÄòIt always takes longer than you expect, even when you take into account Hofstadter‚Äôs Law.‚Äô‚Äù\nThough they‚Äôve yet to ship, we‚Äôve demonstrated key breakthroughs in haptics, photorealistic avatars, field of view, and more. And both Meta Quest and our Orion prototype are blending digital content with our view of the physical world. ‚ÄúThe future I laid out nine years ago is arriving,‚Äù said Abrash, ‚Äú just later than expected.‚Äù\nWhat wasn‚Äôt expected ‚Äî outside of the imaginations of a few visionaries ‚Äî is the emergence of contextual AI, which understands the physical world and our place within it.\nUnlike today‚Äôs large language models (LLMs) and generative AI, which learn about the world solely from what‚Äôs represented by the internet, contextual AI can see what we see, hear what we hear, and understand our context as it unfolds in real time.\n‚ÄúThis is truly something new under the sun,‚Äù Abrash explained, ‚Äúand it will forever alter the way we interact with the digital world, partnering us with computers and with each other to massively amplify our potential in a truly personalized, human-oriented way.‚Äù\nIn order to understand the Second Great Wave of hHuman-Oriented Computing, which Abrash first posited would take the form of AR and VR back in 2018, it‚Äôs helpful to understand the First Great Wave, which marked a paradigm shift in the way humans interact with the digital content and led us to today‚Äôs world of interconnected information devices.\nThe First Great Wave of Human-Oriented Computing: A Timeline\n- 1957 // J. C. R. ‚ÄúLick‚Äù Licklider encounters the experimental TX-2 at MIT Lincoln Laboratory\n‚ÄúThe hope is that, in not too many years, human brains and computing machines will be coupled together very tightly, and that the resulting partnership will think as no human brain has ever thought and process data in a way not approached by the information-handling machines we know today.‚Äù\n‚Äî J. C. R. Licklider\n- 1960s // Licklider funds a number of researchers at ARPA, including Douglas Engelbart\n- 1968 // Engelbart presents the Mother of All Demos, demonstrating the ancestor of today‚Äôs graphical user interface (GUI)\n‚ÄúIf in your office, you, as an intellectual worker, were supplied with a computer display backed up by a computer that was alive for you all day, and was instantly responsive to every action you have‚Äîhow much value could you derive from that?‚Äù\n‚Äî Douglas Engelbart\n- 1970s // Researchers at Xerox PARC develop the first true personal computer, the Alto\n- 1979 // Apple engineers visit Xerox PARC and demo the Alto, which inspires the Lisa and Macintosh systems\n‚ÄúAnd that led to the Mac and Windows and everything that followed,‚Äù noted Abrash. ‚ÄúToday we live in a world that Lick and Engelbart made, to the extent that every one of you has a direct descendant of Lick‚Äôs vision, running a direct descendant of Engelbart‚Äôs interface, in your pocket or bag ‚Äî or in your hand, if I‚Äôm not being interesting enough.‚Äù\nChanging Seas\n‚ÄúThe First Great Wave is the sea we swim in,‚Äù said Abrash. ‚ÄúAnd yet, as sweeping as that revolution has been, it‚Äôs unfinished.‚Äù\nAbrash was quick to note that our experience of the world is determined by the information that flows through our senses and the results we perceive as a result of our actions. And it doesn‚Äôt matter where that information comes from or where those actions take place: Digital or virtual information and actions are just as valid as their physical counterparts.\n‚ÄúThe revolution Lick set in motion has created a vast virtual world that we interact with constantly, but only in a very limited way, through apps running on 2D surfaces, directed by pointing, clicking, tapping, and typing,‚Äù Abrash explained. ‚ÄúOf course, that‚Äôs proven to be tremendously useful, but it engages only a tiny fraction of the available bandwidth of our hands and senses, so it can only deliver a small subset of what we‚Äôre capable of experiencing and doing.‚Äù\nAs a result, our computing devices demand constant attention, pulling us away from the moment and the people right in front of us.\nThe Second Great Wave of Human-Oriented Computing promises to achieve Licklider‚Äôs vision by ‚Äúdriving all our senses at maximum fidelity, while letting our hands move naturally in virtual spaces and actually feel the objects we‚Äôre interacting with,‚Äù said Abrash. ‚ÄúVirtual experiences that are completely indistinguishable from physical reality are a long way off, but we can already drive vision and audio in convincing ways. And as that technology evolves, it will increasingly enable us to work and play in far richer ways, while enabling truly personal human connection regardless of distance.‚Äù\nOf course, the human experience extends beyond our relationship to the outside world ‚Äî which, while important, isn‚Äôt what makes us us. Rather, it‚Äôs the internal process of forming models of the world that sets humans apart.\n‚ÄúTechnology has vastly expanded the range and power of our interactions with the external world,‚Äù noted Abrash. ‚ÄúBut it has barely touched our internal experience. Even though I spend every day with my phone, it has no understanding of my context, my goals, or my decision process, so it can‚Äôt help me with my internal experience. But if it could, it would revolutionize my life in many ways, ranging from reminders to hearing assistance to context-dependent message and call handling to vastly improved memory, and much, much more.‚Äú\nSo what might that look like in practice? Take conversation focus, which we announced yesterday. If you manually turn on conversation focus, it could help you to hear a conversation in a noisy environment. With contextual AI that knows your preferences, it could automatically turn on conversation focus when you find yourself trying to hold a conversation in a noisy environment. When another friend joined the conversation, the AI would know to amplify their voice as well ‚Äî regardless of where you were looking at any given point in the conversation. And all of this would happen without you being made consciously aware of it.\n‚ÄúThe first scenario is a truly valuable feature, and I can‚Äôt wait to have it,‚Äù acknowledged Abrash. ‚ÄúBut thanks to AI that understands my needs and goals, the second scenario is the full and proper augmentation of my audio perception.‚Äù\nNow imagine that your AI can understand the three-dimensional world around you. It can help you find your keys. It can help with calorie counting while on the go. It can proactively surface relevant notes during a meeting. You can recall experiences from your past with near-perfect clarity. And you‚Äôd never get pulled out of the moment to have to manually discern between a robocall and an important message from your mom again.\n‚ÄúThere are literally thousands of possibilities, some large, some small, but they all add up to helping you do what you want to do with your life in a far deeper way than anything that‚Äôs gone before,‚Äù said Abrash. ‚ÄúThis is AI that helps you focus when you need to focus, remember what you need to remember, connect more meaningfully with people, and enhance your experience in the world rather than taking you out of it.‚Äù\nThanks to the advent of large language models and wearable devices that can let an AI see what you see, we‚Äôve opened up the potential for technology that amplifies our intentions rather than fragmenting our attention. Think of contextual AI as your partner to help you take action and process information ‚Äî physical or digital.\n‚ÄúEleven years ago, I would have called all this pie-in-the-sky science fiction,‚Äù said Abrash. ‚ÄúBut the combination of Ray-Ban Metas, LLMs, and a decade of research has made contextual AI the obvious future of human-oriented computing ‚Äî the heart of the Second Great Wave.‚Äù\nAnd to tell us more, he brought out one of the minds who‚Äôve been thinking about this longer than anyone else: Research Science VP Richard Newcombe.\nThe Limits & Potential of Human Networks\nTo start, Newcombe also took us back to 1968 ‚Äî but not to the Mother of All Demos. Rather, he pointed to the Apollo 8 mission and astronaut William Anders, who captured the first full-color photograph of Earth ever taken by a human.\n‚ÄúThis perspective of our world was incredibly novel and powerful,‚Äù Newcombe explained. ‚ÄúIt was unlike anything people had seen before. A world most people knew to be fractured by national borders and political divisions suddenly appeared united and singular ‚Äî unique and precious amongst the vastness of space. ... It helped kickstart the modern environmental movement and quite literally changed the way people understood the world we all share.‚Äù\nThe rise of personal computing coincided with the dawn of a new era of shared global thinking. But, Newcombe noted, there are limits ‚Äî specifically Dunbar‚Äôs number, which posits that humans can only maintain 150 stable relationships.\nOn the other hand, if the theory of six degrees of separation holds true, any two people on Earth can be connected through just four intermediaries.\n‚ÄúThe math is staggering ‚Äî network reach grows exponentially,‚Äù Newcombe said. ‚ÄúJust 80 generations connects us to everyone who ever lived, every thought ever conceived. Our potential proximity to vast human knowledge and thinking power is astonishing ‚Äî Dunbar‚Äôs number and the six degrees of separation represent our limits and our potential.‚Äù\nThe First Great Wave of Human-Oriented Computing helped collapse distance, putting our friends, family, and colleagues just a tap or click away. And the internet and search gave us near-immediate access to a vast repository of human knowledge. Yet there‚Äôs still untapped potential.\n‚ÄúThink about it like this,‚Äù said Newcombe. ‚ÄúThe American mathematician Claude Shannon, famous for developing modern information theory in the 1950s, calculated there are roughly seven to 10 bits of information per English word. At normal speaking speed ‚Äî 150 words per minute ‚Äî that‚Äôs roughly 10 bits per second per person. Think about it: Every law, every letter, every breakthrough in human history ‚Äî indeed, everything I am saying to you right now ‚Äî you understand by way of a communication channel narrower than a 1990s dial-up modem.‚Äù\nBy contrast, Newcombe added that our conscious experience of reality, informed by our senses, operates at roughly 1 billion bits per second. To put that in perspective, he likened it to today‚Äôs WiFi receiving and processing 4K video streams. But when you try to share that experience with others, you‚Äôre back to dial-up speeds. Our networks, then, serve to aggregate human cognition.\n‚ÄúWe‚Äôve invented tools to extend our individual capabilities ‚Äî books for external memory, mathematics for structured reasoning, computers for information processing, and the internet to defy distance,‚Äù said Newcombe. ‚ÄúBut it‚Äôs only through our human networks that we truly transcend the bottleneck of what a single mind can read, hold, reason about, and articulate. These networks are how we cause action and change at scale.‚Äù\nThe Advent of Contextual AI\nAs we stand at the dawn of the AI era, we‚Äôre faced with a technology that can sift through, aggregate, and make predictions about vastly more information than a single mind could handle. Rather than focusing solely on AI‚Äôs ability to automate mundane and repetitive tasks, Newcombe argued that the real impact will be how the technology fundamentally changes the dynamic of human networks.\n‚ÄúImagine AI enabling a fluid network of humans connecting across all of humanity, as capable as any of our best organizations today, so that individuals gain access to the power of entire companies,‚Äù Newcombe said. ‚ÄúImagine such an organization becoming available to each of us ‚Äî unlocking our reach to billions of minds. We‚Äôre at the start of a leap that gives each individual mind access to a federated system of input, aggregation, processing, synthesis, and output ‚Äî something that, until now, only networks of humans could achieve through the filter of time.‚Äù\nHowever, today‚Äôs LLMs are limited. Despite being trained on internet-scale amounts of data, they lack context about and understanding of the physical world and our lived experience within it.\n‚ÄúContextual AI bridges the gap between our embodied reality and our symbolic cognitive world bound by communication and enables AI to understand the reality we are in, together,‚Äù noted Newcombe. ‚ÄúThis is made possible by AI glasses that see what you see and hear what you hear. These form a new generation of wearable computers that understand what you‚Äôre doing and how you work with others to get things done.‚Äù\nAnd because your AI glasses can see what you see and hear what you hear, they have the potential to create a personalized knowledge base, enabling your AI to tailor your interactions and access to what‚Äôs important. Over time, this could help us understand not just isolated events, but also causal patterns.\n‚ÄúThe potential here is for AI to understand how our connected lives weave together to form our shared reality,‚Äù said Newcombe. ‚ÄúOnce it can do that, AI can start to operate directly on our always-on context to understand, predict, and make opportunities for us to work together in the most effective forms. And this will enable AI to understand what is important to us and how we can best communicate ‚Äî with AI and each other.‚Äù\nA Truly Human Interface\nFor more than a decade, Reality Labs has been pushing forward the state of the art in virtual and augmented reality. Codec Avatars will let us defy distance, enabling social teleportation so we can feel truly present with anyone, anywhere, as easily as making a video call. We introduced the world to the potential of AI glasses with Ray-Ban Meta. And yesterday we announced Meta Ray-Ban Display, giving AI glasses a way to share visual information with the user as well as silent, ultra low-friction input with the new Meta Neural Band.\n‚ÄúProject Aria, first introduced in 2020 and now in its second generation, began our development of contextualized AI: research glasses combining sensing and mobile computation to furnish AI with much better context,‚Äù Newcombe explained. ‚ÄúOur future products will begin to unlock the value made possible by these signals including understanding what we‚Äôre looking at and what we‚Äôre trying to get done. These highly personalized context signals will enable highly personalized AI and all the features that could entail: superhuman memory and recall that helps us achieve goals, empathize, learn, and grow. Over time, as AI glasses begin to work with significant aspects of our context across their physical and digital lives, superintelligence will play a significant role in everything we do ‚Äî be it in our physical or digital realities, bringing an understanding of real-life context anywhere we are.‚Äù\nThe technologies we‚Äôre building could unlock the potential for everyone to work together to create the reality we want, grounded in shared understanding.\n‚ÄúAs these technologies converge with true AR ‚Äî immersive displays meet contextual AI ‚Äî we approach something profound: an interface that doesn‚Äôt separate us from reality but enhances our experience of it and our connection to each other,‚Äù said Newcombe. ‚ÄúThere couldn‚Äôt be anything more human than this.‚Äù"
    },
    {
        "Ticker": "META",
        "Company": "Meta Platforms",
        "Source_URL": "https://www.meta.com/de-de/blog/connect-2025-day-2-keynote-recap-vr-development-use-cases-wearable-device-access-toolkit/",
        "Content_Type": "Website Content",
        "Raw_Text": "Personal superintelligence is in sight, and it‚Äôs likely to help form the very fabric of computing from here on out. From today‚Äôs VR headsets to tomorrow‚Äôs AR glasses, AI is the unlock that will bring the next computing platform into much sharper focus.\nWe spent a lot of time yesterday talking about glasses as the first AI-native device to be adopted at scale, and today, we‚Äôre shifting gears to dive deeper on VR and the ways in which AI is changing the game.\nMeta Horizon Studio: Level Up with Generative AI Tools\nWe see a future where you can build entire virtual worlds without ever touching a line of code. That‚Äôs a huge opportunity for VR ‚Äî not just because it equates to more worlds for people to explore but because it opens up the door for anyone to be a creator.\nGenerative AI tools are lowering the barriers of entry for people to build compelling 3D, immersive content without sacrificing the ceiling of what‚Äôs possible. People with zero experience can make some incredible stuff. And seasoned professionals can make some much better stuff ‚Äî and faster.\nWe‚Äôve already got a pretty expansive set of generative AI tools for Horizon. You can generate 3D meshes, textures, skyboxes, ambient audio, sound effects, and TypeScript ‚Äî all with simple text prompts. You can even generate a fully rigged and animated 3D avatar. And our NPC generation system goes beyond that, coming up with a backstory, voice, conversation style, and dialogue based on just a few prompts.\nTake Citadel. It‚Äôs already one of the most popular worlds in Horizon, and when its scripted NPCs were replaced with AI generated characters, time spent increased by nearly 15%.\nWe‚Äôve seen Horizon creators embrace these new tools. Nearly 60% of eligible creators have used them in the last 30 days, and roughly one in three new worlds in the past month has AI-generated assets.\nWe‚Äôre also building an agentic creation system that orchestrates all these pieces behind the scenes, letting you create a fully immersive playable 3D world using nothing but natural language prompts. It won‚Äôt take months or even days. It‚Äôll take minutes.\nThese new tools will be available in the new and improved Horizon Studio, and if you want to be among the first to try it out, you can apply for the beta today.\nFor developers, increased velocity translates to faster iteration and more experimentation. Rather than grayboxing an early prototype, what if you could populate it with vibrant assets and sound? Thanks to generative AI tooling in Horizon Studio, you can.\nMeta Horizon Engine: Driving Improved Quality Across the Metaverse\nTo really raise the quality bar for Horizon, we completely rebuilt the core architecture. Our new standalone Meta Horizon Engine has been fully optimized to bring the metaverse to life. It‚Äôs based on a scriptable render pipeline that opens the door to new visual styles and effects. You get industry standards like physically based rendering materials and lightmaps. It unlocks more complex genres, like RPGs and sims. And there‚Äôs now support for fundamental development features like source control.\nThanks to Horizon Engine, we‚Äôve been able to rebuild your Immersive Home in VR. You can customize it with persistent app windows you can pin in place. You might anchor a web browser to your coffee table or put a giant Instagram portal on your wall to enjoy scrolling through 3D versions of the photos and videos in your feed.\nWhen you change home environments, you‚Äôll see a beautiful transition in lighting and colors. Rather than just swapping out the skybox, it‚Äôs a living, breathing environment all around you. And your home is designed to be social. Soon you‚Äôll be able to invite up to seven friends over to watch movies or play games together.\nLooking for a bigger crowd? Horizon Engine supports higher concurrency in worlds ‚Äî 100+ in a single space ‚Äî which opens the door for massively multiplayer experiences. Public spaces like the new Horizon Central will feel more vibrant and active, and thanks to improved load and travel times, you can quickly jump from there to the new Arena to catch a concert or live event.\nImproving Distribution & Discovery to Boost Dev Success\nWith Horizon Studio lowering the barrier to entry for creation and Horizon Engine raising the ceiling of what‚Äôs possible, there‚Äôs more opportunity for everyone ‚Äî including developers, who can now build games that look and play great across VR and mobile and reach a massive audience by using our social apps for distribution.\nAnd we‚Äôre making it easier for people to connect with great content, both in-headset and out. We‚Äôve redesigned the Horizon Feed in VR, so people now get personalized app recommendations and see more promos that help them discover new apps. We also redesigned our mobile feed with a simpler structure that encourages more discovery. There‚Äôs live video previews, trailers, and more.\nWe‚Äôve overhauled our search surfaces in the Horizon mobile app and in-headset to improve discovery. Before people enter a search term, they‚Äôll see a list of trending searches, categories, and shelves that are relevant to them. And in the Horizon Store, there‚Äôs more places where apps can be promoted.\nWe‚Äôre introducing a new Targeted Re-Engagement Tool that helps you reach specific user cohorts with direct messaging, discounts, or promo codes using in-app or in-store notifications, which significantly increase the efficiency and conversion rate of your offers. For example, you could give a 50% discount to top spenders who haven‚Äôt purchased an in-app item in the last 14 days. And we‚Äôve added new Organization Profiles, which are designed to help you connect with your community so more people discover your work.\nOur Meta Horizon+ VR subscription service is another great tool for developers to drive revenue. Over the past year, we‚Äôve had 67 participating apps that have cumulatively earned $20 million from the program. Synth Riders alone saw a 60% bump in revenue after being added to the rotating Meta Horizon+ games catalog a year ago.\nSocial multiplayer games like Gorilla Tag continue to outperform on the platform. Animal Company, a free-to-play game with over 1 million monthly active players, managed to increase their paying user base by 9x in six months and ranked No. 5 in gross first-year revenue as of this summer. And as of this May, Ghosts of Tabor, a premium paid multiplayer game that launched in Early Access in 2023 before graduating to a full release in 2024, has already made over $30 million in revenue with more than 1 million players across Quest and Steam.\nRegardless of the size of your studio, there‚Äôs money to be made. Over 300 apps on the Meta Horizon Store have now generated more than $1 million in revenue, with an even 10 apps generating over $50 million each.\nIf you‚Äôre already working with your own AI tooling, our new Horizon OS MCP Server can help accelerate your workflows. Connect it to your LLM and you‚Äôll have all the relevant context needed to build for Horizon OS. You can then ask your LLM about Horizon OS and get accurate responses from our knowledge base ‚Äî everything from understanding new platform capabilities to publishing guidance for your app. You can also generate quality code for Unity or the Meta Spatial SDK. That could mean boilerplate code to save time or a new code snippet in a language you‚Äôre less familiar with. Say you need help understanding how to build a VR native component ‚Äî your LLM can now connect to our knowledge base to guide you through all of it.\nIt can help you through some of the hardest problems in VR development, like performance optimization. You can take a trace and ask your LLM to analyze it for performance issues, make recommendations on fixes, and compare results after your changes are made to see the wins.\nAnd your LLMs and computer vision models can also integrate with our Passthrough Camera API thanks to our new Meta Building Blocks for Unity. This makes it possible to build experiences that capture and understand your players‚Äô physical surroundings in much deeper ways.\nGrowing the VR User Base Through Expanded Use Cases\nOf course, it‚Äôs not all fun and games. Just like smartphones, laptops, and desktop computers are expected to include web browsers, email clients, 2D entertainment, and more, VR needs to deliver the best of today‚Äôs computing ‚Äî and make it better.\nThat‚Äôs where 2D panel apps, or window apps, come into play.\nTime spent in this category is up more than 60% over the last year. Let that sink in. And in VR, it can be even better than a 1:1 translation of traditional computing. Take the old-school browser ‚Äî it‚Äôs better on a massive display that you can take with you anywhere. In fact, time spent using our browser over the past year is up 28%, making it one of the most popular apps on the platform.\nAnd Discord is getting in on the action with the upcoming launch of a new native window app on Quest in 2026. People in-headset will be able to stay tapped into that community during their VR exploits and learn about new games just by seeing what their friends are playing ‚Äî and maybe even challenge them to a slashing sesh in Beat Saber. This is a massive opportunity for VR developers thanks to Discord‚Äôs huge player base. Think about it: Discord is home to a highly engaged community of more than 200 million monthly active players who spend a combined 1.9 billion hours playing games each month across thousands of titles on PC alone. With the launch of the Discord app on Quest, VR devs will have an incredible discoverability engine at their disposal.\nPeople are using window apps in-headset in droves ‚Äî which creates a new opportunity for devs to build something special. It‚Äôs now much easier to port Android apps into VR with our new Meta Horizon Android Studio Plugin. There‚Äôs the Meta Spatial SDK for spatializing your app. And the Spatial Simulator is coming soon to help you test how your apps will work in VR.\nAnd just like James Cameron and Lightstorm Vision are committed to producing a new generation of cinematic content that‚Äôs native to VR, we see non-gaming entertainment as a huge opportunity that‚Äôll drive a new wave of adoption in the future.\nNative media apps can deliver content people love in a much more compelling format. For example, Blumhouse‚Äôs new app adds 3D special effects to enhance the immersive horror experience. Disney+ is also coming to Quest and bringing along content from Hulu and ESPN.\nOur Investment in Mobile + Continued Ecosystem Growth\nMaking windowed content available alongside immersive options helps us grow the ecosystem in-headset, and for that same reason, we‚Äôre just as invested in bringing high-quality Horizon content to people on mobile and the web. The more people who can come together in worlds, the better ‚Äî no matter what device they‚Äôre using.\nWe‚Äôve worked hard over the last year to make Meta Horizon on mobile a best-in-class experience, and we‚Äôve made it easier than ever for creators to promote their worlds on Facebook and Instagram. A world can appear in someone‚Äôs feed just like any other kind of post, and they can jump straight into it with a single tap.\nThanks to these changes, more people are using Horizon on mobile than ever before. Monthly active users in mobile worlds are up 4x over the last year, and creators have published more than 5,000 new mobile worlds in that time.\nAs usage grows across VR and mobile, there‚Äôs a big opportunity for Horizon creators to turn their worlds into real businesses. Our $50 million Creator Fund has paid out millions in prizes since it launched in February, and we‚Äôre announcing another $2.5 million contest today. In-world purchases are up 280% this year, and new AI tools make it even easier to create unique items to help you monetize.\nWhether you‚Äôre a seasoned dev, a creator getting started, or somewhere in between, we recommend following the advice that GOLF+ CEO Ryan Engle shared on stage during today‚Äôs keynote: Find your niche. Think long term. And know your audience.\nOne More Thing...\nWhile VR is and will continue to be home to some of the deepest and most magical experiences that AI can make possible, we also know that glasses will bring personal superintelligence into our lives in a radically different way. With today‚Äôs AI glasses, we‚Äôve already seen that your AI assistant levels up when it can see and hear the world from your point of view and start to understand your context. And with the upcoming launch of Meta Ray-Ban Display and the Meta Neural Band, things are about to get even more interesting.\nTo help the dev community make the most of this exciting new opportunity, we‚Äôre introducing our new Wearable Device Access Toolkit in developer preview. This lets your app access the vision and audio capabilities of our AI glasses ‚Äî which unlocks a level of contextual awareness and real-time information that simply isn‚Äôt possible otherwise.\nAnd early results are promising. Disney‚Äôs Imagineering R&D team is working on early prototypes to see how AI glasses could help give guests access to tips while in their parks. Major streaming services like Twitch will enable creators to livestream straight from their glasses ‚Äî creators could even reach multiple platforms simultaneously via streaming software partners like Logitech‚Äôs Streamlabs. And HumanWare is building an integration that gives blind and low-vision people live guidance as they navigate the world.\nIt‚Äôs incredible to live through a paradigm shift as monumental as what we‚Äôre seeing, both with today‚Äôs VR headsets and tomorrow‚Äôs AR glasses. And we‚Äôre not just living through it ‚Äî we‚Äôre building it together with you. We can‚Äôt wait to see what the next year holds."
    },
    {
        "Ticker": "META",
        "Company": "Meta Platforms",
        "Source_URL": "https://www.meta.com/de-de/blog/connect-2025-llamacon-save-the-date/",
        "Content_Type": "Website Content",
        "Raw_Text": "Last year was an eventful one with the introduction of Meta Horizon OS, the launch of Meta Quest 3S, and the expansion of Meta AI‚Äîand 2025 is shaping up to be another banger. Before the months start slipping by, we‚Äôre here with a couple save the dates to reserve some space on your calendars.\nIntroducing LlamaCon: April 29\nFollowing the unprecedented growth and momentum of our open source Llama collection of models and tools, we‚Äôre excited to introduce LlamaCon‚Äîa developer conference for 2025 that will take place April 29.\nAt LlamaCon, we‚Äôll share the latest on our open source AI developments to help developers do what they do best: build amazing apps and products, whether as a start-up or at scale.\nMark your calendars: We‚Äôll have more to share on LlamaCon in the coming weeks.\nMeta Connect Returns: September 17 ‚Äì 18\nVirtual and mixed reality developers, content creators, metaverse mavens, and AI glasses enthusiasts, rejoice! Connect is coming back with a vengeance September 17 ‚Äì 18.\nAs in years past, we‚Äôll share the latest and greatest in Meta Horizon updates, peel back the curtain on tomorrow‚Äôs tech, and give XR devs the tools they need to help build for the next computing platform today.\nThat‚Äôs all we‚Äôve got for now. We‚Äôll be back with updates in the months to come, so in the meantime: Save. Those. Dates."
    },
    {
        "Ticker": "META",
        "Company": "Meta Platforms",
        "Source_URL": "https://www.meta.com/blog/meta-lab-new-locations-wynn-las-vegas-west-hollywood-new-york-ai-glasses-demos/",
        "Content_Type": "Website Content",
        "Raw_Text": "Meta Lab makes it to the Big Apple! That‚Äôs right, our NYC pop-up location is now open at 697 5th Avenue. Swing by 10:00 am to 7:00 pm ET daily.\nExplore our AI glasses lineup, get hands-on with Meta Quest, and grab a little something at the coffee bar in the city that never sleeps.\nSome highlights:\n- Developed with Zoo York, our archival wall and timeline tells the story of NYC skate culture from the 1960s to today, through graffiti, rare film footage, artwork, original decks, and other visual artifacts.\n- Our coffee shop features beverages from Buddies Coffee Roasters, founded by former pro skater Taylor Nawrocki and Rachel Nieves. Their signature drink is a vegan Coquito latte inspired by Puerto Rican heritage.\n- You‚Äôll find a multi-dimensional mural and art from pro skater Zered Bassett‚Äôs ‚ÄúPaper Skaters‚Äù series. His contemporary artwork captures the motion, grit, and spirit of skate culture.\n- We‚Äôve got gear and ephemera from woman-founded Rookie Skateboards, which has been championing inclusivity in the skate community since 1996.\n- And Evan Mock, a multidisciplinary creative, brings his disruptor spirit to Meta Lab with an interactive gallery. It takes visitors on a journey through NYC skate culture as they unlock immersive stories and memories using our AI glasses.\nWith today‚Äôs opening, we‚Äôre now up to four premium retail locations where you can experience the hottest hardware ‚Äî just in time for some holiday shopping. üéÅ\nGreat news for residents of and vacationers to the City of Angles: Our flagship Meta Lab has returned to Los Angeles.\nWhether you‚Äôre looking for the iconic Ray-Ban Meta glasses, ready to change the game with Oakley Meta HSTN, want a new way to train with Oakley Meta Vanguard, or trying to see more without ever looking away thanks to Meta Ray-Ban Display, this brick-and-mortar retail experience is your one-stop shop for all things Reality Labs hardware, including Meta Quest 3 and 3S.\nStop by the shop Sunday through Saturday from 10:00 am to 7:00 pm PT.\nWhether you‚Äôre a Las Vegas local or planning a getaway to the famed Strip, we‚Äôve got good news: The latest Meta Lab location is now open at the Wynn.\nThis pop-up retail experience is your one-stop shop for the hottest AI glasses from Ray-Ban Meta to Oakley Meta HSTN and, coming soon, Oakley Meta Vanguard. And for those who live on the cutting edge, check out Meta Ray-Ban Display ‚Äî our first AI glasses with an in-lens display and wrist-worn control.\nSwing by any time Sunday through Thursday from 10:00 am to 9:00 pm PT or Fridays and Saturdays from 10:00 am to 10:00 pm PT. Schedule a demo today and get ready to wear the future.\nWhen we first opened the doors of Meta Lab in 2024, we were so appreciative of the reception. And today, we‚Äôre thrilled to announce that Meta Lab is returning to Los Angeles as a flagship retail location, expanding to new pop-up spaces in New York and Las Vegas, and still going strong in Burlingame.\nAn experiential retail space that puts people and community first, Meta Lab is a reinvention of the traditional shopping experience. It‚Äôs a lab in the truest sense where we re-think the Meta retail experience, test out new approaches, and learn what works best.\nWhile the original Meta Lab in LA was devoted to Ray-Bay Meta glasses, our new retail space will feature the full line-up of Reality Labs hardware, including AI glasses and virtual reality headsets:\n- Meta Ray-Ban Display & the Meta Neural Band\n- Ray-Ban Meta (Gen 2)\n- Oakley Meta HSTN\n- Oakley Meta Vanguard\n- Meta Quest 3\n- Meta Quest 3S\nDemand for in-person demos of Meta Ray-Ban Display and the Meta Neural Band is strong, with appointments in many major cities already booked out through mid-October. Luckily, our Meta Lab locations will soon offer a premium demo experience, so whether you live in the metro areas or need an excuse to book a vacation, you can soon check out the most advanced AI glasses we‚Äôve ever sold. Select Meta Lab locations are now booking demos of Meta Ray-Ban Display, starting in Los Angeles and Burlingame with availability in Las Vegas and NYC to follow.\nAs an added bonus, we‚Äôre bringing our limited-edition Ray-Ban Meta Wayfarer (Gen 2) Matte Transparent with Brown Mirror Gold lenses exclusively to our Meta Lab locations. Quantities are limited, so stop by on opening day to secure a pair before they‚Äôre gone.\nOur goal with Meta Lab is to set a new standard for conceptual retail as a celebration of community, culture, and creative self-expression (hello, customization!). These spaces anchor on hands-on product experiences while integrating the people who make these cities great. During the three months our original Meta Lab pop-up was open in Los Angeles, we hosted dozens of events and invited in members of the community, from stand-up comedy with Desi Banks and a live podcast with Brooke Averick and Connor Wood to a paint and sip night with Tinashe, a Ray-Ban Meta-focused workshop with Director Drex Lee, and a cooking class with Cassie Yeung.\nHere‚Äôs what to expect at each retail location this year:\nLas Vegas: Our first new retail location will open on October 16, as a pop-up at the Wynn Las Vegas. The 560-square-foot space will give locals and visitors alike the chance to experience and explore all that our hardware has to offer. Find the perfect pair of AI glasses for you, and see how they complement your overall fit thanks to our full-length mirrors.\nLos Angeles: Next up, Meta Lab‚Äôs Los Angeles location on Melrose Avenue will return this fall as our flagship retail store. It‚Äôs expanding to over 20,000 square feet, offering a unique, multi-level retail concept specifically designed to highlight the features and benefits of our hardware. Our opening theme will be ‚ÄúSkating in Southern California, From Dogtown to Present,‚Äù celebrating the Santa Monica skate scene and its evolution through the years. We partnered with legendary skate artist, photographer, and creative director Mark Oblow to bring that theme to life, and the results permeate the space throughout. Pro-Tip: Check out the vinyl listening room to see how analog compares to our glasses‚Äô open-ear speakers.\nNew York: For our New York pop-up, centrally located on 5th Avenue in Midtown Manhattan, we‚Äôve got a different take on our skate theme. The walls will be adorned with black-and-white photography shot by Bill Eppridge for LIFE magazine in the 1960s, capturing the vibrancy and novelty of a new counterculture as it emerged.\nBurlingame: No stress, Bay Area locals. Our location in Burlingame, California ‚Äî the original lab of Meta Lab ‚Äî will remain open and offer a place to demo and purchase all of our products.\nAnd just like we‚Äôre adding more styles and brands to meet the demand for our AI glasses, we‚Äôre also experimenting with new ways for people to buy them. At Connect this year, we debuted a reinvention of a traditional vending machine, which let people try on (and purchase) pairs of Ray-Ban Meta and Oakley Meta glasses. We plan to launch a series of micro-stores with similar concepts and learn from them as we continue to evolve our retail experiences.\nBook a demo at a Meta Lab location to see Meta Ray-Ban Display and the Meta Neural Band in action.\nMeta Lab Locations\nBurlingame, California\n322 Airport Boulevard // 1,550 square feet\nOpen now\nLas Vegas, Nevada\nWynn Plaza // 560 square feet\nOpen now\nLos Angeles, California\n8600 Melrose Avenue // 20,000+ square feet\nOpen now\nNew York, New York\n697 5th Avenue // 5,000 square feet\nOpening soon"
    },
    {
        "Ticker": "META",
        "Company": "Meta Platforms",
        "Source_URL": "https://www.meta.com/emerging-tech/emg-wearable-technology/",
        "Content_Type": "Website Content",
        "Raw_Text": "MOBILE M√ñGLICHKEITEN\nDie n√§chste Innovationswelle wird deine Welt bald ver√§ndern.\nAI Glasses-Steuerung\nKI lautlos aufrufen\nFreih√§ndige Interaktion\nDeine Welt steuern\nDas Meta Neural Band (ein EMG-Armband) ist ein zentraler Bestandteil der √ñkosysteme von Meta Ray-Ban Display und Orion. Bei der Kombination mit den Display-Brillen erm√∂glicht EMG eine subtile, direkte Steuerung deiner Display-Brillen oder Augmented-Reality-Brillen. Gemeinsam konzipiert: Die Brille in Kombination mit dem ganzt√§gigen Tragekomfort eines Meta Neural Bands l√§sst dich nahtlos mit deiner digitalen Welt interagieren ‚Äì egal, wohin dich der Tag f√ºhrt.\nMUSKELAKTIVIERTE TECHNOLOGIE\nElektromyografie-Technologie (EMG) ist eine wegweisende Innovation, die es erm√∂glichen k√∂nnte, Smart-Ger√§te den ganzen Tag √ºber mit einfachen Gesten wie Fingertippen, Daumenwischen oder Handgelenkrollen zu steuern ‚Äì ganz ohne Maus, Touchscreen oder Fernbedienung. √úber diese einfachen Gesten hinaus kann die Technologie auch komplexere Interaktionen erkennen, etwa Handschrift oder Tippen auf einer Oberfl√§che.\nEMG misst die Muskelaktivit√§t am Handgelenk bei Gesten mit dem Armband und erkennt Signale, die auf die Absichten der Nutzer*innen schlie√üen lassen, etwa Klicken/Ausw√§hlen oder Scrollen. Maschinelles Lernen wandelt diese Signale in digitale Befehle f√ºr deine Smart-Ger√§te um, w√§hrend haptisches Feedback best√§tigt, dass die Geste ausgef√ºhrt wurde.\nMit Reaktionszeiten im Millisekundenbereich liefert EMG schnelle Eingaben bei minimalem Aufwand und erkennt deine Muskelbewegungen zuverl√§ssig, selbst wenn deine H√§nde nicht im Sichtfeld der Brillenkamera sind.\nInklusion und Barrierefreiheit\nDas Meta Neural Band bietet sofort einsatzbereite Funktionen f√ºr eine breite Palette von Nutzer*innen. Die Modelle f√ºr maschinelles Lernen wurden mit Daten einer Vielzahl einwilligender Studienteilnehmender trainiert, um ein System f√ºr den t√§glichen Gebrauch zu entwickeln. Im Forschungsbereichevaluieren und entwickeln wir kontinuierlich das Potenzial von EMG weiter, um Menschen mit unterschiedlichen Graden an Handgeschicklichkeit und -funktion zu unterst√ºtzen.\nEin herausragendes Beispiel daf√ºr ist unsere Partnerschaft mit dem Labor von Dr. Mazzocca ist sicher, dass VR in der Orthop√§die einen gro√üen Einfluss auf die Ergebnisse f√ºr Patienten haben wird. Doug Weber an der Carnegie Mellon University, dessen Ziel es ist, allen Menschen ‚Äì auch Personen mit Handl√§hmungen ‚Äì die m√ºhelose Verbindung zu Freund*innen und Angeh√∂rigen zu erm√∂glichen.\nVerantwortungsvolle Innovationen\nBei Meta arbeiten wir aktiv mit Wissenschaftler*innen, politischen Entscheidungstr√§ger*innen und f√ºhrenden akademischen Expert*innen zusammen, um die verantwortungsvolle Entwicklung neuer Technologien sicherzustellen. Diese Kooperationen umfassen Round-Table-Diskussionen, Fachpanels sowie Beratung durch Ethikexpert*innen.\nZudem unterst√ºtzen wir weiterhin fortgeschrittene universit√§re Forschungsprogramme, die sich auf die EMG-Technologie und ihre Anwendungen konzentrieren. Die aus diesen Kooperationen gewonnenen Erkenntnisse haben unsere EMG-Implementierung ma√ügeblich gepr√§gt und ein solides Rahmenwerk geschaffen, das den Schutz der Privatsph√§re und die Sicherheit der Nutzer*innen in den Vordergrund stellt.\nEMG steht f√ºr Elektromyographie.\nEMG, genauer gesagt Oberfl√§chen-Elektromyographie, misst die Muskelaktivit√§t am Handgelenk anhand elektrischer Signale, die auf der beabsichtigten Bewegung des*der Nutzer*in basieren. Maschinelles Lernen √ºbersetzt diese Signale in digitale Befehle und macht EMG so zu einer schnellen und leistungsf√§higen Eingabemethode.\nDie EMG-Sensoren im Meta Neural Band erm√∂glichen eine einfache, unkomplizierte und nat√ºrliche Navigation und Auswahl sowie die Eingabe von Befehlen f√ºr die Meta Ray-Ban Display-Brille und die n√§chsten Generationen von Computing-Plattformen. Dabei bleibst du immer pr√§sent in der Welt um dich herum und in der Welt der Menschen, die dich umgeben.\nEMG-Sensoren erfassen die Muskelaktivit√§t am Handgelenk anhand elektrischer Signale, die auf beabsichtigten Bewegungen wie einer Handbewegung oder einer feinen Geste beruhen.\nJa. EMG am Handgelenk ist eine sichere, nicht-invasive Technologie ‚Äì sie erfordert keinerlei Implantate oder chirurgische Eingriffe, anders als bei manchen klinischen EMG-Verfahren. EMG-Sensoren erfassen ausschlie√ülich Ausgangssignale des*der Nutzer*in aus Muskelaktivierungen ‚Äì sie senden keine Informationen zur√ºck an den K√∂rper. Mehr dazu √ºber den Entwicklungsprozess von EMG-Wearable-Technologie.\nDas EMG-Armband verarbeitet elektrische Signale aus Muskelbewegungen sowie Beschleunigungs-, Rotations- und Orientierungsdaten von Handgelenk und Hand.\nEMG-Armb√§nder wurden mit Blick auf den Schutz der Privatsph√§re entwickelt. Das maschinelle Lernen, das EMG-Signale in digitale Befehle √ºbersetzt, erfolgt lokal auf dem Ger√§t.\nAnders als kamerabasiertes Hand-Tracking ben√∂tigt EMG keine H√§nde im Sichtfeld einer Kamera, bietet dadurch mehr Interaktionsfreiheit und funktioniert auch bei schlechten Lichtverh√§ltnissen zuverl√§ssig. Im Vergleich zu bewegungsbasierten Sensoren liefert EMG hohe Pr√§zision bei geringer Latenz und erkennt auch Gesten wie Tippen und Eingaben. Beschleunigungssensoren, die typischerweise f√ºr die Bewegungserfassung verwendet werden, sind nicht in der Lage, Gesten zum Tippen und Eingeben zu erkennen.\nWeitere Ressourcen\nTauche tiefer ein in Metas Innovationen und Durchbr√ºche auf dem Weg zu einer st√§rker vernetzten Welt ein\nWir verwenden Cookies und √§hnliche Technologien, um Inhalte in Meta-Produkten bereitzustellen und zu verbessern. Dar√ºber hinaus verwenden wir sie, um mithilfe der durch Cookies auf und au√üerhalb von Meta empfangenen Informationen die Sicherheit zu verbessern sowie um Meta-Produkte f√ºr Personen, die ein Konto haben, bereitzustellen und zu verbessern.\nDu bestimmst, welche optionalen Cookies wir verwenden d√ºrfen. In unserer Cookie-Richtlinie erf√§hrst du mehr √ºber Cookies und wie wir sie verwenden. Dort kannst du deine Auswahl au√üerdem jederzeit √ºberpr√ºfen oder √§ndern.\nWenn du ein Facebook-Konto hast, kannst du mithilfe dieser Tools festlegen, wie unterschiedliche Daten zur Personalisierung von Werbeanzeigen verwendet werden.\nUm dir bessere Werbung zu zeigen, verwenden wir Informationen, die Werbetreibende und andere Partner uns zu deinen Aktivit√§ten au√üerhalb von Produkten der Meta-Unternehmen, zum Beispiel auf deren Websites und Apps, bereitstellen. In deinen Einstellungen f√ºr Werbung kannst du festlegen, ob wir diese Informationen verwenden d√ºrfen, um dir Werbung zu zeigen.\n√úber das Meta Audience Network k√∂nnen Werbetreibende dir Werbeanzeigen in Apps und auf Websites au√üerhalb der Produkte der Meta-Unternehmen zeigen. Um relevante Werbeanzeigen auszuliefern, bestimmt das Audience Network zum Beispiel anhand deiner Werbepr√§ferenzen, an welchen Werbeanzeigen du interessiert sein k√∂nntest. Diese kannst du in deinen Einstellungen f√ºr Werbung verwalten.\nIn den Werbepr√§ferenzen kannst du festlegen, ob wir dir Werbung zeigen sollen, und ausw√§hlen, welche Informationen wir daf√ºr verwenden d√ºrfen.\nDu kannst dir deine Aktivit√§ten au√üerhalb von Facebook ansehen. Dies ist eine Zusammenfassung deiner Interaktionen mit Unternehmen und Organisationen, die diese mit uns teilen. Als Interaktion gilt zum Beispiel, wenn du eine App √∂ffnest oder eine Website besuchst. Unternehmen und Organisationen verwenden unsere Business-Tools wie Facebook Login oder das Meta-Pixel, um diese Informationen mit uns zu teilen. So k√∂nnen wir dir ein individuelleres Nutzungserlebnis auf Meta-Produkten bieten. Erfahre mehr √ºber Aktivit√§ten au√üerhalb von Facebook, wie wir sie verwenden und wie du sie verwalten kannst.\nCookies sind kleine Textdateien, die zum Speichern und Empfangen von Kennungen in einem Webbrowser verwendet werden. Wir verwenden Cookies und √§hnliche Technologien, um Meta-Produkte anzubieten und um Informationen, die wir √ºber Nutzer erhalten, etwa zu ihren Aktivit√§ten auf anderen Websites und in anderen Apps, nachvollziehen zu k√∂nnen.\nSolltest du kein Konto haben, verwenden wir keine Cookies, um Werbeanzeigen f√ºr dich zu personalisieren. Informationen zu deinen Aktivit√§ten, die wir erhalten, verwenden wir lediglich f√ºr die Sicherheit und Integrit√§t unserer Produkte.\nIn unserer Cookie-Richtlinie erf√§hrst du mehr √ºber Cookies und wie wir sie verwenden.\nMithilfe von Cookies k√∂nnen wir die Meta-Produkte anbieten, sch√ºtzen und optimieren, beispielsweise indem wir Inhalte personalisieren, Werbeanzeigen individuell zuschneiden und ihre Performance messen, sowie ein sichereres Nutzungserlebnis erm√∂glichen.\nWelche Cookies wir verwenden, kann sich aufgrund von Optimierungen und Aktualisierungen der Meta-Produkte von Zeit zu Zeit √§ndern. Unabh√§ngig davon verwenden wir Cookies zu folgenden Zwecken:\nIn unserer Cookie-Richtlinie erf√§hrst du mehr √ºber Cookies und wie wir sie verwenden.\nZu den Meta-Produkten z√§hlen die Facebook-, Instagram- und Messenger-App sowie weitere in unserer Datenschutzrichtlinie aufgef√ºhrten Funktionen, Apps, Technologien, Software oder Dienste, die Meta anbietet.\nWeitere Infos zu Meta-Produkten findest du in unserer Datenschutzrichtlinie.\nDu kannst bestimmen, inwiefern wir optionale Cookies verwenden d√ºrfen:\nDu kannst diese Auswahl jederzeit in deinen Cookie-Einstellungen einsehen oder √§ndern."
    },
    {
        "Ticker": "META",
        "Company": "Meta Platforms",
        "Source_URL": "https://www.meta.com/blog/surface-emg-wristband-electromyography-human-computer-interaction-hci/",
        "Content_Type": "Website Content",
        "Raw_Text": "- Wristband technology that uses muscle signals as a form of input can facilitate more inclusive human-computer interactions (HCI) for people with a wide range of neuromotor abilities.\n- We continue to invest in external research and partnerships that focus on prioritizing equity and accessibility when designing neuromotor systems at the wrist for better HCI.\n- By considering everyone, we can help build a truly inclusive new computing platform that puts people‚Äîand their diverse lived experiences‚Äîat the center.\nToday at Connect, we unveiled Orion, which we believe is the most advanced pair of AR glasses ever made. Orion‚Äôs input and interaction system builds on our research into surface electromyography (EMG) and includes a wristband that lets people swipe, click, and scroll through content on their glasses‚Äîall without the need for controllers.\nWe‚Äôve continued that research into surface EMG to facilitate inclusive human-computer interactions (HCI) for people with a wide range of neuromotor abilities. Surface EMG uses external sensors around the wrist to detect electrical muscle signals that control the wrist and hand‚Äîand that technology opens up an easy-to-use, convenient, and rich new form of HCI. The surface EMG wristband used in Orion is our latest iteration of this technology. Our goal is to eventually bring this technology to consumers in a wristband form factor for effortless HCI on the go.\nWe‚Äôre designing this technology with inclusivity in mind, developing wearable devices and input control algorithms that are performant across behavioral, physiological, and motor abilities. Regardless of the size and shape of your hand or how you move it, HCI with an EMG wristband should simply work‚Äîand it may even be able to adapt itself to your unique movements over time.\nSurface EMG input is inherently inclusive compared to traditional physical controllers, enabling functional control for people with atypical anatomy as well as those with limited range of motion. Unlike camera-based systems that detect physical hand movements from the hands and fingers‚Äîor handheld joystick-based controllers that need to be pressed and pushed‚Äîmuscle signals at the wrist can provide control signals even if you can‚Äôt produce large movements or if you have fewer than five fingers on your hand.\nWe‚Äôve partnered with leading external experts to accelerate research that explores how to design this technology so that it works for a diverse group of people.\nSupporting External Research Focused on Equity and Accessibility\nAs part of our work to develop EMG wristbands at scale for consumers, we support external research labs to expand the accessibility potential of neuromotor interfaces. Today, we‚Äôre sharing the latest updates from some of the teams we‚Äôve helped fund with academic gifts. These teams have all used their own in-house or commercial surface EMG electrodes and software to increase knowledge on how to design a more inclusive EMG system with funding provided by our 2021 Request for Proposals (RFP).\nAt the University of Utah, Dr. Jacob George‚Äôs team develops virtual interactions for everyone, even those with hand paralysis. Their research demonstrated how surface EMG signals at the wrist remain viable for control, even in cases where the signal-to-noise ratio is reduced, for example following a stroke. Notably, research participants who were unable to extend their physical fingers were able to move all of the fingers of a virtual hand avatar using EMG. This research shows the expressive potential of surface EMG in virtual environments beyond the physical capabilities of an individual.\nAt the University of Washington, Drs. Jennifer Mankoff and Momona Yamagami (now at Rice University) designed algorithms using surface EMG and movement sensors that allow people with different neuromotor capabilities to choose how they perform a gesture in order to interact with a computer. Research participants included people with limited motion due to muscular dystrophy, spinal cord injury, and other conditions. Through this project, the participants had full freedom to design their own individualized gestures for typical computer actions, such as ‚Äúpan,‚Äù ‚Äúrotate,‚Äù and ‚Äúzoom.‚Äù\nAs a 2023 follow-on to the RFP, we funded Drs. Lee Miller and Jonathon Schofield at the University of California at Davis, who are developing surface EMG algorithms for people with different ages and skin characteristics. Since surface EMG sensors are noninvasive and record muscle activity through the skin‚Äôs surface, it‚Äôs important to consider how characteristics like skin elasticity, hydration, body-mass index, and even the amount of hair on the arm impacts muscle signals. The team published initial approaches to develop algorithms that are robust across individuals with different wrist characteristics and continues to pursue this work.\nNew Accessibility Collaborations with Meta‚Äôs Research Prototypes\nIn 2023, we supported newly launched external projects using our own hardware prototypes and algorithms. Now that our surface EMG research has matured‚Äîeven informing product prototypes like Orion‚Äîwe can share our internally designed systems with select external partners who conduct projects under approvals from their local ethics committees to tackle equity- and accessibility-related use cases. This is an exciting evolution of the accessibility projects listed above.\nOur wrist-based surface EMG input hardware prototypes are sensitive enough to detect small neuromotor signals, called motor unit action potentials, that are often still present even after the most serious spinal cord injuries. These signals are too small to generate overt movements, but developing systems that use them for HCI can be truly game-changing for individuals with paralysis‚Äîimagine being able to control computers effectively with your muscle signals, even while not being able to physically move your fingers. As part of our efforts to evaluate this technology‚Äôs potential to serve a wide range of people, we‚Äôre collaborating with Dr. Doug Weber‚Äôs team at Carnegie Mellon University to demonstrate how these controls could work even for people with complete hand paralysis.\nOur university partners have shown that our wristbands, algorithms, and user experiences can guide people with spinal cord injury to produce small muscle signals and use them to control computer-based activities (including gaming and screen navigation)‚Äîeven on day one of training. These exciting outcomes show the equitable potential of the tech both for typical consumers and for those with paralysis who are currently limited in how they interact with technology and the world around them. The team continues to expand this research program by recruiting new participants and evaluating how control can improve with additional practice.\nTactile controllers‚Äîlike a keyboard or touchscreen‚Äîcan be difficult for people with tremor to control. EMG wristbands can overcome the challenges of holding a controller or maintaining consistent contact with a touchscreen by letting people use their hands in free space to swipe, tap, or pinch as best they can, with the system able to still identify their goal. To do this, our internal surface EMG input research focuses on using machine learning models to translate movements‚Äîswipes, taps, and even handwriting‚Äîinto computer interactions like navigation, selection, and text entry. The algorithms are based on a person‚Äôs neuromotor signals, rather than on their physical ability to move. This approach could unlock the potential to make computing more broadly accessible, including for those with hand tremors.\nIn collaboration with an external research organization, we‚Äôre demonstrating how our input control algorithms can work for those who struggle to use handheld devices due to tremor. Through this research program, we‚Äôre developing and validating algorithms that translate these gestures into computer commands even when the hand is shaking‚Äîand learning how to make the technology even more robust for different levels of tremor severity.\nAcross all of these studies, we‚Äôre incredibly excited to report promising early outcomes, and we look forward to learning more as we continue to focus on developing a new technology that considers everyone. Stay tuned for future updates as we continue this work."
    },
    {
        "Ticker": "META",
        "Company": "Meta Platforms",
        "Source_URL": "https://www.meta.com/blog/reality-labs-10-year-anniversary-next-computing-platform-vr-mr-ar-xr/",
        "Content_Type": "Website Content",
        "Raw_Text": "This year marks the 10-year anniversary of Reality Labs. We‚Äôre continuing the celebration with a new five-week series that shines a spotlight on the developers who make the magic possible.\n- Jesse Schell On the History of VR, the Rise of MR, & the Potential of Digital Worlds\n- Tam Armstrong On the Nuances of XR & the Pillars of Immersive Gaming\n- Maureen Fan On Medium-Agnostic IP, the Magic of XR Interaction, & Making You Matter\n- Tommy Palm On Mobile, MR, & the Magic of Multiplayer\n- Denny Unger On VR Mechanics, Multi-Modality, & the Inevitability of AR Glasses\nTL;DR: We‚Äôre standing on the edge of the next computing platform‚Äîone that finally focuses on people over products. We‚Äôve been building toward this moment for the past decade, investing in some serious R&D in both AI and the metaverse to help usher in the next great wave of human-oriented computing. And our products including Meta Quest 3 and Ray-Ban Meta smart glasses are already changing the way millions of people experience the world.\n- We brought your hands into VR‚Äîboth with controllers and without.\n- We invented inside-out tracking, so people could explore new worlds sans wires.\n- We gave you eye tracking and Natural Facial Expressions at a sub-$2,000 price point.\n- We let you seamlessly blend the virtual and physical worlds for about the price of a game console.\n- And we did it all with a community of developers, creators, and dreamers whose collective passion and perseverance put the reality in virtual, augmented, and mixed reality.\nToday, we‚Äôre celebrating the last 10 years of metaverse and AI work at Reality Labs‚Äîand looking ahead toward what the future holds.\nIt‚Äôs now been 10 years since Mark Zuckerberg announced the acquisition of Oculus and began pushing forward his vision of the metaverse in earnest. That‚Äôs a decade of investment in Reality Labs, including futuristic research, continuous product development, and a whole lotta prototypes.\nIf you‚Äôve been following Reality Labs for some time, odds are you‚Äôre familiar with Chief Scientist Michael Abrash. It‚Äôs always been clear to him that MR and AR would be the successor to the personal computer‚Äîa vision he shares with Zuckerberg.\n‚ÄúInstead of being separated by distance, people could be physically present with each other no matter where in the world they happened to be,‚Äù Abrash says. ‚ÄúInstead of me working on my one screen, I could have lots of screens that I could then share with you from any number of perspectives, I could switch workspaces with a click, and our avatars could collaborate as if we were in the same room, using tools like virtual whiteboards that are better than their physical-world equivalents. I joined Meta because it was clear that XR was going to change the world and that Mark Zuckerberg saw the same thing and had the commitment so the resources were going to be there‚Äîthat this was really going to happen.‚Äù\nToday, we‚Äôre taking a step back to celebrate a decade of innovation, the people who had faith in this space long before it became mainstream, and the community that‚Äôs made it all possible.\nStep Into Rift\nIt all started with a dev kit and a dream... and a healthy amount of duct tape. Over 9,000 enthusiasts pledged a combined $2.4 million to help make the dream a reality. From DK1 to DK2, the Oculus team transported people from their living rooms to stand on top of a skyscraper and look a Tyrannosaurus rex in the eye‚Äîand they were just getting started.\nRift kicked off the modern VR era in March 2016, followed by our first-generation Touch controllers in December of that year. Featuring an ergonomic design that encouraged natural hand poses and interactions, Touch opened up a new world of possibilities, like scaling a cliff face with your own hands in The Climb and gesturing to cast spells in The Mage‚Äôs Tale.\nThis was an era of dedicated VR rooms and often complex sensor setups‚Äîuntil Rift S shipped in May 2019, incorporating the inside-out tracking technology first introduced by Quest to eliminate the need for external sensors. We‚Äôre investing in PC VR now more than ever with Air Link, remote desktop, and more. But we saw the freedom that wireless, standalone headsets could offer, so we cut the cord and went all-in on all-in-one.\nVR on the Go\nDesigned to be the easiest way to jump into VR, Go was our first standalone VR product, shipping in 2018. Starting at $199 USD, it offered dramatically improved visual clarity while also reducing the screen door effect seen on Rift.\nAn early predecessor for the media and entertainment experiences people have come to know and love on Quest, Go gave people a front-row seat to concerts, movies, sporting events, and more while also letting them hang out with friends in ways that simply aren‚Äôt possible with today‚Äôs smartphones. And after we built on the 3DOF foundation of Go with Quest‚Äôs 6DOF tracking and improved optics and display, the experience has only gotten better.\nWe partnered with Xiaomi for the global launch of Go and on the Mi VR Standalone headset exclusively for the Chinese market. Go also marked the beginning of our years-long partnership with Qualcomm to meet the computing demands of the standalone VR‚Äîand ultimately MR‚Äîproduct category, as part of our ongoing strategy of working with others in the industry to bring the technology to everyone.\nEmbarking on an Epic Quest\nAlongside Rift S, we introduced Quest‚Äîthe world‚Äôs first 6DOF standalone VR headset, shipping in 2019. For the first time, people could freely explore virtual worlds, no wires required. And we let them explore and interact with the virtual world using their own hands, thanks to AI breakthroughs in hand tracking. In October 2020, we launched Quest 2, welcoming even more people into the fold. And 2021 brought with it a number of technological developments like the original Passthrough and Air Link, which lets people stream their PC VR games wirelessly to Quest over WiFi. We continue pushing out new software updates at a steady clip, increasing the value of people‚Äôs headsets over time.\nWith Quest Pro in October 2022, we introduced the world to compelling mixed reality, complete with eye tracking and Natural Facial Expressions. And just a year later, October 2023 ushered in Meta Quest 3 as the world‚Äôs first mass-market MR headset‚Äîletting people around the globe seamlessly blend the virtual and physical worlds.\nFrom multitasking your way through the mundane (YouTube on a massive screen while doing the dishes, anyone?) to courtside seats during the NBA season, from summer blockbuster concerts in Meta Horizon Worlds to Hollywood hits in your own personal theater, from fully immersive worlds to some of your favorite Xbox games, Quest 3 has established itself as the ultimate home entertainment system. A wide range of fitness and wellness apps makes Quest 3 a great way to get those gains. Super Rumble delivers battle royale action with friends in and out of headset. And with new partnerships, games, and experiences being announced at seemingly every turn, it‚Äôs only getting better from here.\nA Portal Into New Forms of Connection\nBuilding 8 made waves in 2017 with its research into brain-computer interfaces and new modes of hands-free communication, but it was a video calling device that established its legacy.\nMeta‚Äôs answer to the smart speaker, Portal served up ambient information, entertainment, and more with a screen and AI-powered Smart Camera to keep people in the frame during video calls. An award-winning playwright used Portal to collaborate with actors and a director from 3,000 miles away. Military families used Portal to stay connected and enjoy Story Time during deployment. A Bay Area couple got engaged over Portal, and a family used it to spend coast-to-coast quality time during the pandemic.\nOf course, the path to innovation isn‚Äôt always linear. While it eventually went the way of the buffalo, people loved Portal. It gave them new ways to connect across the miles, and it was just plain fun. From grandkids sharing their latest craft projects with far-flung family to never having to miss a birthday party (or even Sunday dinner), Portal helped people defy distance in an important way. A smart home device ahead of its time... and one of the first to integrate screens for video calling.\nSmart Phones Meet Smart Glasses\nImagine a world where you could tap into all the utility and benefits of a computer or smartphone in a more human-centric, simplified way. You‚Äôd be able to accomplish everything you set out to do and empowered like never before. Contextually-aware AI could help you explore and learn about the world around you, and you‚Äôd have access to rich 3D virtual information at a glance. You‚Äôd also be better able to connect with friends and family‚Äîno matter where in the world they happened to be.\nNow imagine if you could do it all with a pair of lightweight, stylish glasses‚Äîa first-of-its-kind, convenient, easy-to-use device that wouldn‚Äôt force you to choose between the physical world and the digital world, but would instead bring them together in a seamless, intuitive way. These glasses would let you look up and stay present in the world around you rather than pulling your attention away to the periphery in the palm of your hand.\nThat‚Äôs our long-term vision for AR glasses, and we‚Äôre building toward it now. Today‚Äôs smart glasses are an important stepping stone‚Äîtaking a stylish, all-day wearable form factor and loading it with smart tech, sensors, and more. That‚Äôs why we partnered with EssilorLuxottica and started with the iconic fashion people love before reimagining what it could be.\nIn September 2021, we launched Ray-Ban Stories, our first-generation smart glasses. With impressive open-air audio, they were ideal for listening to music, catching up on podcasts, and taking calls on the go. But more importantly, Ray-Ban Stories gave people a new way to capture the moment while staying fully present in it, thanks to hands-free capture of high-quality photos and videos.\nOctober 2023 saw the debut of the Ray-Ban Meta smart glasses collection, which doubled down and improved upon everything that made the first generation great. We integrated Meta AI on Ray-Ban Meta smart glasses (currently only in the US) and optimized it for a hands-free, on-the-go experience. By saying, ‚ÄúHey Meta,‚Äù people can spark creativity, get information, and control their glasses‚Äîjust by using their voice. And thanks to AI, the glasses will keep getting better and smarter over time. New AI experiences are coming soon including multimodal AI, so the glasses can understand what you‚Äôre seeing to provide helpful answers.\nLet‚Äôs say it‚Äôs dinnertime and you‚Äôre out of ideas. You‚Äôll be able to lay out some ingredients on the counter and ask Meta AI to give you recipe suggestions. Or say you‚Äôre traveling and come across a sign in German. You‚Äôll be able to look at the text and ask Meta AI to translate it for you on the fly.\nRelentless Research\nLast but not least, we turn the spotlight to Reality Labs Research (RL-R). Much like Meta‚Äôs Fundamental AI Research team (FAIR), RL-R was founded with an understanding of the importance of making big bets on extended time horizons. RL-R has a rich legacy of long-tail research dedicated to pushing the state of the art forward, from display systems to haptics and beyond. And we regularly publish our work, sharing it with the broader research community so they have an opportunity to extend it.\nBut it‚Äôs not all moonshots and research prototypes. RL-R is home to a number of successful tech transfers, including hand tracking on Quest and audio enhancements in our smart glasses products. And it was RL-R working in concert with our product team that made the pancake lenses and polarization optics that shipped in Quest Pro and later in Quest 3 a viable reality.\nFar from content to rest on their laurels, the teams within RL-R (and product teams throughout the company) continue to explore new lines of inquiry that could well become industry standard in the years to come, including EMG, hyper-realistic Codec Avatars, and contextualized AI‚Äîall of which are exciting new technologies that we believe will help people seamlessly interact with their devices and each other as the next great paradigm shift in computing takes hold.\n& Speaking of AI...\nFor those keeping score at home, AI has always played a key role in our long-term metaverse vision and the work we‚Äôre doing to get there‚Äîfrom computer vision to machine learning and beyond. And recent breakthroughs like those in the field of large language models (LLMs) are accelerating our progress.\nThis is where our two big bets as a company come together. While Quest 3 offers compelling MR experiences, from fully immersive to those that let you interact with the physical world and digital content in tandem, and Ray-Ban Meta smart glasses give you access to the utility and entertainment of Meta AI, AR glasses will deliver the best of both worlds as these two technological paths converge. Just like the smartphone didn‚Äôt eliminate the laptop or desktop computer, AR glasses won‚Äôt be the end of MR headsets. Rather, we see a constellation of devices forming the fabric of the future. And thanks to the rapid breakthroughs in AI models over the last 18 months, we now think that the smart glasses we have on the road to full AR glasses are going to have much broader appeal and be more useful than we previously thought‚Äîeven in the absence of displays.\nBuilding for the Future\nWe‚Äôre building a new kind of computing platform that will help you better connect with the people and things you care about in a way that‚Äôs much more natural and intuitive than what‚Äôs possible with today‚Äôs 2D screens. That means experiences that help you sync up with friends and family to work, learn, play, shop, and create‚Äîas well as completely new experiences that don‚Äôt easily fit into the mold of how we use computers and smartphones today.\nTechnology‚Äîespecially technology that connects people‚Äîshould be available to as many people as possible. That‚Äôs the foundation of what we do, and it‚Äôs core to our DNA. Innovation is all about delivering new breakthroughs‚Äîand to truly do that, you need to bring those breakthroughs to the masses.\n‚ÄúWhat we‚Äôre working on can literally only happen in a few places in the world,‚Äù Abrash notes. ‚ÄúMark has invested in building a remarkable critical mass of expertise, mission, and resources in Reality Labs, creating a once-in-50-years opportunity to change the way the world works, plays, and connects, in the same way that personal computing did‚Äîbut even more so. That next evolution in computing will happen. But how it happens really depends on who does it. And if you want to be part of that, then this is a good place to be.‚Äù\nIt‚Äôs been an honor to spend the last decade working alongside a community of builders to make the next computing platform a reality. We‚Äôre committed to continuing to build out in the open and with partners who help pave the way. And we can‚Äôt wait to see what the next 10 years have in store."
    },
    {
        "Ticker": "META",
        "Company": "Meta Platforms",
        "Source_URL": "https://www.meta.com/de-de/blog/meta-lab-new-locations-wynn-las-vegas-west-hollywood-new-york-ai-glasses-demos/",
        "Content_Type": "Website Content",
        "Raw_Text": "Meta Lab makes it to the Big Apple! That‚Äôs right, our NYC pop-up location is now open at 697 5th Avenue. Swing by 10:00 am to 7:00 pm ET daily.\nExplore our AI glasses lineup, get hands-on with Meta Quest, and grab a little something at the coffee bar in the city that never sleeps.\nSome highlights:\n- Developed with Zoo York, our archival wall and timeline tells the story of NYC skate culture from the 1960s to today, through graffiti, rare film footage, artwork, original decks, and other visual artifacts.\n- Our coffee shop features beverages from Buddies Coffee Roasters, founded by former pro skater Taylor Nawrocki and Rachel Nieves. Their signature drink is a vegan Coquito latte inspired by Puerto Rican heritage.\n- You‚Äôll find a multi-dimensional mural and art from pro skater Zered Bassett‚Äôs ‚ÄúPaper Skaters‚Äù series. His contemporary artwork captures the motion, grit, and spirit of skate culture.\n- We‚Äôve got gear and ephemera from woman-founded Rookie Skateboards, which has been championing inclusivity in the skate community since 1996.\n- And Evan Mock, a multidisciplinary creative, brings his disruptor spirit to Meta Lab with an interactive gallery. It takes visitors on a journey through NYC skate culture as they unlock immersive stories and memories using our AI glasses.\nWith today‚Äôs opening, we‚Äôre now up to four premium retail locations where you can experience the hottest hardware ‚Äî just in time for some holiday shopping. üéÅ\nGreat news for residents of and vacationers to the City of Angles: Our flagship Meta Lab has returned to Los Angeles.\nWhether you‚Äôre looking for the iconic Ray-Ban Meta glasses, ready to change the game with Oakley Meta HSTN, want a new way to train with Oakley Meta Vanguard, or trying to see more without ever looking away thanks to Meta Ray-Ban Display, this brick-and-mortar retail experience is your one-stop shop for all things Reality Labs hardware, including Meta Quest 3 and 3S.\nStop by the shop Sunday through Saturday from 10:00 am to 7:00 pm PT.\nWhether you‚Äôre a Las Vegas local or planning a getaway to the famed Strip, we‚Äôve got good news: The latest Meta Lab location is now open at the Wynn.\nThis pop-up retail experience is your one-stop shop for the hottest AI glasses from Ray-Ban Meta to Oakley Meta HSTN and, coming soon, Oakley Meta Vanguard. And for those who live on the cutting edge, check out Meta Ray-Ban Display ‚Äî our first AI glasses with an in-lens display and wrist-worn control.\nSwing by any time Sunday through Thursday from 10:00 am to 9:00 pm PT or Fridays and Saturdays from 10:00 am to 10:00 pm PT. Schedule a demo today and get ready to wear the future.\nWhen we first opened the doors of Meta Lab in 2024, we were so appreciative of the reception. And today, we‚Äôre thrilled to announce that Meta Lab is returning to Los Angeles as a flagship retail location, expanding to new pop-up spaces in New York and Las Vegas, and still going strong in Burlingame.\nAn experiential retail space that puts people and community first, Meta Lab is a reinvention of the traditional shopping experience. It‚Äôs a lab in the truest sense where we re-think the Meta retail experience, test out new approaches, and learn what works best.\nWhile the original Meta Lab in LA was devoted to Ray-Bay Meta glasses, our new retail space will feature the full line-up of Reality Labs hardware, including AI glasses and virtual reality headsets:\n- Meta Ray-Ban Display & the Meta Neural Band\n- Ray-Ban Meta (Gen 2)\n- Oakley Meta HSTN\n- Oakley Meta Vanguard\n- Meta Quest 3\n- Meta Quest 3S\nDemand for in-person demos of Meta Ray-Ban Display and the Meta Neural Band is strong, with appointments in many major cities already booked out through mid-October. Luckily, our Meta Lab locations will soon offer a premium demo experience, so whether you live in the metro areas or need an excuse to book a vacation, you can soon check out the most advanced AI glasses we‚Äôve ever sold. Select Meta Lab locations are now booking demos of Meta Ray-Ban Display, starting in Los Angeles and Burlingame with availability in Las Vegas and NYC to follow.\nAs an added bonus, we‚Äôre bringing our limited-edition Ray-Ban Meta Wayfarer (Gen 2) Matte Transparent with Brown Mirror Gold lenses exclusively to our Meta Lab locations. Quantities are limited, so stop by on opening day to secure a pair before they‚Äôre gone.\nOur goal with Meta Lab is to set a new standard for conceptual retail as a celebration of community, culture, and creative self-expression (hello, customization!). These spaces anchor on hands-on product experiences while integrating the people who make these cities great. During the three months our original Meta Lab pop-up was open in Los Angeles, we hosted dozens of events and invited in members of the community, from stand-up comedy with Desi Banks and a live podcast with Brooke Averick and Connor Wood to a paint and sip night with Tinashe, a Ray-Ban Meta-focused workshop with Director Drex Lee, and a cooking class with Cassie Yeung.\nHere‚Äôs what to expect at each retail location this year:\nLas Vegas: Our first new retail location will open on October 16, as a pop-up at the Wynn Las Vegas. The 560-square-foot space will give locals and visitors alike the chance to experience and explore all that our hardware has to offer. Find the perfect pair of AI glasses for you, and see how they complement your overall fit thanks to our full-length mirrors.\nLos Angeles: Next up, Meta Lab‚Äôs Los Angeles location on Melrose Avenue will return this fall as our flagship retail store. It‚Äôs expanding to over 20,000 square feet, offering a unique, multi-level retail concept specifically designed to highlight the features and benefits of our hardware. Our opening theme will be ‚ÄúSkating in Southern California, From Dogtown to Present,‚Äù celebrating the Santa Monica skate scene and its evolution through the years. We partnered with legendary skate artist, photographer, and creative director Mark Oblow to bring that theme to life, and the results permeate the space throughout. Pro-Tip: Check out the vinyl listening room to see how analog compares to our glasses‚Äô open-ear speakers.\nNew York: For our New York pop-up, centrally located on 5th Avenue in Midtown Manhattan, we‚Äôve got a different take on our skate theme. The walls will be adorned with black-and-white photography shot by Bill Eppridge for LIFE magazine in the 1960s, capturing the vibrancy and novelty of a new counterculture as it emerged.\nBurlingame: No stress, Bay Area locals. Our location in Burlingame, California ‚Äî the original lab of Meta Lab ‚Äî will remain open and offer a place to demo and purchase all of our products.\nAnd just like we‚Äôre adding more styles and brands to meet the demand for our AI glasses, we‚Äôre also experimenting with new ways for people to buy them. At Connect this year, we debuted a reinvention of a traditional vending machine, which let people try on (and purchase) pairs of Ray-Ban Meta and Oakley Meta glasses. We plan to launch a series of micro-stores with similar concepts and learn from them as we continue to evolve our retail experiences.\nBook a demo at a Meta Lab location to see Meta Ray-Ban Display and the Meta Neural Band in action.\nMeta Lab Locations\nBurlingame, California\n322 Airport Boulevard // 1,550 square feet\nOpen now\nLas Vegas, Nevada\nWynn Plaza // 560 square feet\nOpen now\nLos Angeles, California\n8600 Melrose Avenue // 20,000+ square feet\nOpen now\nNew York, New York\n697 5th Avenue // 5,000 square feet\nOpening soon"
    },
    {
        "Ticker": "META",
        "Company": "Meta Platforms",
        "Source_URL": "https://www.meta.com/de-de/blog/live-translation-ai-glasses-deep-dive/",
        "Content_Type": "Website Content",
        "Raw_Text": "From the babel fish of The Hitchhiker‚Äôs Guide to the Galaxy to Star Trek‚Äôs universal translator, the ability to break down language barriers is critical to many works of science fiction. And it‚Äôs reflective of a basic human need: to be understood by others.\nSo it was no surprise when the tech world was wow‚Äôd by Mark Zuckerberg‚Äôs successful live demo of live translation at Connect 2024. Since then, we‚Äôve expanded live translation availability across all our AI glasses, with support for English, French, German, Italian, Portuguese, and Spanish on Ray-Ban Meta, Oakley Meta Vanguard, and Oakley Meta HSTN, and support for English, French, Italian, and Spanish on Meta Ray-Ban Display. And today, we‚Äôre sharing the story behind this transformative technology ‚Äî and the people who made it possible.\nFrom Prototype to Product\nIn true Silicon Valley fashion, the path from prototype to product was a winding one. In fact, live translation was originally conceived as a demo feature for our then-unannounced Meta Ray-Ban Display glasses. But the teams working on the project quickly realized that Ray-Ban Meta, which was already in-market, was the perfect proverbial testbed.\n‚ÄúThanks to Ray-Ban Meta‚Äôs five-microphone array, beamforming could be used to distinguish between the person wearing the glasses and their conversation partner, which in turn helps ensure the accuracy of the translations,‚Äù explains Product Manager Nish Gupta. ‚ÄúAnd rather than relying on a display to show the translated text, we could leverage the glasses‚Äô speakers to play the translation back in near-real time.‚Äù\nOf course, that resulted in a fairly complicated process. Say you have two people, one Spanish speaker wearing Ray-Ban Meta glasses and one French speaker without glasses. When the French speaker speaks, that audio is first transcribed into text. The text is then translated from French to Spanish. Then a text-to-speech model converts the Spanish text into audio, which is played through the speakers of the Spanish speaker‚Äôs glasses. And all of this happens in near-real time ‚Äî and everything‚Äôs processed entirely on the glasses.\nHuman-Centric Design\n‚ÄúWe put people at the center of our design process,‚Äù explains Director of Product Management Ashish Garg. ‚ÄúThe team thought about a lot of edge-case scenarios, including travel. And when people travel, what do they lack? Good internet connectivity. So we thought, ‚ÄòWhat if they can download it ahead of time? Can they use it in airplane mode?‚Äô We really thought about the end-to-end user journey.‚Äù\n‚ÄúThis is a very complicated feature by design,‚Äù adds Product Manager Emerson Qin. ‚ÄúIt‚Äôs not processed on the server, so being able to fit a very powerful, useful model onto glasses that can run without internet access ‚Äî that itself is already a difficult exercise and it manifests into many other difficulties. Because everything happens on device, we don‚Äôt have as much information or logging that we would like in order to better the feature. So during the journey of development, that added a lot of challenges for us to know where we were and if the quality was really meeting the bar. And there‚Äôs no smarter way to address that except for testing it non-stop.‚Äù\nOvercoming Obstacles + Limiting Latency\nAs the project grew, so did the challenges. The team had to rethink everything ‚Äî from how the glasses would interact with users to how to make the experience seamless for both the wearer and their conversation partner. The models had to be optimized to fit within the glasses‚Äô memory and avoid overheating. And the team had to drive down latency from 5+ seconds to just 2.7 seconds ‚Äî a roughly 46% improvement ‚Äî to help conversations feel more fluid and natural.\n‚ÄúThis was only made feasible by the team constantly pushing the technical boundaries,‚Äù notes Qin of these latency improvements. ‚ÄúThe most notable innovation here is enabling the model to understand, translate, and generate speech audio, all in a streaming fashion ‚Äî all done within the interval of a few words without having to wait for a complete phrase or sentence.‚Äù\nAnd still, we‚Äôre just getting started.\n‚ÄúRemember that the feature is still in development,‚Äù notes Software Engineering Manager Fei Wang. ‚ÄúThere‚Äôs still noticeable latency, and the accuracy isn‚Äôt perfect. We launched now so we can improve the product over time. It‚Äôll get faster and more accurate, and we‚Äôll be able to add additional languages.‚Äù\nThat said, it‚Äôs important to keep in mind that each new language requires bespoke model training and evaluation ‚Äî and that‚Äôs specific to the device‚Äôs form factor. As Qin explains, ‚ÄúIn order for us to ship a new language, everything has to be redone, per device, so it‚Äôs difficult for us to scale. We still have a long way to go to add a lot more languages. Everything‚Äôs bespoke, so please bear with us.‚Äù\nEarly Impact\nEven during early testing and its initial rollout through our Early Access program, live translation has made a difference in people‚Äôs lives. People are using the feature to connect with family, navigate new places, and break down barriers at work and in their communities. And live translation is already seeing strong engagement on par with other popular AI use cases.\nVisiting an art museum in another country? Use live translation so you can better understand commentary from your local docent. Getting to know your future in-laws? Have deep conversations without having to rely on a third-party translator. Attending an international conference? Now you can meet and converse with new colleagues from across the globe.\nWhile it‚Äôs still early days, we‚Äôre thrilled by the response we‚Äôve seen thus far. Unlike with earbuds, which can make you feel a bit cut off from your environment (or even signal to other people that you‚Äôre not available for conversation), the open-ear speakers on our AI glasses feel more natural and ensure that you stay fully present in the moment and with the people around you. As an added bonus, our live translation feature includes a near-real time transcription of the conversation in both languages right in the Meta AI app, so you can show your phone to your conversation partner and help them follow along, too. Regardless of which form factor you choose, as more languages are added across devices, the tech industry is helping the world feel a little more connected ‚Äî one conversation at a time.\nA Team With a Shared Dream\nIt‚Äôs gratifying for the team to see that tangible impact. But perhaps the greatest accomplishment lies in the dynamics of the team itself.\n‚ÄúI‚Äôve been working at this company for over 10 years, and this is the best team I‚Äôve ever worked on,‚Äù says Research Scientist Baiyang Liu. ‚ÄúBecause it‚Äôs not just about the tech ‚Äî it‚Äôs about the people who believe in it and can make it happen. People worked day and night to get there. Dedicated people solved these problems together because they want to make it work.‚Äù\n‚ÄúThe way the team coalesced was miraculous,‚Äù agrees Product Designer Amy Pu. ‚ÄúEveryone truly believed in this feature and had the same vision. A lot of people on the team have first languages other than English, so we could immediately understand the benefit. Travel is ranked highly as a use case, but nobody‚Äôs traveling all the time, so I think multilingual families are where this technology really shines. Think of how meaningful it would be to look at your grandma‚Äôs face and understand what she‚Äôs saying ‚Äî even when you don‚Äôt speak the same language. Our goal is to create a world where people can understand any language anytime, anywhere. That would be truly empowering.‚Äù\nWhether you‚Äôre traveling abroad, collaborating with international colleagues, or seeking stronger multigenerational connections among your family, the benefits of near-instant translation are clear. An IRL universal translator would help us better navigate the world, communicate with our loved ones, and broaden our horizons. And live translation on AI glasses is an important step in that direction.\n‚ÄúWe‚Äôve loved seeing such strong reception and hearing how this technology is making a difference in people‚Äôs lives,‚Äù says Garg. ‚ÄúFrom nuanced social settings with friends and family to high-stakes meetings with colleagues, we‚Äôre seeing people use this feature in a wide variety of ways and places. And we‚Äôre still hard at work to expand the list of available languages as quickly as possible to better scale and meet the needs of more people across the globe.‚Äù"
    },
    {
        "Ticker": "META",
        "Company": "Meta Platforms",
        "Source_URL": "https://www.meta.com/blog/rss/",
        "Content_Type": "Website Content",
        "Raw_Text": "<title>What to Watch: November 2025 Highlights</title>\n<description><![CDATA[It‚Äôs November, and that means it‚Äôs time to sit back, get cozy, and relax. For whatever you want to watch, Meta Quest has you covered...]]></description>\n<link>https://www.meta.com/blog/what-to-watch-free-meta-quest-tv-vr-film-November-2025/</link>\n<pubDate>Fri, 14 Nov 2025 08:41:36 -0800</pubDate>"
    }
]