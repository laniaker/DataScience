[
    {
        "Ticker": "AVGO",
        "Company": "Broadcom",
        "Source_URL": "https://www.broadcom.com/",
        "Content_Type": "Website Content",
        "Raw_Text": "Access our library of semiconductor-related documentation and software downloads\nLatest Products\nBroadcom Inc. is a global technology leader that designs, develops and supplies a broad range of semiconductor, enterprise software and security solutions. Broadcom’s category-leading product portfolio serves critical markets including cloud, data center, networking, broadband, wireless, storage, industrial and enterprise software. Our solutions include service provider and enterprise networking and storage, mobile device and broadband connectivity, mainframe, cybersecurity and private and hybrid cloud infrastructure. Broadcom is a Delaware corporation headquartered in Palo Alto, CA."
    },
    {
        "Ticker": "AVGO",
        "Company": "Broadcom",
        "Source_URL": "https://www.broadcom.com/solutions/ai-solutions/ai-infrastructure",
        "Content_Type": "Website Content",
        "Raw_Text": "AI Infrastructure: Open, Scalable and Power-Efficient Technologies for AI Clusters\nDiscover Broadcom’s next-generation AI networking, connectivity solutions and custom accelerators (AI XPUs), revolutionizing the industry. Broadcom prioritizes delivering open, scalable and power efficient systems with exceptional performance across cloud environments, data centers, and networking systems. Broadcom understands the vital role of equipping its partners with leading network infrastructure to deliver transformative AI technology.\nAI Insights\nIntroducing Ethernet Scale Up Networking: Advancing Ethernet for Scale-Up AI Infrastructure\nAs AI systems continue to grow in scale and complexity, Ethernet is once again evolving to meet the challenge. At the OCP Global Summit 2025, Broadcom, together with AMD, ARM, Arista, Cisco, HPE Networking, Marvell, Meta, Microsoft, NVIDIA, OpenAI and Oracle announced a new ...\nMyths of AI networking debunked\nAs AI infrastructure scales at an unprecedented rate, a number of outdated assumptions keep resurfacing – especially when it comes to the role of networking in large-scale training and inference systems. Many of these myths are rooted in technologies that worked well for small ...\nScale-up is simple. Ethernet makes it smarter.\nThere’s a lingering belief in the industry that scaling up AI systems is difficult. That doing so means locking into proprietary interconnect or creating a completely new interconnect standard, managing tightly constrained topologies, and living with higher cost along with a new ...\nOFC 2025: Broadcom shines a light on the future of AI and optical networking\nBroadcom made a powerful impact at this year’s Optical Fiber Communications (OFC) Conference and Exhibition, unveiling a cutting-edge portfolio of optical interconnect solutions designed to accelerate AI infrastructure. The company’s strong presence extended across numerous ...\nOptical Interconnects: enabling large-scale AI clusters\nBroadcom leads the industry in three core optical interconnect technologies essential to building large-scale AI networks.AI server clusters are large and are only going to get larger. Optical links provide the connections needed to allow these clusters to scale at distance.\nInnovations in AI infrastructure: building custom AI accelerators\nSince last year’s public release of GenAI, people around the globe have been using it to research, tell stories, compose essays, create music and art, and write software code. ChatGPT, the popular chatbot from OpenAI, took only two months to reach 100 million monthly active ...\nBroadcom PCIe technology helps enable and accelerate AI infrastructure\nOne of the biggest challenges in building an AI infrastructure is how to connect tens of thousands of servers across multiple data centers to create a network large enough to run AI workloads. At the same time, we need to create a network inside each of those servers.The scale ...\nHow to scale the optical interconnect using Co-Packaged Optics (CPO)\nOptics are becoming increasingly important for both front-end and back-end networks for AI clusters. As communication bandwidth increases, CPO paves the path forward for optical links in these clusters.GPU clusters used to train large language models require a high degree of ...\nScaling AI infrastructure is a distributed computing problem\nIn mid-March, Broadcom hosted the Enabling AI Infrastructure event devoted to enabling AI infrastructure. During this event, which focused on networking and connectivity technologies for AI, a team of Broadcom executives provided participants with new and pertinent perspectives ...\nExecutive Videos on AI\nEnabling AI Infrastructure (27 min 11 sec)\nExecutive Insights\nTomahawk 6 – Davisson Launch\nIntroducing Scale-Up Ethernet (SUE)\nIntroducing Tomahawk 6\nMaking AI Real\nDell Technologies World 2024\nMWC Barcelona 2024\nCharlie Kawwas, President of the Semiconductor Solutions Group, sits down with theCUBE for a conversation\nNetworking the AI Future\nEthernet's Role in Scaling AI Infrastructure\nSix-Five: “Enabling AI Infrastructure”\nInvestor Meeting 2024 - Episode 209\nFor providers contending with the ever-increasing demand for generative AI clusters, the key to success will be a network-centric platform, based on open solutions, that scales at the lowest power. The innovations we've introduced extend our leadership for custom AI accelerators, Ethernet, PCI Express and optical interconnect portfolios. Built on our world-class foundational technologies like SerDes and DSP, they provide the best custom XPUs and merchant networking solutions enabling AI infrastructure.\nPresident, Semiconductor Solutions Group, Broadcom\nFeatured Technologies\n3.5D XDSiP™ Platform Technology\nInnovative multi-dimensional stacked die platform for AI XPUs.\nMerchant Silicon Networking Chips\nLeading-edge Ethernet for scale-up / scale-out AI networking.\nTomahawk® 6 – Davisson\n102.4-Tb/s Multilayer CPO Switch with 200G SerDes.\nSian3 DSP\n3-nm CMOS 1.6T (8:8) PAM-4 Transceiver PHY with Integrated Laser Driver.\nPCIe Gen 6 Solutions\nSimplified interoperability and system design for open AI infrastructure"
    },
    {
        "Ticker": "AVGO",
        "Company": "Broadcom",
        "Source_URL": "https://www.broadcom.com/company",
        "Content_Type": "Website Content",
        "Raw_Text": "Company\nOverview\nA Global Infrastructure Technology Leader\nSixty years of engineering excellence, product performance and financial success\nMore than 99% of web traffic uses Broadcom technology\nStay Informed\nGet the Latest Updates\nNewsroom\nAccess product and financial news releases — past and present.\nInvestors\nFind stock information, SEC filings and more in the investor center.\nBlog\nRead about Broadcom's latest innovations in the B-Connected blog.\nCompany Resources\nCareers\nContinuous innovation drives our digital transformation\n- $9.3B investment in R&D in FY24\n- A broad IP portfolio with more than 20,000 patents\n- 25 category-leading semiconductor and infrastructure software divisions\nRecognition\nIndustry Accolades\nWe're recognized by Forbes, Time, Newsweek and others for our leadership, trustworthiness and corporate responsibility.\nContact Us\nConnect\nContact our sales team or see locations of our offices worldwide.\nLogo Request\nRequest a Broadcom logo and learn how to use it properly.\nFeedback and Comments\nSend us your comments or ask a question of one of our experts.\nBriefing Centers\nRead about our executive briefing centers in Palo Alto, Calif., and Reston, Va."
    },
    {
        "Ticker": "AVGO",
        "Company": "Broadcom",
        "Source_URL": "https://www.broadcom.com/company/corporate-responsibility/supplier-responsibility",
        "Content_Type": "Website Content",
        "Raw_Text": "We are committed to supporting ethical business conduct, respecting human rights and responsibly sourcing materials throughout our global supply chain.\nRespecting Human Rights\nOur Human Rights Principles reflect our commitment to respecting human rights and avoiding complicity in any human rights abuse throughout our company, operations, supply chain and communities.\nOur employment and supply chain practices and policies, including our Supplier Code, support the fundamental human rights principles of freely chosen employment, non-discrimination, the elimination of forced and underage labor, and the rights of workers to engage in peaceful assembly, organize, freely associate and bargain collectively, as articulated in the International Labour Organization (ILO) Conventions.\nRespecting human rights is a shared responsibility, and we strive to align our approach and actions with the UN Guiding Principles on Business and Human Rights and ILO Conventions. We expect our suppliers to also respect internationally recognized human rights.\nResponsible Business Alliance\nBroadcom is a member of the Responsible Business Alliance (RBA). We are committed to upholding RBA’s vision of a global electronics industry that creates sustainable value for workers, the environment and business.\nResponsible Minerals Sourcing\nWe are committed to socially responsible sourcing of conflict minerals (cassiterite, columbite-tantalite, gold, wolframite, and/or their derivatives, tin, tantalum and tungsten) included in our products.\nWe are a member of the Responsible Minerals Initiative (RMI) and leverage the RMI’s resources to conduct our conflict minerals supply chain due diligence, including using RMI’s database of suppliers that are listed on the Responsible Minerals Assurance Process (RMAP) Conformant Smelter List.\nInquiries regarding our Conflict Minerals Policy may be made to: conflict.minerals@broadcom.com."
    },
    {
        "Ticker": "AVGO",
        "Company": "Broadcom",
        "Source_URL": "https://www.broadcom.com/blog/introducing-ethernet-scale-up-networking-advancing-ethernet-for-scale-up-ai-infrastructure",
        "Content_Type": "Website Content",
        "Raw_Text": "As AI systems continue to grow in scale and complexity, Ethernet is once again evolving to meet the challenge. At the OCP Global Summit 2025, Broadcom, together with AMD, ARM, Arista, Cisco, HPE Networking, Marvell, Meta, Microsoft, NVIDIA, OpenAI and Oracle announced a new collaborative effort called Ethernet for Scale-Up Networking (ESUN).\nWith the initiation of this workstream, the OCP Community now has the opportunity to work on areas that address scale-up connectivity across accelerated AI infrastructure. The scale-up domain in XPU-based systems can be viewed in two primary areas: 1) network functionality, and 2) XPU-endpoint functionality.\nFirst, the network aspect of scale-up focuses on how traffic is sent out across the network switches themselves, including protocol headers, error handling, and lossless data transfer. This is what ESUN intends to address and what the planned OCP workstream by the same name will focus on. The workstream itself is planned to kick off shortly after the OCP Global Summit.\nSecond, in the XPU-endpoint domain, the design depends on factors such as workload partitioning, memory ordering, and load balancing, and it is often tightly co-designed with the XPU architecture itself.\nWhat is ESUN?\nESUN is a new workstream collaboration designed as an open technical forum to advance Ethernet in the rapidly growing scale-up domain for AI systems. This initiative brings together operators and leading vendors to collaborate on leveraging and adapting Ethernet for the unique demands of scale-up networking.\nKey Focus Areas:\n- Technical Collaboration: ESUN serves as an open forum where operators, equipment and component manufacturers can jointly advance Ethernet solutions optimized for scale-up networking.\n- Interoperability: The initiative emphasizes development and interoperability of XPU network interfaces and Ethernet switch ASICs for scale-up.\n- Technical Focus: Initial focus will be on L2/L3 Ethernet framing and switching, enabling robust, lossless, and error-resilient single-hop and multi-hop topologies.\n- Standards Alignment: ESUN will actively engage with organizations such as UEC (Ultra-Ethernet Consortium) and IEEE 802.3 (Ethernet) to align with open standards, incorporate best practices, and accelerate innovation.\n- Ecosystem Enablement: By leveraging Ethernet’s mature hardware and software ecosystem, ESUN will encourage diverse implementations and drive rapid adoption across the industry.\nWhat Are the Focus Areas for ESUN?\nESUN focuses solely on open, standards-based Ethernet switching and framing for scale-up networking—excluding host-side stacks, non-Ethernet protocols, application-layer solutions, and proprietary technologies.\nHow is this Different from SUE?\nOCP has previously launched an effort to advance the endpoint functionality for scale-up networking through the SUE-Transport (Scale-Up Ethernet Transport) workstream (originally named SUE, it has been renamed and clarified as SUE-T in contrast to ESUN). SUE-T will carry forward some of the SUE work which was seeded with the Broadcom contribution of the version 1.0 specification.\nBroadcom Commitment\nBroadcom is looking forward to working with industry partners to advance an open and standards-based Ethernet architecture for scale-up AI systems. This effort reflects Broadcom’s ongoing commitment to support the OCP community and the broader ecosystem in defining interoperable, high-performance networking for AI workloads.\nBroadcom’s recently-introduced Tomahawk 6 and Tomahawk Ultra switch families are aligned with this objective, providing the bandwidth, scalability, and congestion management capabilities required in scale-up AI deployments. In addition, Broadcom contributed the SUE (Scale-Up Ethernet) specification to OCP, which has evolved into the SUE-Transport (SUE-T) workstream. Together, SUE-T and ESUN define a unified, open framework that enables Ethernet to efficiently scale from endpoint to switch fabric, supporting the next generation of scale-up AI infrastructure.\nJoin the Conversation\nAt OCP, ESUN’s workstream will host regular, open meetings for all interested parties. Whether you’re representing an operator, vendor, or standards body, your participation is welcome as we shape the future of scale-up networking together.\nLook for the official announcement in the OCP Global Summit opening, Broadcom and partner keynotes. Let’s build the next generation of open, scalable, and interoperable networking for AI and beyond."
    },
    {
        "Ticker": "AVGO",
        "Company": "Broadcom",
        "Source_URL": "https://www.broadcom.com/blog-search",
        "Content_Type": "Website Content",
        "Raw_Text": "When you interact with Broadcom as set forth in the Privacy Policy through visiting any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience.\nCookie PolicyPrivacy Policy\nThese cookies are necessary for the website to function and cannot be switched off in Broadcom’s systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nThese cookies enable the website to provide enhanced functionality and personalization. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.\nThese cookies allow Broadcom to count visits and traffic sources so Broadcom can measure and improve the performance of its site. They help Broadcom to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies Broadcom will not know when you have visited our site and will not be able to monitor its performance.\nThese cookies may be set through Broadcom’s site by its advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising."
    },
    {
        "Ticker": "AVGO",
        "Company": "Broadcom",
        "Source_URL": "https://www.broadcom.com/blog/myths-of-ai-networking-debunked",
        "Content_Type": "Website Content",
        "Raw_Text": "As AI infrastructure scales at an unprecedented rate, a number of outdated assumptions keep resurfacing – especially when it comes to the role of networking in large-scale training and inference systems. Many of these myths are rooted in technologies that worked well for small clusters. But today’s systems are scaling to hundreds of thousands – and soon, millions – of GPUs. Those older models no longer apply. Let’s walk through some of the most common myths – and why Ethernet has clearly emerged as the foundation for modern AI networking.\nMyth #1: You Cannot Use Ethernet for High Performance AI Networks\nThis myth has already been busted. Ethernet is now the de facto networking technology for AI at scale. Most, if not all, of the largest GPU clusters deployed in the past year have used Ethernet for scale-out networking.\nEthernet delivers performance that matches or exceeds what alternatives like InfiniBand offer – while providing a stronger ecosystem, broader vendor support, and faster innovation cycles. InfiniBand, for example, wasn’t designed for today’s scale. It’s a legacy fabric being pushed beyond its original purpose.\nMeanwhile, Ethernet is thriving: multiple vendors are shipping 51.2T switches, and Broadcom recently introduced Tomahawk 6, the industry’s first 102.4T switch. Ecosystems for optical and electrical interconnect are also mature, and clusters of 100K GPUs and beyond are now routinely built on Ethernet.\nMyth #2: You Need Separate Networks for Scale-Up and Scale-Out\nThis was acceptable when GPU nodes were small. Legacy scale-up links originated in an era when connecting two or four GPUs was enough. Today, scale-up domains are expanding rapidly. You’re no longer connecting four GPUs – you’re designing systems with 64, 128, or more in a single scale-up cluster. And that’s where Ethernet, with its proven scalability, becomes the obvious choice.\nUsing separate technologies for local and cluster-wide interconnect only adds cost, complexity, and risk. What you want is the opposite: a single, unified network that supports both. That’s exactly what Ethernet delivers – along with interface fungibility, simplified operations, and an open ecosystem.\nTo accelerate this interface convergence, we’ve contributed the Scale-Up Ethernet (SUE) framework to the Open Compute Project, helping the industry standardize around a single AI networking fabric.\nMyth #3: You Need Proprietary Interconnects and Exotic Optics\nThis is another holdover from a different era. Proprietary interconnects and tightly coupled optics may have worked for small, fixed systems – but today’s AI networks demand flexibility and openness.\nEthernet gives you options: third-generation co-packaged optics (CPO), module-based retimed optics, linear drive optics, and the longest-reach passive copper. You’re not locked into one solution. You can tailor your interconnect to your power, performance, and economic goals – with full ecosystem support.\nMyth #4: You Need Proprietary NIC Features for AI Workloads\nSome AI networks rely on programmable, high-power NICs to support features like congestion control or traffic spraying. But in many cases, that’s just masking limitations in the switching fabric.\nModern Ethernet switches – like Tomahawk 5 & 6 – integrate load balancing, rich telemetry, and failure resiliency directly into the switch. That reduces cost, lowers power, and frees up power for what matters most: your GPUs/ XPUs.\nLooking ahead, the trend is clear: NIC functions will increasingly be embedded into XPUs. The smarter strategy is to simplify, not over-engineer.\nMyth #5: You Have to Match Your Network to Your GPU Vendor\nThere’s no good reason for this. The most advanced GPU clusters in the world – deployed at the largest hyperscalers – run on Ethernet.\nWhy? Because it enables flatter, more efficient network topologies. It’s vendor-neutral. And it supports innovation – from AI-optimized collective libraries to workload-specific tuning at both the scale-up and scale-out levels.\nEthernet is a standards-based, well understood technology with a very vibrant ecosystem of partners. This allows AI clusters to scale more easily, and completely decoupled from the choice of GPU/XPU, delivering an open, scalable and power efficient system\nThe Bottom Line\nNetworking used to be an afterthought. Now it’s a strategic enabler of AI performance, efficiency, and scalability.\nIf your architecture is still built around assumptions from five years ago, it’s time to rethink them. The future of AI is being built on Ethernet – and that future is already here.\nClick here to explore more about Ethernet technology and here to learn more about Merchant Silicon."
    },
    {
        "Ticker": "AVGO",
        "Company": "Broadcom",
        "Source_URL": "https://www.broadcom.com/blog/scale-up-is-simple-ethernet-makes-it-smarter",
        "Content_Type": "Website Content",
        "Raw_Text": "There’s a lingering belief in the industry that scaling up AI systems is difficult. That doing so means locking into proprietary interconnect or creating a completely new interconnect standard, managing tightly constrained topologies, and living with higher cost along with a new set of operational tools.\nThe truth is: Scale-up is simpler than scale-out. And with the right foundation, such as Scale-Up Ethernet (SUE) announced by Broadcom last month, it's far more flexible.\nOver the past few years, Ethernet has proven itself as the best technology for scale-out AI networking — connecting XPU nodes within a data center and across data centers. Now it’s time to bring Ethernet’s advantages to scale-up: enabling fast, reliable, and open communication within a node or rack, across tens to hundreds of tightly coupled XPUs.\nWhy Ethernet for scale-up?\nWhen AI models run at massive scale, performance bottlenecks often emerge inside the system — where XPUs need to share memory, coordinate collectives or route outputs for Mixture-of-Experts inference.\nIn large-scale AI deployments, particularly scale-up architectures, multiple computing elements (XPUs and CPUs) operate closely together, often within a single server or tightly integrated system. As models become extremely large — potentially trillions of parameters — the compute and memory requirements surpass what any single processing unit can handle efficiently. This necessitates highly efficient intra-system communication, enabling XPUs to rapidly share data, synchronize computation, and collectively execute operations like all-reduce, broadcast, and gather, which are common in distributed model training and inference.\nThese scale-up domains demand:\n- High Bandwidth: Modern XPUs can have 40 – 100 TB/s of HBM bandwidth. Scale-up links need to match that.\n- Low Latency: Round-trip latency under 2µs is essential for remote memory access and fast synchronization.\n- Reliability: As XPUs access each other’s memory, every transfer must complete reliably — without introducing congestion, backpressure, or retries visible to the application.\n- Efficiency: When moving small payloads — often 64 bytes at a time — the networking overhead must be minimal.\nThese requirements are pushing interconnect technologies to their limits — and it’s here that Ethernet stands apart.\nIntroducing the Scale-Up Ethernet (SUE) framework\nTo bring Ethernet to scale-up domains, Broadcom developed the Scale-Up Ethernet (SUE) framework — a clean, standards-based specification for high-performance XPU interconnect.\nWe’ve contributed the full SUE specification to the Open Compute Project, making it available for others to build upon.\nAt a high level, SUE defines:\n- Interface Semantics: A consistent model for command, data, and management traffic between XPUs and the switch.\n- Memory Model: A simple, cache-coherent-like interface that supports put, get, and atomic operations over Ethernet, designed to emulate tightly coupled memory access.\n- Packet Format: Lightweight, latency-optimized headers that minimize per-transfer overhead – crucial for small data units.\n- Congestion Management: Support for link-level retry (LLR), credit-based flow control (CBFC), and priority flow control (PFC) to ensure lossless operation and deterministic latency.\n- Fungible Interfaces: The same XPU port can be used for both scale-up and scale-out, allowing for dynamic partitioning of resources and simplified hardware design.\nSUE also allows flexibility in bandwidth — with 800G per SUE instance and supporting multiple network planes for load balancing, fault isolation, and increased aggregate throughput — and supports scale-up systems up to 1,024 XPUs in a single domain.\nBelow, Ram Velaga, Senior Vice President and General Manager, Core Switching Group at Broadcom, sits down with The Cube and explains Scale Up Ethernet (SUE).\nUnifying the fabric\nAI infrastructure shouldn’t require two separate networks — one for intra-node and one for inter-node communication. With SUE, you can unify scale-up and scale-out on a single Ethernet fabric, using the same optical and copper interconnects, the same visibility tools, and the same platform-wide telemetry.\nThat translates to:\n● Lower system complexity\n● Greater agility in system design\n● Reduced TCO\n● Vendor-neutral innovation\nUnlike proprietary or semi-custom interconnect models, SUE runs on Ethernet — open by design, not just by license. No special chiplets, PHYs, or switch fabrics required.\nSUE enables multi-vendor system design without dictating board layout, rack architecture, or thermal envelope. It works with the global Ethernet ecosystem, without needing specialized integration or vendor certification.\nConclusion\nAI infrastructure is evolving fast — and the networking needs within an XPU node are now just as critical as those between nodes.\nWith Scale-Up Ethernet, we’ve made scale-up simple, efficient, and open. We’re excited to see how the industry builds on this foundation."
    },
    {
        "Ticker": "AVGO",
        "Company": "Broadcom",
        "Source_URL": "https://www.broadcom.com/blog/ofc-2025-broadcom-shines-a-light-on-the-future-of-ai-and-optical-networking",
        "Content_Type": "Website Content",
        "Raw_Text": "Broadcom made a powerful impact at this year’s Optical Fiber Communications (OFC) Conference and Exhibition, unveiling a cutting-edge portfolio of optical interconnect solutions designed to accelerate AI infrastructure. The company’s strong presence extended across numerous technical and executive sessions, where it led dynamic discussions on key topics—from the manufacturability of co-packaged optics (CPO) to the growing demand for high-speed lasers powering next-generation AI workloads.\nA vibrant, collaborative partner ecosystem\n“OFC’s 50th anniversary provides the opportunity to recognize the industry’s many achievements, including Broadcom’s industry-first contributions to this field,” said Charlie Kawwas, Ph. D., President, Semiconductor Solutions Group, Broadcom. “A year ago, Broadcom committed to pushing technical boundaries to pioneer new open, scalable and power-efficient technologies to enable AI infrastructure. Our portfolio of optical interconnect solutions, highlighted at OFC 2025, paves the way to 200T by addressing the performance, power, and scalability challenges of AI clusters.”\nBroadcom’s successful technical collaboration with ecosystem partners was on display at more than 20 partner booths across the show floor. We showcased a wide range of novel technologies, underscoring our commitment to developing cutting-edge solutions for AI infrastructure, including:\n- XPU-CPO: The industry’s first 6.4-Tbps optics attach for custom AI accelerator (XPU) enabling high bandwidth, long reach scale-up fabric connectivity for AI servers.\n- Sian3: State-of-the-art 3nm 200G/lane DSP delivering industry’s lowest power consumption with enhanced performance for 800G and 1.6T optical transceivers over SMF.\n- Sian2M: The industry’s first 200G/lane DSP with integrated VCSEL drivers enabling low power, short reach MMF links in AI clusters.\n- 200G/lane Lasers: Leading-edge 200G VCSEL, EML and CWL technologies facilitating high speed interconnects for front-end and back-end networks of large-scale AI clusters.\n- 400G EML: The industry’s first demonstration of 400G EML technology for next-generation AI optical interconnects.\n- PCIe Gen6 over Optics: The industry’s first demonstration of PCIe Gen6 optical connectivity for AI scale-up fabric using Broadcom’s market-proven 100G VCSEL and photodetector.\n- LPO / BCM957608 NIC: Industry-leading 400G PCIe Ethernet NIC connecting with LPO module to enable scalable AI networks with high performance and efficiency.\n- Co-Packaged & Near-Packaged Copper: State-of-the-art 200G/lane copper link solutions enabling cost-effective, high-bandwidth connectivity in emerging AI architectures.\n- 7m+ AEC for 800G: Industry’s first 800G AEC retimer solution extending DAC cable reach beyond 7 meters.\nAn industry working together\nMany of the show’s technical workshops, technology showcases, and forums were standing-room-only. Our executives and engineers participated in lively conversations with their peers, and shared valuable data and insights about how to solve the new technical challenges for AI infrastructure. Among the sessions Broadcom participated in included:\n- The evolution from copper to optical – where is the line?\nManish Mehta, VP of Marketing and Operations, Optical Systems Division, discussed the limitations of passive copper, such as reach, and how optical technologies can overcome these barriers to enable larger AI clusters. By transitioning to optics, scalability ceilings for AI infrastructure can be broken. Mehta highlighted Broadcom’s progress in CPO solutions, and showed, for the first time in our industry, CPO reliability data. He noted that many customers are actively exploring trials for this innovative technology.\n- CPO → towards large-scale networks\nAnand Ramaswamy, Solutions Architect, Optical Systems Director, noted that Broadcom has demonstrated that CPO is manufacturable and already in production. Ramaswamy shared data from Broadcom’s 51.2 Tbps systems, showcasing the performance and reliability of CPO. He emphasized Broadcom’s investments in foundational technologies like photonic integrated circuits, CMO electronics, advanced packaging, and automation to enable CPO at scale.\n- High-power and multi-wavelength laser light sources: How can they address the needs of AI/ML interconnect?\nJohn Johnson, director of Indium Phosphide Component Research and Development, Optical Systems Division, discussed the demand for high-power lasers to support AI workloads. While higher power levels are requested, challenges such as managing high-optical power densities and reliability remain. Future architectures may use multiple channels or wavelengths to distribute power requirements.\n- Towards 400G/λ IM-DD: How to pick up the next factor of 2?\nVasu Parthasarathy, Technical Director, discussed advancing Ethernet speeds to 400G. He highlighted challenges like mismatched transmitter and receiver specifications and excessive margins that hinder interoperability and discussed aligning industry standards to enable smoother evolution to 400G networking.\n- Modular structures with EML thin film LN and ring-based design\nPrashanth Bhasker, Optoelectronic Design Engineer, showcased Broadcom’s ultra-high-speed Electro-absorption Modulated Lasers (EMLs) for 400G applications.\nBroadcom’s optical connectivity: Looking ahead to the future\nOptical connectivity is essential for open, scalable, power-efficient AI infrastructure. Broadcom will continue to deliver innovations to address technical challenges and enable AI to help customers and improve global businesses and lives.\nMissed the 50th anniversary of OFC? Visit here for our latest OFC news, photos, videos, partner collaborations. OFC returns to Los Angeles March 15-19, 2026. See you there!"
    },
    {
        "Ticker": "AVGO",
        "Company": "Broadcom",
        "Source_URL": "https://www.broadcom.com/blog/optical-interconnects-enabling-large-scale-ai-clusters",
        "Content_Type": "Website Content",
        "Raw_Text": "Broadcom leads the industry in three core optical interconnect technologies essential to building large-scale AI networks.\nAI server clusters are large and are only going to get larger. Optical links provide the connections needed to allow these clusters to scale at distance. Optical technology can support that growth in terms of both scale and cost.\nThe Core Optical Interconnects for AI\nThere are three core optical interconnects:\n- Vertical-Cavity Surface-Emitting Lasers, or VCSELs, are a workhorse for optical AI interconnect technology across the industry. Its low power and low cost make it ideal for data communications and sensing applications. The only limitation is that it operates best at shorter link distances.\n- Electro-absorption Modulated Lasers, or EMLs, are ideal for AI systems that scale to greater distances and hundreds of thousands, or even millions of units. This technology provides better performance at very high bandwidth and typically are first to reach volume deployments at next generation data rates.\n- Co-Packaged Optics, or CPO, is an advanced heterogeneous integration of high-speed silicon photonics onto application-specific integrated circuits for addressing next-generation bandwidth and power challenges. We see this new technology providing the power and cost leadership for future generations of AI systems and enabling infrastructures supporting large-scale AI networks.\nLeading the Way\nBroadcom VCSEL technology has led the industry in time-to-market, volume, performance and reliability at 25, 50 and 100 gigabit per second per lane (G/lane). We are also very encouraged by our progress on 200G/lane VCSELs, an achievement that many thought impossible technically as it was assumed the technology could not sustain the data rates needed for the future of AI. We have also extended our EML technology from 100G/lane to 200G/lane and will be shipping large scale production volumes of these new 200G/lane EMLs very soon.\nFoundational Technologies\nJust a few months ago, we had our first commercial shipments of our Bailly 51.2-Tbps CPO Ethernet switch system. This is a combination of our 51.2-Tbps Tomahawk 5 switch with complete optical links directly integrated into the package. What that means is that all 512 lanes are directly, optically attached to the Tomahawk 5 switch itself. This is a foundational technology for AI infrastructure that could scale for all kinds of applications where optics are required.\nWhy is CPO needed? With AI systems, the bandwidth and the amount of components continues to scale. As it does, the cost of the optics continues to rise. Our solution is integration into silicon photonics, allowing us to put more components onto an individual chip. In the history of semiconductors, that has been the proven way to reduce costs.\nAnother benefit is that the optics can be placed on a common package with the core chip. That placement eliminates the complex electrical lanes between the ASICs and the optics. Typical 800G pluggable transceivers consume about 16 watts per link. The Bailly CPO system would reduce that optical link power to 5 watts, a 70 percent savings over typical deployments today. Perhaps even more impressive, it scales and delivers better efficiency when the optical link migrates to 1.6T, typically pluggable transceivers will consume 25 watts vs. 8 watts with CPO. That’s a big deal in terms of both power savings and the optics.\nA third major benefit is enhanced reliability. Pluggable transceivers have approximately a 2 percent failure rate. By integrating more components into the chip, reliability is enhanced with CPO. Rather than integrate lasers directly on silicon, what we have done is maintain the laser system as a pluggable and easily replaceable component in the system. Everything else is built on core silicon technology with its long history of reliability.\nBroadcom is focused on providing silicon photonics in a high-density fashion. Doing so allows us to move the optics off the subsystems that are these pluggable transceivers and move them directly onto ASIC substrates. We are doing exactly that for the first time with our Tomahawk 5 switch: 512 lanes of optical connectivity across the entire device.\nThis is just the beginning. We see this technology being extended to many areas. It is foundational for enabling AI infrastructure and driving the entire technology industry forward. We see CPO engines being applied to various types of ASIC substrates for scale-up and scale-out AI interconnects. Customers essentially get the performance of pluggable transceivers but with lower power, costs, and a tremendous reliability advantage. Reliability that will enable AI infrastructure and allow our customers to sleep at night.\nTo learn more and watch my recent presentation, please visit our page here. You can also listen to my podcast interview here with Bloomberg Intelligence."
    },
    {
        "Ticker": "AVGO",
        "Company": "Broadcom",
        "Source_URL": "https://www.broadcom.com/blog/innovations-in-ai-infrastructure-building-custom-ai-accelerators",
        "Content_Type": "Website Content",
        "Raw_Text": "Since last year’s public release of GenAI, people around the globe have been using it to research, tell stories, compose essays, create music and art, and write software code. ChatGPT, the popular chatbot from OpenAI, took only two months to reach 100 million monthly active users, making it the fastest growing consumer application in history.\nBut what’s behind GenAI and the AI infrastructure — the hardware, software, and firmware that makes it all run? The “cloud titans” running that infrastructure, –Google, Meta, Microsoft, and Amazon – make up more than 80 percent of the market. With the fast adoption of GenAI, these cloud titans are now “AI titans.”\nThat’s because to successfully support those using GenAI, these titans must support incredibly large internal workloads. They have vast amounts of data on which large language models, or LLMs, can be used to advance AI technologies in areas such as virtual assistants, chatbots and recommendation engines. In supporting commercial businesses, there is an expanding array of new areas in which LLMs can play a role, such as customer operations, supply-chain management, R&D and cybersecurity.\nBecause LLMs are trained on vast sets of data, AI infrastructure requires specialized compute accelerators that are interconnected to form massive clusters. In the infrastructure hardware world, most people know GPUs — general purpose graphics processing units. Then there are XPUs, “X” application-specific purpose units that are becoming critical for AI infrastructures. AI-specific XPUs are called custom AI accelerators because, literally, that’s what they are: custom chips that provide the processing power to drive AI infrastructure.\nXPU Benefits and Customer Partnerships\nCompanies running AI infrastructure want a custom XPU when they have internal workloads critical for specific applications. By customizing the architecture of their accelerator, the (XPU to XPU) IO bandwidth and the memory bandwidth are able to process those application workloads at larger volumes, more efficiently than with general silicon.\nThe benefits of these custom chips can be explained by a simple equation: performance divided by total cost of ownership, or TCO. TCO is the sum of the cost of the chip, the power, and the infrastructure that puts it together.\nAt Broadcom, we work with our customers to customize their architecture so they can maximize the performance of their AI infrastructure and internal workloads. Optimizing hardware also leads to a smaller, less expensive, and reduced footprint. Customers that engage with us to co-develop these custom chips tend to save billions of dollars of capital expenses because they are designed to perform exactly as planned, with the right ratio of accelerator, memory bandwidth, and I/O bandwidth. Optimizing the hardware for AI infrastructure also means lowering the power and, therefore, the overall cost of these custom chips.\nTaken together, these advantages combine to make Custom XPUs the ideal financial and engineering solution for consumer AI.\nThe Broadcom Engagement Model\nCustom AI chips have essentially four layers: compute, memory, network I/O, and a reliable packaging technology. The complex architecture, along with intricacies in packaging, require us to look at what our customers want to accomplish with their AI workloads – three, five, and even 10 years into the future.\nWorking with our customers, we have developed, optimized, and automated a flow to build the accelerators as small, fast, and with as minimal space as possible. This is a shared responsibility owned by both Broadcom and the customer.\nBroadcom owns the memory layer. We create the right memory solutions, the right connectivity, cooling, testing, and reliability qualification to ensure that the AI accelerators are reliable and ready to go. It’s worth noting that we also are running these interfaces significantly faster than any current standard.\nWe own the software tools that allow us to quickly put together chiplets to make the IOs wider or thinner to match the exact precision and ratio customers need for their AI workloads. This is the flexibility brought by software automation that we’ve also developed over the years.\nOur accumulated knowledge over the last decade of co-developing XPUs uniquely positions us to quickly assist our customers. For example, we have finessed the mechanical, thermal, and electrical flows, and fine-tuned our testing and automation to streamline design and development.\nBroadcom has the hardware, firmware, and software for a variety of networking solutions that can be included in the XPU as plug-and-play chiplets. This allows our customers to emulate, simulate, and take the accelerator to production faster and more efficiently.\nSumming it up: Our Custom XPU engagement model allows our partners to get the best quality, performance, and TCO tuned for their workloads.\nThe Broadcom Advantage\nBroadcom is committed to building on its decade-long industry leadership in custom silicon. We know what custom silicon builds require. That is why we have such a large and proactive investment in R&D – about $3 billion annually -- in creating technology of the future. The vast majority of this investment is prioritized for the AI market.\nFor example, our Co Package Optics Cores (proven in our latest XPU) can save 80 watts of system power per XPU. Imagine the benefits of putting a million of these accelerators together with that power and cost savings.\nWe continue to lead in areas such as high speed SERDES, chip to chip connectivity, high speed memory, 2.5 and 3D packaging, and time to market, at an annual cadence or faster, depending on customer needs.\nOur objective is to continue to drive the best performance for AI workloads in XPUs at the lowest possible TCO for our customers. There are always challenges ahead, but we know that we have the expertise and experience to meet whatever comes next, to help our customers achieve even greater success.\nTo learn more and watch my presentation, please visit our page here."
    },
    {
        "Ticker": "AVGO",
        "Company": "Broadcom",
        "Source_URL": "https://www.broadcom.com/blog/broadcom-pcie-technology-helps-enable-and-accelerate-ai-infrastructure",
        "Content_Type": "Website Content",
        "Raw_Text": "When you interact with Broadcom as set forth in the Privacy Policy through visiting any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience.\nCookie PolicyPrivacy Policy\nThese cookies are necessary for the website to function and cannot be switched off in Broadcom’s systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nThese cookies enable the website to provide enhanced functionality and personalization. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.\nThese cookies allow Broadcom to count visits and traffic sources so Broadcom can measure and improve the performance of its site. They help Broadcom to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies Broadcom will not know when you have visited our site and will not be able to monitor its performance.\nThese cookies may be set through Broadcom’s site by its advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising."
    },
    {
        "Ticker": "AVGO",
        "Company": "Broadcom",
        "Source_URL": "https://www.broadcom.com/blog/how-to-scale-the-optical-interconnect-using-co-packaged-optics-cpo",
        "Content_Type": "Website Content",
        "Raw_Text": "Optics are becoming increasingly important for both front-end and back-end networks for AI clusters. As communication bandwidth increases, CPO paves the path forward for optical links in these clusters.\nGPU clusters used to train large language models require a high degree of parallel processing generating a renewed interest in optical interconnect scale and innovation. Today’s AI clusters contain tens of thousands of GPUs as companies hope to build clusters of over one million GPUs before the end of the decade. Because the cluster spans many rows of server racks, optical interconnects that enable GPU to GPU communication through a switch fabric are growing rapidly in volume. But even with the recent growth in scale-out cluster volume for back-end networks, most GPU bandwidth is still routed locally through electrical links. Future migrations to larger scale-up domains (e.g. hundreds of GPUs) could increase optical bandwidth requirements per GPU by an additional order of magnitude. Broadcom has invested in co-packaged optics to scale the optical interconnect into the age of AI. I will focus on two areas of scale in this post: integration and manufacturing.\nIntegration\nIntegration is at the heart of CPO, and has to be done right.\nThe first image below visually summarizes our approach to high density photonic integration for CPO. Charlie Kawwas, Broadcom’s Semiconductor Solutions Group President, compares the quantity of (128) 400G optical modules required to fully populate a 51.2 Tbps switch to Broadcom’s Bailly 51.2T equivalent CPO solution. All of the 128 optical modules (denoted by the blue tabs) surrounding the table collapse into eight 6.4 Tbps optical engines co-packaged on a common substrate with a 51.2 Tbps Tomahawk® 5 switch ASIC.\nThe second image offers a close-up of our TH5-Bailly 51.2T CPO device. Each of the 6.4 Tbps Bailly optical engines contains hundreds of photonic components, delivering an order of magnitude increase in integration density compared to silicon photonics used in traditional pluggable transceivers. As a point of reference, we can offer 6.4 Tbps for just double the silicon area of our 400 Gbps photonic integrated circuit (PIC). That is an 8x improvement in silicon area efficiency with positive implications on cost, power, and shoreline bandwidth density. CPO offers a compelling solution to match the growing GPU I/O bandwidth requirement with equivalent beachfront optical interconnect density.\nManufacturing\nOne of the key challenges to solve as we take CPO mainstream is the ability to manufacture at scale. Innovations in manufacturing become as important as the technical innovations that integrate high-bandwidth optics with switches.\nTo meet the evolving interconnect demands highlighted above, Broadcom has also invested for scale in CPO manufacturing automation. Traditional pluggable transceivers are riddled with unpredictable quality and reliability due to manual assembly and test processes prevalent throughout the industry. At Broadcom, we have emphasized selecting best-in-class silicon manufacturing processes when available. If standard manufacturing processes and tools are unavailable, we build our own automation. Please watch the following video which gives a sneak peek into our end-to-end automated manufacturing process. From PIC and EIC fabrication, to wafer test, chip-to-wafer bonding, optical component attach, and CPO assembly and test, we are working to minimize sources of variation due to manual handling. We hope this video will provide evidence of Broadcom’s commitment to building and shipping CPO in high volume, not just to test the market and determine if the technology sticks.\nFinal Thoughts\nWe are undoubtedly excited about the distance we have covered with CPO, and the possibilities that open up in the world of AI hardware. Our journey to deliver a mass production CPO solution is not yet complete. We recently announced that we have started to ship systems to cloud Titans and we are fortunate to be working with a group of committed partners to bring our CPO platform to market. Thank you for continuing to follow our CPO progress and stay tuned for more updates.\nLearn more\nConnecting the future with Broadcom CPO"
    },
    {
        "Ticker": "AVGO",
        "Company": "Broadcom",
        "Source_URL": "https://www.broadcom.com/blog/scaling-ai-infrastructure-is-a-distributed-computing-problem",
        "Content_Type": "Website Content",
        "Raw_Text": "When you interact with Broadcom as set forth in the Privacy Policy through visiting any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience.\nCookie PolicyPrivacy Policy\nThese cookies are necessary for the website to function and cannot be switched off in Broadcom’s systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nThese cookies enable the website to provide enhanced functionality and personalization. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.\nThese cookies allow Broadcom to count visits and traffic sources so Broadcom can measure and improve the performance of its site. They help Broadcom to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies Broadcom will not know when you have visited our site and will not be able to monitor its performance.\nThese cookies may be set through Broadcom’s site by its advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising."
    },
    {
        "Ticker": "AVGO",
        "Company": "Broadcom",
        "Source_URL": "https://www.broadcom.com/info/ai/3point5d",
        "Content_Type": "Website Content",
        "Raw_Text": "Benefits\nBroadcom’s 3.5D XDSiP has arrived to power a new era of custom compute. Key benefits of this innovative platform include:\n- Enhanced Interconnect Density - Achieves a 7x increase in signal density between stacked dies compared to F2B technology.\n- Superior Power Efficiency - Delivers a 10x reduction in power consumption in die-to-die interfaces by utilizing 3D HCB instead of planar die-to-die PHYs.\n- Reduced Latency - Minimizes latency between compute, memory, and I/O components within the 3D stack.\n- Compact Form Factor - Enables smaller interposer and package sizes, resulting in cost savings and improved package warpage."
    },
    {
        "Ticker": "AVGO",
        "Company": "Broadcom",
        "Source_URL": "https://www.broadcom.com/company/news/product-releases/63641",
        "Content_Type": "Website Content",
        "Raw_Text": "When you interact with Broadcom as set forth in the Privacy Policy through visiting any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience.\nCookie PolicyPrivacy Policy\nThese cookies are necessary for the website to function and cannot be switched off in Broadcom’s systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nThese cookies enable the website to provide enhanced functionality and personalization. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.\nThese cookies allow Broadcom to count visits and traffic sources so Broadcom can measure and improve the performance of its site. They help Broadcom to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies Broadcom will not know when you have visited our site and will not be able to monitor its performance.\nThese cookies may be set through Broadcom’s site by its advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising."
    },
    {
        "Ticker": "AVGO",
        "Company": "Broadcom",
        "Source_URL": "https://www.broadcom.com/company/news/product-releases/63631",
        "Content_Type": "Website Content",
        "Raw_Text": "Product News Release\nOpenAI and Broadcom announce strategic collaboration to deploy 10 gigawatts of OpenAI-designed AI accelerators\nMulti-year partnership enables\nNews:\nOpenAI and Broadcom will co-develop systems that include accelerators and Ethernet solutions from Broadcom for scale-up and scale-out- Broadcom to deploy racks of AI accelerator and network systems targeted to start in the second half of 2026, to complete by end of 2029\n“Partnering with Broadcom is a critical step in building the infrastructure needed to unlock AI’s potential and deliver real benefits for people and businesses,” said\n“Broadcom’s collaboration with\n“Our collaboration with Broadcom will power breakthroughs in AI and bring the technology’s full potential closer to reality,” said\n“Our partnership with\nFor Broadcom, this collaboration reinforces the importance of custom accelerators and the choice of Ethernet as the technology for scale-up and scale-out networking in AI data centers.\nFor more information on the news, please visit openai.com/podcast.\nAbout Broadcom\nBroadcom Inc. (NASDAQ: AVGO) is a global technology leader that designs, develops, and supplies a broad range of semiconductor, enterprise software and security solutions. Broadcom's category-leading product portfolio serves critical markets including cloud, data center, networking, broadband, wireless, storage, industrial, and enterprise software. Our solutions include service provider and enterprise networking and storage, mobile device and broadband connectivity, mainframe, cybersecurity, and private and hybrid cloud infrastructure. Broadcom is a Delaware corporation headquartered in Palo Alto, CA. For more information, go to www.broadcom.com.\nAbout\nPress Contact\npress.relations@broadcom.com\nCautionary Note Regarding Forward-Looking Statements\nThis announcement contains forward-looking statements (including within the meaning of Section 21E of the Securities Exchange Act of 1934, as amended, and Section 27A of the Securities Act of 1933, as amended) concerning Broadcom. These statements include, but are not limited to, statements regarding Broadcom’s collaboration with\nParticular uncertainties that could materially affect future results include risks associated with: global political and economic conditions and uncertainty; government regulations, trade restrictions and trade tensions; fluctuations in the timing and volume of significant customer demand; ability to make successful investments in research and development and successfully expand Broadcom’s business strategy or adopt Broadcom’s new business models; ability to continue winning business and the timing of such wins; dependence on senior management and the ability to attract and retain qualified personnel; ability to protect against cybersecurity threats and a breach of security systems; dependence on contract manufacturing and outsourced supply chain; dependency on a limited number of suppliers; ability to accurately estimate customers’ demand and adjust the manufacturing and supply chain accordingly; ability to improve manufacturing capacity and quality; involvement in legal proceedings; quarterly and annual fluctuations in operating results; Broadcom’s competitive performance; ability to maintain or improve gross margin; ability to protect Broadcom’s intellectual property and the unpredictability of any associated litigation expenses; significant indebtedness and the need to generate sufficient cash flows to service and repay such debt; and other events and trends on a national, regional, industry-specific and global scale, including those of a political, economic, business, competitive and regulatory nature.\nBroadcom’s filings with the Securities and Exchange Commission (SEC) are available without charge at the SEC’s website at https://www.sec.gov and include some important risk factors that may affect future results. Actual results may vary from the forward-looking statements provided. Broadcom undertakes no intent or obligation to publicly update or revise the forward-looking statements made in this announcement, except as required by law.\nSource: Broadcom Inc."
    },
    {
        "Ticker": "AVGO",
        "Company": "Broadcom",
        "Source_URL": "https://www.broadcom.com/company/news/product-releases/63626",
        "Content_Type": "Website Content",
        "Raw_Text": "Product News Release\nBroadcom Announces Tomahawk® 6 – Davisson, the Industry’s First 102.4-Tbps Ethernet Switch with Co-Packaged Optics\n“TH6-Davisson, our third-generation CPO Ethernet switch, represents a significant advancement for AI infrastructure,” said Near Margalit, Vice President and General Manager, Optical Systems Division, Broadcom. “By enhancing link stability and energy efficiency, we’re enabling smoother, more cost-effective AI model training. We designed this platform to scale large AI clusters by delivering on the three imperatives for optical interconnect: higher model FLOPs utilization, reduced job interruptions, and improved cluster reliability.”\nCPO: Purpose-Built for AI Networking\nThe rise of large-scale AI training and inference is driving unprecedented east-west traffic in data centers, as XPUs and GPUs exchange vast datasets across tens of thousands of servers. Traditional pluggable optics are straining under this growth—consuming more power, introducing higher latency, and demanding larger system footprints. TH6-Davisson embodies Broadcom’s lead-edge and robust CPO architecture that has been fine-tuned over multiple generations of development to overcome these barriers and deliver the bandwidth, efficiency, and reliability required for next-generation AI networks.\nWorld-Class Energy Efficiency\nTH6-Davisson was architected from the ground up for power efficiency. By heterogeneously integrating TSMC Compact Universal Photonic Engine (TSMC COUPE™) technology-based optical engines with advanced substrate-level multi-chip packaging, the switch dramatically reduces the need for signal conditioning and minimizes trace loss and reflections. The result is a 70% reduction in optical interconnect power consumption—more than 3.5x lower than traditional pluggable solutions—delivering a step change in energy efficiency for hyperscale and AI data centers.\nImproved Link Stability and Traffic Performance\nAs AI training jobs scale, link stability has become a critical bottleneck, with even minor interruptions causing measurable losses in XPU and GPU utilization. TH6-Davisson addresses this challenge by directly integrating optical engines onto a common package with the Ethernet switch. This highly integrated design eliminates many of the sources of manufacturing and test variability inherent in pluggable transceivers. The result is significantly improved link flap performance and higher cluster reliability as validated by a TH5-Bailly link flap study here.\nBandwidth Improvement and Interoperability\nOperating at 200 Gbps per channel, TH6-Davisson doubles the line rate and overall bandwidth of Broadcom’s second-generation TH5-Bailly CPO solution. Designed for interoperability, it seamlessly interconnects with DR-based transceivers as well as LPO and CPO optical interconnects running at 200 Gbps per channel. This ensures frictionless connectivity with the industry’s most advanced NICs, XPUs, and fabric switches, enabling next-generation AI and cloud clusters to scale without compromise.\nCPO Roadmap\nLeveraging the advanced CMOS nodes and continuous breakthroughs in device technology, Broadcom is now developing its fourth-generation CPO solution. The new generation will double per-channel bandwidth to 400 Gbps while achieving greater levels of energy efficiency, further extending Broadcom’s leadership in enabling the scale and sustainability of future AI and cloud networks.\nTH6-Davisson BCM78919 Product Highlights\n- 102.4 Tbps Switching Capacity\n- 16 x 6.4 Tbps Davisson DR Optical Engines\n- 200 Gbps per link bandwidth\n- Field-Replaceable ELSFP Laser Modules\n- Supports Scale-Up cluster size of 512 XPUs and up to 100,000+ XPUs in two-tier networks at 200 Gbps per link\n- IEEE 802.3 Compliance — Interoperable with existing 400G and 800G standards.\nAvailability\nBroadcom is currently sampling the TH6-Davisson BCM78919 device to its early access customers and partners. Contact your local Broadcom sales representative for samples and pricing. For more information on Broadcom CPO, please click here.\nSupporting Quotes:\n\"We are pleased that Broadcom continues to drive innovation with the launch of its new TH6-Davisson CPO solution. TH6-Davisson's advanced optical integration, optimized power efficiency, and enhanced performance will present a compelling value proposition that aligns with Celestica’s strategy to deliver next-generation AI infrastructure and workload deployments.\"\n“Corning continues to collaborate closely with Broadcom to ensure that their CPO connectivity needs can be met with a high degree of performance and reliability as AI-enabled data centers continue to scale. We are continuously innovating to enable increasingly powerful and efficient AI networks, such as complete faceplate-to-chip optical assemblies for TH6-Davisson systems, and we are excited to continue collaborating with Broadcom toward their next generation CPO systems.”\n“HPE is proud to continue our collaboration with Broadcom on TH6-Davisson for our next-generation of HPE Networking AI-native solutions. Together, we can enable cloud customers to unlock new levels of efficiency, reliability and performance, paving the way for the next era of AI infrastructure.”\n\"At Micas, we have partnered with Broadcom across its first two generations of CPO engines, and its most recent CPO engine, Bailly, has undergone millions of hours of testing and demonstrated exceptional reliability. All the data points to a turning point for hyperscaler adoption of CPO. The timing of Tomahawk® 6 – Davisson could not be better. As the industry scales out AI networks with 800G today and prepares for 1.6T interconnects, Davisson delivers the great power savings, CAPEX efficiency, and rock-solid stability needed to confidently build the next generation of large-scale AI infrastructure.\"\n\"NextHop is proud to collaborate with Broadcom on the 102.4T, Tomahawk-6 switch based on Broadcom’s third-generation Davisson Co-Packaged Optics (CPO). This architecture delivers a significant reduction in power consumption (pJ/bit) and increased scalability in support of hyperscaler AI factories, at unmatched price/performance. Coupled with Nexthop’s hardened SONiC, our hyperscale customers can get a complete, highly reliable system with all the benefits of the CPO.\"\n“TSMC is dedicated to collaborating with industry innovators like Broadcom to drive energy-efficient, high-performance breakthroughs that meet the growing demands of AI and enable scalable technologies for next-generation networks. By leveraging our COUPE process, advanced semiconductor manufacturing expertise, and through a close partnership, we are proud to support Broadcom in bringing the Tomahawk 6 – Davisson platform to fruition.”\nAbout Broadcom\nBroadcom, the pulse logo, and Connecting everything are among the trademarks of Broadcom. The term \"Broadcom\" refers to\nPress Contact:\npress.relations@broadcom.com\nTelephone: +1 408 433 8649\nSource: Broadcom Inc."
    },
    {
        "Ticker": "AVGO",
        "Company": "Broadcom",
        "Source_URL": "https://www.broadcom.com/company/news/product-releases/63621",
        "Content_Type": "Website Content",
        "Raw_Text": "When you interact with Broadcom as set forth in the Privacy Policy through visiting any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience.\nCookie PolicyPrivacy Policy\nThese cookies are necessary for the website to function and cannot be switched off in Broadcom’s systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nThese cookies enable the website to provide enhanced functionality and personalization. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.\nThese cookies allow Broadcom to count visits and traffic sources so Broadcom can measure and improve the performance of its site. They help Broadcom to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies Broadcom will not know when you have visited our site and will not be able to monitor its performance.\nThese cookies may be set through Broadcom’s site by its advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising."
    },
    {
        "Ticker": "AVGO",
        "Company": "Broadcom",
        "Source_URL": "https://www.broadcom.com/company/news/product-releases/63616",
        "Content_Type": "Website Content",
        "Raw_Text": "Product News Release\nBroadcom Showcases Industry-Leading Quality and Reliability of Co-Packaged Optics\nCPO technology is a key enabler for next-generation data center architectures, offering unprecedented bandwidth density and power efficiency by tightly integrating optical engines with switch silicon. Ensuring the reliability of this innovative integration has been a key focus for Broadcom and its partners.\nTesting of Broadcom CPO solutions at Meta has demonstrated one million link hours without a single link flap, validating Broadcom’s engineering excellence and rigorous qualification process. Link flaps, brief connectivity disruptions, are a critical reliability metric in high-performance data center networks. The absence of link flaps in Meta’s high-temperature lab characterization environment highlights the industrial-grade stability and reliability of Broadcom’s CPO implementation.\n“Achieving one million link flap-free hours is a strong validation of Broadcom’s commitment to quality and innovation,” said Near Margalit, vice president and general manager of Broadcom’s Optical Systems Division. “This milestone shows that CPO is not just a research concept — it is production-proven and ready to scale.”\nBroadcom’s CPO platform has been designed from the ground up with system-level reliability in mind, including:\n- Advanced thermal management and control systems\n- Proven optical engine packaging with integrated monitoring\n- Robust firmware and link diagnostics\n- End-to-end validation across electrical, optical, and mechanical domains\nAs hyperscale data centers push beyond 51.2 Tb/s switch bandwidth, CPO offers a sustainable path forward by addressing the power constraints and physical limitations of traditional pluggable optics. Broadcom continues to collaborate with ecosystem partners to drive CPO adoption and interoperability across the industry.\n“This milestone reinforces the long-term vision for co-packaged optics as the foundation for next-generation AI and cloud infrastructure,” added Margalit. “Broadcom is proud to lead this transformation with our growing ecosystem of partners.”\nTo learn more about Broadcom’s CPO solutions, please visit www.broadcom.com/CPO\n1. ECOC 2025, “Co-Packaged Optics Technology Evaluation for Hyperscale Data Center Fabric Switches” by Siamak Amiralizadeh,\nAbout Broadcom\nBroadcom, the pulse logo, and Connecting everything are among the trademarks of Broadcom. The term \"Broadcom\" refers to\nPress Contact:\npress.relations@broadcom.com\nTelephone: +1 408 433 8649\nSource: Broadcom Inc."
    },
    {
        "Ticker": "AVGO",
        "Company": "Broadcom",
        "Source_URL": "https://www.broadcom.com/company/news/product-releases/63361",
        "Content_Type": "Website Content",
        "Raw_Text": "Product News Release\nBroadcom Ships Jericho4, Enabling Distributed AI Computing Across Data Centers\nAs AI models grow in size and complexity, the infrastructure requirements exceed the power and physical limits of a single data center. Distributing XPUs across multiple facilities — each provisioned with tens to hundreds of megawatts of power — requires a new class of router, optimized for very high-bandwidth, secure and lossless transport across regional distances.\n“The Jericho4 family is engineered to extend AI-scale Ethernet fabrics beyond individual data centers, supporting congestion-free RoCE and 3.2 Tbps HyperPort for unprecedented interconnect efficiency,” said\nA single Jericho4 system scales to 36,000 HyperPorts, each operating at 3.2 Tb/s, with deep buffering, line-rate MACsec, and RoCE transport over 100km+ distances.\nBroadcom’s 3.2T HyperPort technology consolidates four 800GE links into a single logical port — eliminating load balancing inefficiencies, boosting utilization by up to 70%, and streamlining traffic flow across large fabrics.\nThanks to deep buffering and intelligent congestion control, Jericho4 ensures lossless RoCE across 100km+ enabling truly distributed AI infrastructure unconstrained by power and space limitations at a single location.\nJericho4 supports MACsec encryption on every port at full speed to protect data moving between data centers, delivering strong security without compromising performance — even at the highest traffic loads.\nManufactured on a 3nm process, Jericho4 features Broadcom’s advanced 200G PAM4 SerDes with industry-leading reach. This eliminates the need for extra components like retimers, resulting in lower power usage, reduced cost, and higher system reliability.\nJericho4 is fully compliant with specifications developed by the\nLearn More\nVisit the Jericho4 product page here. Explore the comprehensive Scale-Up/Scale-Out media kit here for resources and insights on Broadcom’s scalable solutions. For in-depth details on Broadcom’s CPO technology, click here.\nAvailability\nJericho4 is sampling to customers now.\nAbout Broadcom\nBroadcom, the pulse logo, and Connecting everything are among the trademarks of Broadcom. The term \"Broadcom\" refers to Broadcom Inc., and/or its subsidiaries. Other trademarks are the property of their respective owners.\nPress Contact:\npress.relations@broadcom.com\nTelephone: +1 310 498 5254\nIndustry quotes:\nMichael KT Lee, Senior Vice President, R&D, Accton\n“Accton has successfully delivered systems to customers using Broadcom’s Distributed Disaggregated Chassis (DDC) scheduled fabric solutions. With the availability of Jericho4, Accton is looking forward to collaborating with Broadcom to design new platforms that scale out the AI network further, incorporating features such as MACsec, long-reach 200G SerDes, and UEC as the building blocks for evolving demands of scale-out AI clusters, while improving the energy efficiency and the modular flexibility needed for mega-scale GPU clusters.”\n“Broadcom’s Jericho4 sets a new standard for scalable, lossless Ethernet routing that perfectly complements Arista’s high-performance R-Series systems and EOS software, enabling distributed AI data centers to operate at unprecedented scale and efficiency.”\n“Broadcom’s Jericho4 sets a new benchmark for scale, bandwidth, and energy efficiency — critical for connecting distributed AI workloads across Training and Inferencing use cases. Arrcus’ ArcOS software enables Jericho4 offering from Broadcom, with a disaggregated, carrier-grade software stack for high-performance, programmable networks across edge, core, and multi-cloud environments. Together with Broadcom, we’re delivering the agility and operational simplicity required for AI data center interconnects, 5G transport, and beyond.”\n“DriveNets Network Cloud-AI has already established industry-leading performance in Ethernet-based AI networking, powering Jericho3-based white boxes in a distributed disaggregated model. With the deep-buffer and low-latency capabilities of Jericho4, we are poised to deliver even greater AI networking scalability and performance, enabling a unified, high-efficiency solution for both AI back-end fabrics and storage networks in support of next-generation AI infrastructure demands.\"\n“Jericho4’s deep buffers, long-reach RoCE, and HyperPort innovation are exactly what our customers need to stitch together massive AI fabrics across data centers and metro regions. Combined with Tomahawk, Broadcom gives us the complete toolbox to build high-performance networks at any scale—from leaf-spine clusters to region-wide AI interconnects.”\n\"Modern day Hyperscaler AI clusters continue to outgrow the power and physical footprint of a single data center building. Cloud providers now have to build larger clusters that are distributed across multiple locations and diverse geographies. Broadcom's Jericho4 family of devices, supporting deep buffer, lossless Ethernet, RoCE, MACsec and advanced congestion management capabilities over long distances, now enable Nexthop to provide scalable network architecture options from a single rack to Gigawatt scale AI clusters.\"\n\"Broadcom’s Jericho4 family of silicon delivers the scale, performance, and efficiency we need to push AI infrastructure to the next level. As AI workloads stretch across data centers and regions, Nokia’s 7250 IXR routers — powered by Jericho4 — ensure high-throughput, lossless connectivity for the most demanding distributed AI systems. Broadcom continues to be our trusted partner in helping Nokia meet the evolving needs of the AI era.\"\n“UfiSpace's mission is to deliver solutions that offer the best possible combination of performance, efficiency, and customer choice. With the future of AI built on distributed computing across data centers, Jericho4 strikes a crucial balance between these elements. Its silicon serves as a benchmark for building cost-effective, massively scalable AI networks, ultimately making distributed AI a reality.”\nSource: Broadcom Inc."
    },
    {
        "Ticker": "AVGO",
        "Company": "Broadcom",
        "Source_URL": "https://www.broadcom.com/company/news/product-releases/63341",
        "Content_Type": "Website Content",
        "Raw_Text": "Product News Release\nBroadcom Ships Tomahawk Ultra: Reimagining the Ethernet Switch for HPC and AI Scale-up\n“Tomahawk Ultra is a testament to innovation, involving a multi-year effort by hundreds of engineers who reimagined every aspect of the Ethernet switch,” said\nShattering Myths, Redefining Performance\nBuilt from the ground up to meet the extreme demands of HPC environments and tightly coupled AI clusters, Tomahawk Ultra redefines what an Ethernet switch can deliver. Long perceived as higher-latency and lossy, Ethernet takes on a new role:\n- Ultra-low latency: Achieves 250ns switch latency at full 51.2 Tbps throughput.\n- High performance: Delivers line-rate switching performance even at minimum packet sizes of 64 bytes, supporting up to 77 billion packets per second.\n- Adaptable, optimized Ethernet headers: Reduces header overhead from 46 bytes down to as low as 10 bytes, while maintaining full Ethernet compliance —boosting network efficiency and enabling flexible, application-specific optimizations.\n- Lossless fabric: Implements Link Layer Retry (LLR) and Credit-Based Flow Control (CBFC) to eliminate packet loss and ensure reliability.\n“AI and HPC workloads are converging into tightly coupled accelerator clusters that demand supercomputer-class latency — critical for inference, reliability, and in-network intelligence from the fabric itself,” said\nBuilt for HPC and AI Scale-Up\nTomahawk Ultra is optimized for the tightly coupled, low-latency communication patterns found in both high-performance computing systems and AI clusters. With ultra-low latency switching and adaptable optimized Ethernet headers, it provides predictable, high-efficiency performance for large-scale simulations, scientific computing, and synchronized AI model training and inference.\nWhen deployed with Scale-Up Ethernet (SUE specification available to the public here), Tomahawk Ultra enables sub-400ns XPU-to-XPU communication latency, including the switch transit time — setting a new benchmark for tightly synchronized AI compute at scale.\nBy reducing Ethernet header overhead from 46 bytes to just 10 bytes, while maintaining full Ethernet compliance, Tomahawk Ultra dramatically improves network efficiency. This optimized header is adaptable per application, offering both flexibility and performance gains across diverse HPC and AI workloads.\nTomahawk Ultra incorporates lossless fabric technology that eliminates packet drops during high-volume data transfer. Incorporating LLR, the switch detects link errors using Forward Error Correction and automatically retransmits packets, avoiding drops at the wire level. Simultaneously, CBFC prevents buffer overflows that traditionally caused packet loss. Together, these mechanisms create a truly lossless Ethernet fabric, delivering the level of reliability demanded by today’s most data-intensive workloads.\nTomahawk Ultra also accelerates performance through In-Network Collectives solving one of the most persistent bottlenecks in AI and machine learning workloads. Rather than burdening XPUs with collective operations like AllReduce, Broadcast, or AllGather, Tomahawk Ultra executes these directly within the switch chip. This can reduce job completion time and improve utilization of expensive compute resources. Importantly, this capability is endpoint-agnostic, enabling immediate adoption across a wide range of system architectures and vendor ecosystems.\nDesigned with innovations in topology-aware routing to support advanced HPC topologies including Dragonfly, Mesh and Torus, Tomahawk Ultra is also compliant with the UEC standard and embraces the openness and rich ecosystem of Ethernet networking.\nIntroducing SUE-Lite\nAs part of Broadcom’s Ethernet-forward strategy for AI scale-up, the company has introduced SUE-Lite — an optimized version of the SUE specification tailored for power and area-sensitive accelerator applications. SUE-Lite retains the key low-latency and lossless characteristics of full SUE, while further reducing the silicon footprint and power consumption of Ethernet interfaces on AI XPUs and CPUs.\nThis lightweight variant enables easier integration of standards-compliant Ethernet fabrics in AI platforms, promoting broader adoption of Ethernet as the interconnect of choice in scale-up architectures.\nPlatform for AI Scale-Up and HPC Scale-Out\nTogether with the 102.4 Tbps Tomahawk 6, Tomahawk Ultra forms the foundation of a unified Ethernet architecture: enabling scale-up Ethernet for AI, and scale-out Ethernet for HPC and distributed workloads.\nTomahawk Ultra is 100% pin-compatible with Tomahawk 5, ensuring a very fast time-to-market. It is shipping now for deployment in rack-scale AI training clusters and supercomputing environments. To learn more about the Broadcom Tomahawk Ultra family click here. Explore the full Scale-Up/Scale-Out media kit for resources and insights into Broadcom’s scalable solutions here.\nAbout Broadcom\nBroadcom, the pulse logo, and Connecting everything are among the trademarks of Broadcom. The term \"Broadcom\" refers to Broadcom Inc., and/or its subsidiaries. Other trademarks are the property of their respective owners.\nPress Contact:\npress.relations@broadcom.com\nTelephone: +1 310 498 5254\nIndustry Quotes\nMichael KT Lee, Senior Vice President,\n“Networking needs within an XPU node are as critical as those between nodes. With 51.2 Tbps Ethernet switching, 250-ns latency, credit-based flow control, and configurable optimized header, the Tomahawk Ultra is a perfect solution for building high-bandwidth, high-reliability, high-efficiency, and low-latency lossless systems ready for scale-up AI and HPC applications. Accton is excited to embrace the launch and collaborate closely with Broadcom to bring the Tomahawk Ultra solutions to the market.”\n“Low latency is essential to unleashing the full potential of AI — from reducing training times to powering real-time inference. By combining Broadcom’s new Tomahawk Ultra switch with AMD Instinct™ GPUs and EPYC™ processors, we’re enabling high-performance, standards-based Ethernet solutions for AI infrastructure. Together, we’re advancing an open ecosystem that brings our vision of AI everywhere, for everyone, closer to reality.”\n“Arista appreciates the combination of ultra-low latency and scale-Up Ethernet innovations of Tomahawk Ultra for AI networking. Once again Broadcom is setting the pace in the AI and the switch industry.”\n“The launch of Broadcom’s Tomahawk Ultra marks a groundbreaking advancement in Ethernet innovation, particularly for AI and HPC scale-up environments. With ultra-low latency and a lossless fabric, it significantly accelerates job completion times — critical for modern AI workloads. At Arrcus, we’re proud to champion an open, standards-based networking ecosystem. Combined with our high-performance ArcOS network operating system, customers can unlock scalable infrastructure that is both flexible and future-ready.”\nWangson Wang, General Manager of Data Networks Infrastructure, Delta Electronics\n“Delta Electronics is constantly looking ahead, and we’re thrilled to confirm that our 51.2T Ethernet switch platform is ready to harness the full power of Broadcom's Tomahawk Ultra chip. We see Tomahawk Ultra as a game-changer for AI scale-up and HPC. The collaboration between Delta and Broadcom demonstrates our dedication to pushing the boundaries of what is possible in Data Center network infrastructure. Building on the success of Delta’s current 800G switches, the newly launched Tomahawk Ultra chips enable us to deliver advanced solutions that offer not only unmatched performance and efficiency, but also high reliability and scalability for the most demanding network workloads — supporting rapid AI/ML network deployments for our customers.”\n“HPE is committed to delivering open, high-performance and easy-to-manage Ethernet-based solutions for the modern data center. We commend Broadcom on its new offering, and its ultra-low latency, high throughput and support for in-network collectives align perfectly with what today’s workloads demand. It reflects our shared vision for building the most advanced and open data center infrastructure solutions with operational simplicity at its core.\"\n“Broadcom's Tomahawk Ultra Series with its high throughput and ultra-low latency enables all-to-all connectivity across up to 64 Intel® Gaudi® 3 AI accelerators per rack with total HBM bandwidth of 76.8TB/s, capable of scaling the connectivity across multiple racks. This rack-level bandwidth unlocks new possibilities for training and real-time inference of the most complex LLMs, redefining industry SLAs. Through our collaboration with Broadcom, Intel is showcasing the open architecture and modular design advantage and full capability of our rack scale platform built for large-scale, global AI deployments.”\n“Inventec congratulates Broadcom on the launch of Tomahawk Ultra Ethernet switch, which significantly enhances the efficiency and sustainability of AI solutions by delivering the industry’s lowest switch latency, 250 nanoseconds, and leading power efficiency with 800W at 51.2T performance. At Inventec, our vision is to develop cutting-edge artificial intelligence products that drive sustainable change for humanity and the environment through close partnership with Broadcom to deliver high-performance, scalable solutions, supporting customers’ evolving AI and high-performance computing needs.”\n“The Tomahawk Ultra represents a bold leap forward in AI workloads and HPC clusters, delivering an unmatched combination of bandwidth, latency, and cutting-edge features like In-Network Collectives and scale-up Ethernet. By leveraging non-proprietary Ethernet, the Tomahawk Ultra will empower customers to scale their data intensive applications with unparalleled performance, efficiency, and reliability — paving the way for groundbreaking innovations in data-intensive computing.”\n“Broadcom’s Tomahawk Ultra is a major step forward for scale-up Ethernet in AI and HPC. With 250ns latency, 51.2 Tbps switching, and advanced features like Link Layer Retry, In-Network Collectives, and the AI Fabric Header, it delivers the performance, reliability, and efficiency our customers need for AI at scale. Thanks to pin compatibility with Tomahawk 5, Micas can rapidly bring Tomahawk Ultra-based systems to market, enabling seamless upgrades to meet the demands of next-generation AI infrastructure.”\n“With Tomahawk Ultra, Broadcom has driven AI Networking to a new level, allowing us to enable a new generation of low latency and lossless scale-up Ethernet solutions. Along with Nexthop SONiC, we now offer some of the most efficient scale-up and UEC compatible scale-out Ethernet solutions for the world’s largest hyperscalers.”\n“At QCT, we are committed to delivering next-generation AI and HPC infrastructure that meets the demands of extreme scale, performance, and efficiency. Broadcom’s Tomahawk Ultra Ethernet switch is a game-changer for the AI era, enabling 51.2 Tbps of switching capacity with ultra-low 250ns latency to dramatically accelerate AI training and inferencing workloads. We are excited to continue collaborating with Broadcom to push the next frontier of AI with Ethernet-based infrastructure.”\n“The Tomahawk Ultra delivers high performance and full pin-to-pin compatibility with Tomahawk 5. This seamless upgrade path shortens our development cycle for next-generation platforms, and we’re excited to integrate it into our upcoming solutions.”\nRobert CL Lin, President of\n“Broadcom’s Tomahawk Ultra sets a new benchmark in Open Ethernet for AI and HPC. Designed for GPU scale-up, the Tomahawk Ultra achieves 250ns latency at 51.2 Tbps, supporting 64B line-rate switching and lossless fabrics. This innovation represents a significant step forward for the industry. Wistron is seamlessly aligning these scalable AI systems, and the Tomahawk Ultra solution offering.”\n“We’re proud to partner with Broadcom on the innovative Tomahawk Ultra. Purpose-built for the demands of AI and HPC, this advanced platform combines high performance with open Ethernet flexibility — enabling our customers to deploy scalable, reliable, and future-ready networks.”\nSource: Broadcom Inc."
    },
    {
        "Ticker": "AVGO",
        "Company": "Broadcom",
        "Source_URL": "https://www.broadcom.com/company/news/product-releases/63146",
        "Content_Type": "Website Content",
        "Raw_Text": "When you interact with Broadcom as set forth in the Privacy Policy through visiting any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience.\nCookie PolicyPrivacy Policy\nThese cookies are necessary for the website to function and cannot be switched off in Broadcom’s systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nThese cookies enable the website to provide enhanced functionality and personalization. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.\nThese cookies allow Broadcom to count visits and traffic sources so Broadcom can measure and improve the performance of its site. They help Broadcom to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies Broadcom will not know when you have visited our site and will not be able to monitor its performance.\nThese cookies may be set through Broadcom’s site by its advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising."
    },
    {
        "Ticker": "AVGO",
        "Company": "Broadcom",
        "Source_URL": "https://www.broadcom.com/company/news/product-releases/63126",
        "Content_Type": "Website Content",
        "Raw_Text": "Product News Release\nBroadcom Announces Third-Generation Co-Packaged Optics (CPO) Technology with 200G/lane Capability\nKey partner milestones with CPO switches demonstrate ecosystem readiness for next-generation AI scale-up and scale-out networks\nBroadcom’s Legacy in CPO\nBroadcom’s leadership in CPO began in 2021 with its first-generation Tomahawk 4-\nBuilding on that success, the second-generation Tomahawk 5-Bailly (TH5-Bailly) chipset became the industry’s first volume-production CPO solution. As part of the TH5-Bailly production, Broadcom focused on automated testing and scalable manufacturing processes, setting the stage for high-volume production of future generations. The deployment of Broadcom’s 100G/lane CPO product line has enabled the company to gain unmatched expertise in CPO system design, seamlessly integrating optical and electrical components to maximize the performance while delivering the lowest power optical interconnects in the industry.\nToday, with the announcement of the third-generation 200G/lane CPO product line, alongside commitment to developing a fourth-generation 400G/lane solution, Broadcom continues to lead the industry in delivering the lowest power and highest bandwidth density optical interconnects.\nA Rapidly Developing CPO Ecosystem\nBroadcom’s leadership in CPO is driven not only by its cutting-edge switch ASICs and optical engine technology but also by a comprehensive ecosystem of passive optical components, interconnects, and system solutions partners. Through its 100G/lane CPO product line, Broadcom has proven its ability to scale its technology, meeting the growing demands of inference-based AI and supporting the next wave of AI-driven applications.\n“Broadcom has spent years perfecting our CPO platform solutions, as evidenced by the maturity of our second generation 100G/lane products and the ecosystem readiness,” said Near Margalit, Ph. D., vice president and general manager of the Optical Systems Division, Broadcom. “With our third-generation 200G/lane CPO solutions, we are once again setting the bar for the next-generation of AI interconnects. Our commitment to delivering the industry-leading performance, power efficiency, and scalability will help our customers meet the demands of today’s rapidly evolving AI infrastructure.”\nKey Partner Milestones Towards Mass Deployment\nBroadcom’s advancements in CPO technology are supported by the growing number of key publicly announced partnerships across the ecosystem, as several major collaborators announced significant milestones this week:\n- Corning Incorporated announced collaboration with Broadcom on advanced fiber and connector technology including shipments of components on the TH5-Bailly platform.\n- Delta Electronics announced production for the TH5-Bailly 51.2T CPO Ethernet switch in a compact 3RU form factor, available in both air-cooled and liquid-cooled configurations.\nFoxconn Interconnect Technology revealed the production release of CPO LGA sockets and Pluggable Laser Source (PLS) cages and connectors, critical components for ensuring reliable, high-performance system integration.- Micas Networks announced production of the TH5-Bailly network switch system that delivers more than 30% system level power savings compared to systems with traditional pluggable modules.\n- Twinstar Technologies celebrated milestone volume shipments of high-density CPO fiber cables, further enabling the scaling of optical interconnects in next-gen data center infrastructures.\nThese partner milestones demonstrate the continued progress in building a complete, fully integrated CPO ecosystem that enables the next generation of AI networking solutions.\nGen 3 CPO: Unlocking 200G/lane CPO Systems\nBroadcom’s 200G/lane CPO technology is designed for next-generation, high-radix scale-up and scale-out networks, which will demand parity with copper interconnect reliability and power efficiency. This capability is crucial for enabling scale-up domains exceeding 512 nodes, while also addressing the bandwidth, power, and latency challenges associated with the increasing size of next-generation foundation model parameters.\nBroadcom’s Gen 3 solutions are engineered to address scale-up interconnects, where issues such as link flaps and operational disruptions can significantly affect the industry’s ability to achieve the lowest cost per token. Broadcom’s Gen 3 and 4 roadmap includes close collaboration with ecosystem partners to optimize the integration of CPO solutions, ensuring they meet the demanding requirements of hyperscale data centers and AI workloads. Additionally, Broadcom remains committed to open standards and system-level optimization, which are essential to the continued success and evolution of our CPO technology.\nFor more information on Broadcom’s CPO technology and latest advancements, please click here: www.broadcom.com/cpo\nSupporting Quotes\n“Corning has been collaborating with Broadcom for several years to ensure that their CPO connectivity needs can be met with a high degree of performance and reliability as AI-enabled datacenters continue to scale,” said Mike O’Day, Senior Vice President and General Manager,\n“We are excited to bring this state-of-the-art CPO switch to market, empowering data centers to achieve even greater efficiency and performance,” said Wangson Wang, General Manager of Data Networks Infrastructure BU at Delta. “Our goal is to support the next generation of networking infrastructure with innovative solutions that deliver unparalleled speed, reduced energy consumption, and scalable growth for AI networks.\n“We’re deepening our partnership with Broadcom to drive innovation in 200G co-packaged optics,” said\n“At Micas Networks, we’re excited to see Broadcom pushing the boundaries of networking technology with their next-generation 200G per lane CPO solutions. Our collaboration with Broadcom has been instrumental in launching the industry's first volume production, 100G per lane, CPO system delivering ultra low-power optical interconnects into AI fabrics,” said\nAbout Broadcom\nBroadcom, the pulse logo, and Connecting Everything are among the trademarks of Broadcom. The term \"Broadcom\" refers to\nPress Contact:\npress.relations@broadcom.com\nTelephone: +1 408 433 8649\nSource: Broadcom Inc."
    },
    {
        "Ticker": "AVGO",
        "Company": "Broadcom",
        "Source_URL": "https://www.broadcom.com/company/news/product-releases/63021",
        "Content_Type": "Website Content",
        "Raw_Text": "Product News Release\nBroadcom Advances Optical Connectivity for AI Infrastructure with Industry-Leading Solutions at OFC 2025\nAI workloads are rapidly increasing, driving the need for higher bandwidth, lower latency, and more power-efficient optical interconnects. Broadcom is meeting these evolving demands with a comprehensive portfolio of innovative solutions designed to support the growth and scalability of AI clusters. These solutions include low-power, high-bandwidth DSP, SerDes and CPO for reduced power consumption and improved signal integrity, and PCIe Gen6 over optics for enhanced connectivity between AI accelerators and other system components.\nAt OFC, Broadcom is showcasing a wide range of novel technologies underscoring our commitment to developing cutting-edge solutions for AI infrastructure:\n- XPU-CPO: Industry’s first 6.4-Tbps optics attach for custom AI accelerator (XPU) enabling high bandwidth, long reach scale-up fabric connectivity for AI servers.\n- Sian3: State-of-the-art 3nm 200G/lane DSP delivering industry’s lowest power consumption with enhanced performance for 800G and 1.6T optical transceivers over SMF.\n- Sian2M: Industry’s first 200G/lane DSP with integrated VCSEL drivers enabling low power, short reach MMF links in AI clusters.\n- 200G/lane Lasers: Leading-edge 200G VCSEL, EML and CWL technologies facilitating high speed interconnects for front-end and back-end networks of large-scale AI clusters.\n- 400G EML: Industry’s first demonstration of 400G EML technology for next-generation AI optical interconnects.\n- PCIe Gen6 over Optics: Industry’s first demonstration of PCIe Gen6 optical connectivity for AI scale-up fabric using Broadcom’s market-proven 100G VCSEL and photodetector.\n- LPO / BCM957608 NIC: Industry-leading 400G PCIe Ethernet NIC connecting with LPO module to enable scalable AI networks with high performance and efficiency.\n- Co-Packaged & Near-Packaged Copper: State-of-the-art 200G/lane copper link solutions enabling cost-effective, high-bandwidth connectivity in emerging AI architectures.\n- 7m+ AEC for 800G: Industry’s first 800G AEC retimer solution extending DAC cable reach beyond 7 meters.\n“OFC’s 50th anniversary provides the opportunity to recognize the industry’s many achievements, including Broadcom’s industry-first contributions to this field,” said\nIn addition, Broadcom is collaborating with more than 15 partners to demonstrate a wide array of its industry-leading solutions across the show floor. Throughout the conference, Broadcom is speaking on the technical challenges and advancements in optical networking and communications. Key talks and technical panel sessions this year include:\nHigh Power and Multi-Wavelength Laser Light Sources: How Can They Address the Needs of AI/ML Interconnect?,Sunday, March 30 ,1:00pm –3:30pm ,Room 215 .- How Do Co-Packaged Optics Become Manufacturable?,\nSunday, March 30 ,4:00pm –6:30pm , Rooms 203-204. - Short and Sweet: How Do We Cost-Optimize a 10-Meter Link for Scaling Up Machine Learning Clusters?,\nSunday, March 30 ,4:00pm –6:30pm , Rooms 211-212. - Towards 400G/λ IM-DD: How to Pick up the Next Factor of 2?,\nSunday March 30 ,4:00pm –6:30pm , Rooms 213-214. - The Evolution from Copper to Optical – Where is the line?,\nMonday, March 31 ,1:00pm –2:00pm ,Optica Executive Forum . - Optimized Interconnect for AI Scale-Out and Scale-Up,\nTuesday April 1 ,12:15pm –12:45pm , Expo Theater III. - Modular Structures with EML Thin Film LN and Ring-Based,\nTuesday, April 1 ,2:00pm –4:00pm ,Room 301 .\nThe 2025 conference takes place in\nAbout Broadcom\nBroadcom, the pulse logo, and Connecting Everything are among the trademarks of Broadcom. The term \"Broadcom\" refers to\nPress Contact:\npress.relations@broadcom.com\nTelephone: +1 408 433 8649\nSource: Broadcom Inc."
    }
]